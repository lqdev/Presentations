<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.Spark</name>
    </assembly>
    <members>
        <member name="T:Microsoft.Spark.VersionAttribute">
            <summary>
            Base class for custom attributes that involve the Spark version.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.VersionAttribute.#ctor(System.String)">
            <summary>
            Constructor for VersionAttribute class.
            </summary>
            <param name="version">Spark version</param>
        </member>
        <member name="P:Microsoft.Spark.VersionAttribute.Version">
            <summary>
            Returns the Spark version.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.SinceAttribute">
            <summary>
            Custom attribute to denote the Spark version in which an API is introduced.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SinceAttribute.#ctor(System.String)">
            <summary>
            Constructor for SinceAttribute class.
            </summary>
            <param name="version">Spark version</param>
        </member>
        <member name="T:Microsoft.Spark.RemovedAttribute">
            <summary>
            Custom attribute to denote the Spark version in which an API is removed.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RemovedAttribute.#ctor(System.String)">
            <summary>
            Constructor for RemovedAttribute class.
            </summary>
            <param name="version">Spark version</param>
        </member>
        <member name="T:Microsoft.Spark.DeprecatedAttribute">
            <summary>
            Custom attribute to denote the Spark version in which an API is deprecated.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.DeprecatedAttribute.#ctor(System.String)">
            <summary>
            Constructor for DeprecatedAttribute class.
            </summary>
            <param name="version">Spark version</param>
        </member>
        <member name="T:Microsoft.Spark.UdfWrapperAttribute">
            <summary>
            Custom attribute to denote that a class is a Udf Wrapper.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Broadcast`1">
            <summary>
            A broadcast variable. Broadcast variables allow the programmer to keep a read-only variable
            cached on each machine rather than shipping a copy of it with tasks. They can be used, for
            example, to give every node a copy of a large input dataset in an efficient manner. Spark
            also attempts to distribute broadcast variables using efficient broadcast algorithms to
            reduce communication cost.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.Value">
            <summary>
            Get the broadcasted value.
            </summary>
            <returns>The broadcasted value</returns>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.Unpersist">
            <summary>
            Asynchronously delete cached copies of this broadcast on the executors.
            If the broadcast is used after this is called, it will need to be re-sent to each
            executor.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.Unpersist(System.Boolean)">
            <summary>
            Delete cached copies of this broadcast on the executors. If the broadcast is used after
            this is called, it will need to be re-sent to each executor.
            </summary>
            <param name="blocking">Whether to block until unpersisting has completed</param>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.Destroy">
            <summary>
            Destroy all data and metadata related to this broadcast variable. Use this with
            caution; once a broadcast variable has been destroyed, it cannot be used again.
            This method blocks until destroy has completed.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.OnSerialized(System.Runtime.Serialization.StreamingContext)">
            <summary>
            Serialization callback function that adds to the JvmBroadcastRegistry when the
            Broadcast variable object is being serialized.
            </summary>
            <param name="context">The current StreaminContext being used</param>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.CreateTempFilePath(Microsoft.Spark.SparkConf)">
            <summary>
            Function that creates a temporary directory inside the given directory and returns the
            absolute filepath of temporary file name in that directory.
            </summary>
            <param name="conf">SparkConf object</param>
            <returns>Absolute filepath of the created random file</returns>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.CreateBroadcast(Microsoft.Spark.SparkContext,`0)">
            <summary>
            Function to create the Broadcast variable (org.apache.spark.broadcast.Broadcast)
            </summary>
            <param name="sc">SparkContext object of type <see cref="T:Microsoft.Spark.SparkContext"/></param>
            <param name="value">Broadcast value of type object</param>
            <returns>Returns broadcast variable of type <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/></returns>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.CreateBroadcast_V2_3_1_AndBelow(Microsoft.Spark.Interop.Ipc.JvmObjectReference,System.Object)">
            <summary>
            Calls the necessary functions to create org.apache.spark.broadcast.Broadcast object
            for Spark versions 2.3.0 and 2.3.1 and returns the JVMObjectReference object.
            </summary>
            <param name="javaSparkContext">Java Spark context object</param>
            <param name="value">Broadcast value of type object</param>
            <returns>Returns broadcast variable of type <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/></returns>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.CreateBroadcast_V2_3_2_AndAbove(Microsoft.Spark.Interop.Ipc.JvmObjectReference,Microsoft.Spark.SparkContext,System.Object)">
            <summary>
            Calls the necessary Spark functions to create org.apache.spark.broadcast.Broadcast
            object for Spark versions 2.3.2 and above, and returns the JVMObjectReference object.
            </summary>
            <param name="javaSparkContext">Java Spark context object</param>
            <param name="sc">SparkContext object</param>
            <param name="value">Broadcast value of type object</param>
            <returns>Returns broadcast variable of type <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/></returns>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.WriteToFile(System.Object)">
            <summary>
            Function that creates a file in _path to store the broadcast value in the given path.
            </summary>
            <param name="value">Broadcast value to be written to the file</param>
        </member>
        <member name="M:Microsoft.Spark.Broadcast`1.Dump(System.Object,System.IO.Stream)">
            <summary>
            Function that serializes and stores the object passed to the given Stream.
            </summary>
            <param name="value">Serializable object</param>
            <param name="stream">Stream to which the object is serialized</param>
        </member>
        <member name="T:Microsoft.Spark.BroadcastRegistry">
            <summary>
            Global registry to store the object value of all active broadcast variables from
            the workers. This registry is only used on the worker side when Broadcast.Value() is called
            through a UDF.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.BroadcastRegistry.Add(System.Int64,System.Object)">
            <summary>
            Function to add the value of the broadcast variable to s_registry.
            </summary>
            <param name="bid">Id of the Broadcast variable object to add</param>
            <param name="value">Value of the Broadcast variable</param>
        </member>
        <member name="M:Microsoft.Spark.BroadcastRegistry.Remove(System.Int64)">
            <summary>
            Function to remove the Broadcast variable from s_registry.
            </summary>
            <param name="bid">Id of the Broadcast variable object to remove</param>
        </member>
        <member name="M:Microsoft.Spark.BroadcastRegistry.Get(System.Int64)">
            <summary>
            Returns the value of the Broadcast variable object of given Id.
            </summary>
            <param name="bid">Id of the Broadcast variable object</param>
            <returns>Value of the Broadcast variable with given Id</returns>
        </member>
        <member name="T:Microsoft.Spark.JvmBroadcastRegistry">
            <summary>
            Stores the JVMObjectReference object of type org.apache.spark.broadcast.Broadcast for all
            active broadcast variables that are sent to the workers through the CreatePythonFunction.
            This registry is only used on the driver side.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.JvmBroadcastRegistry.Add(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Adds a JVMObjectReference object of type <see cref="T:Microsoft.Spark.Broadcast`1"/> to the list.
            </summary>
            <param name="broadcastJvmObject">JVMObjectReference of the Broadcast variable</param>
        </member>
        <member name="M:Microsoft.Spark.JvmBroadcastRegistry.Clear">
            <summary>
            Clears s_jvmBroadcastVariables of all the JVMObjectReference objects of type
            <see cref="T:Microsoft.Spark.Broadcast`1"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.JvmBroadcastRegistry.GetAll">
            <summary>
            Returns the static member s_jvmBroadcastVariables.
            </summary>
            <returns>A list of all broadcast objects of type <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/></returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Internal.Java.Util.ArrayList">
            <summary>
            ArrayList class represents a <c>java.util.ArrayList</c> object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Java.Util.ArrayList.#ctor(Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Create a <c>java.util.ArrayList</c> JVM object
            </summary>
            <param name="jvm">JVM bridge to use</param>
        </member>
        <member name="T:Microsoft.Spark.Interop.Internal.Java.Util.Hashtable">
            <summary>
            Hashtable class represents a <c>java.util.Hashtable</c> object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Java.Util.Hashtable.#ctor(Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Create a <c>java.util.Hashtable</c> JVM object
            </summary>
            <param name="jvm">JVM bridge to use</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Java.Util.Hashtable.Put(System.Object,System.Object)">
            <summary>
            Maps the specified key to the specified value in this Hashtable.
            Neither the key nor the value can be null.
            </summary>
            <param name="key">The Hashtable key</param>
            <param name="value">The value</param>
        </member>
        <member name="T:Microsoft.Spark.Interop.Internal.Java.Util.Properties">
            <summary>
            Properties class represents a <c>java.util.Properties</c> object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Java.Util.Properties.#ctor(Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Create a <c>java.util.Properties</c> JVM object
            </summary>
            <param name="jvm">JVM bridge to use</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Java.Util.Properties.#ctor(Microsoft.Spark.Interop.Ipc.IJvmBridge,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Create a <c>java.util.Properties</c> JVM object and populate the entries
            using <paramref name="properties"/>
            </summary>
            <param name="jvm">JVM bridge to use</param>
            <param name="properties">Dictionary used to populate the
            <c>java.util.Properties</c> JVM object</param>
        </member>
        <member name="T:Microsoft.Spark.Interop.Internal.Scala.Option">
            <summary>
            Exposes subset of scala.Option[T] APIs.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Scala.Option.IsEmpty">
            <summary>
            Returns true if the option is None, false otherwise.
            </summary>
            <returns>true if the option is None, false otherwise</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Scala.Option.IsDefined">
            <summary>
            Returns true if the option is an instance of Some, false otherwise.
            </summary>
            <returns>true if the option is an instance of Some, false otherwise</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Scala.Option.Get">
            <summary>
            Returns the option's value as object type if the option is nonempty,
            otherwise throws an exception on JVM side.
            </summary>
            <returns>object that this Option is referencing to</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Internal.Scala.Option.OrNull">
            <summary>
            Returns the option's value if it is nonempty, or `null` if it is empty.
            </summary>
            <returns>object that this Option is referencing to</returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Internal.Scala.Seq`1">
            <summary>
            Limited read-only implementation of Scala Seq[T] so that Seq objects can be read
            into POCO collection types such as List.
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.CallbackConnection">
            <summary>
            CallbackConnection is used to process the callback communication between
            Dotnet and the JVM. It uses a TCP socket to communicate with the JVM side
            and the socket is expected to be reused.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackConnection._callbackHandlers">
            <summary>
            Keeps track of all <see cref="T:Microsoft.Spark.Interop.Ipc.ICallbackHandler"/>s by its Id. This is accessed
            by the <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackServer"/> and the <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/>.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackConnection.ConnectionStatus.OK">
            <summary>
            Connection is normal.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackConnection.ConnectionStatus.SOCKET_CLOSED">
            <summary>
            Socket is closed by the JVM.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackConnection.ConnectionStatus.REQUEST_CLOSE">
            <summary>
            Request to close connection.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackConnection.Run(System.Threading.CancellationToken)">
            <summary>
            Run and start processing the callback connection.
            </summary>
            <param name="token">Cancellation token used to stop the connection.</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackConnection.ProcessStream(System.IO.Stream,System.IO.Stream,System.Boolean@)">
            <summary>
            Process the input and output streams.
            </summary>
            <param name="inputStream">The input stream.</param>
            <param name="outputStream">The output stream.</param>
            <param name="readComplete">True if stream is read completely, false otherwise.</param>
            <returns>The connection status.</returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.CallbackFlags">
            <summary>
            Enums with which the Dotnet CallbackConnection communicates with
            the JVM CallbackConnection.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackFlags.CLOSE">
            <summary>
            Flag to indicate connection should be closed.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackFlags.CALLBACK">
            <summary>
            Flag to indiciate callback should be called.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackFlags.DOTNET_EXCEPTION_THROWN">
            <summary>
            Flag to indicate an exception thrown from dotnet.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackFlags.END_OF_STREAM">
            <summary>
            Flag to indicate end of stream.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.CallbackServer">
            <summary>
            CallbackServer services callback requests from the JVM.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackServer._callbackHandlers">
            <summary>
            Keeps track of all <see cref="T:Microsoft.Spark.Interop.Ipc.ICallbackHandler"/>s by its Id. This is accessed
            by the <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackServer"/> and the <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/>
            running in the worker threads.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackServer._connections">
            <summary>
            Keeps track of all <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/> objects identified by its
            <see cref="P:Microsoft.Spark.Interop.Ipc.CallbackConnection.ConnectionId"/>. The main thread creates a
            <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/> each time it receives a new socket connection
            from the JVM side and inserts it into <see cref="F:Microsoft.Spark.Interop.Ipc.CallbackServer._connections"/>. Each worker
            thread calls <see cref="M:Microsoft.Spark.Interop.Ipc.CallbackConnection.Run(System.Threading.CancellationToken)"/> and removes the connection
            once this call is finished. <see cref="M:Microsoft.Spark.Interop.Ipc.CallbackConnection.Run(System.Threading.CancellationToken)"/> will not return
            unless the <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/> needs to be closed.
            Also, <see cref="F:Microsoft.Spark.Interop.Ipc.CallbackServer._connections"/> is used to bound the number of worker threads
            since it gives you the total number of active <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/>s.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackServer._waitingConnections">
            <summary>
            Each worker thread picks up a CallbackConnection from _waitingConnections
            and runs it.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackServer._tokenSource">
            <summary>
            A <see cref="T:System.Threading.CancellationTokenSource"/> used to notify threads that operations
            should be canceled.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.CallbackServer._callbackCounter">
            <summary>
            Counter used to generate a unique id when registering a <see cref="T:Microsoft.Spark.Interop.Ipc.ICallbackHandler"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackServer.RegisterCallback(Microsoft.Spark.Interop.Ipc.ICallbackHandler)">
            <summary>
            Produce a unique id and register a <see cref="T:Microsoft.Spark.Interop.Ipc.ICallbackHandler"/> with it.
            </summary>
            <param name="callbackHandler">The handler to register.</param>
            <returns>A unique id associated with the handler.</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackServer.Run(Microsoft.Spark.Network.ISocketWrapper)">
            <summary>
            Runs the callback server.
            </summary>
            <param name="listener">The listening socket.</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackServer.Run">
            <summary>
            Runs the callback server.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackServer.StartServer(Microsoft.Spark.Network.ISocketWrapper)">
            <summary>
            Starts listening to any connection from JVM.
            </summary>
            <param name="listener"></param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackServer.RunWorkerThread">
            <summary>
            <see cref="M:Microsoft.Spark.Interop.Ipc.CallbackServer.RunWorkerThread"/> is called for each worker thread when it starts.
            <see cref="M:Microsoft.Spark.Interop.Ipc.CallbackServer.RunWorkerThread"/> doesn't return (except for the error cases), and
            keeps pulling from <see cref="F:Microsoft.Spark.Interop.Ipc.CallbackServer._waitingConnections"/> and runs the retrieved
            <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackConnection"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.CallbackServer.Shutdown">
            <summary>
            Shuts down the <see cref="T:Microsoft.Spark.Interop.Ipc.CallbackServer"/> by canceling any running threads
            and disposing of resources.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.ForeachBatchCallbackHandler">
            <summary>
            <see cref="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.ForeachBatch(System.Action{Microsoft.Spark.Sql.DataFrame,System.Int64})"/> callback handler.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.ICallbackHandler">
            <summary>
            Interface for handling callbacks between the JVM and Dotnet.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.IJvmBridge">
            <summary>
            Interface of the bridge between JVM and CLR.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JsonSerDe">
            <summary>
            Json.NET Serialization/Deserialization helper class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JsonSerDe.SortProperties(Newtonsoft.Json.Linq.JObject)">
            Note: Scala side uses JSortedObject when parsing JSON, so the properties
            in JObject need to be sorted.
            <summary>
            Extension method to sort items in a JSON object by keys.
            </summary>
            <param name="jObject"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JsonSerDe.SortProperties(Newtonsoft.Json.Linq.JArray)">
            <summary>
            Extend method to sort items in a JSON array by keys.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JvmBridge">
            <summary>
            Implementation of thread safe IPC bridge between JVM and CLR
            Using a concurrent socket connection queue (lightweight synchronization mechanism)
            supporting async JVM calls like StreamingContext.AwaitTermination()
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JvmObjectId">
            <summary>
            JvmObjectId represents the unique owner for a JVM object.
            The reason for having another layer on top of string id is
            so that JvmObjectReference can be copied.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.#ctor(System.String,Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Constructor for JvmObjectId class.
            </summary>
            <param name="id">Unique identifier</param>
            <param name="jvm">JVM bridge object</param>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectId.Id">
            <summary>
            An unique identifier for an object created on the JVM.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectId.Jvm">
            <summary>
            JVM bridge object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.op_Implicit(Microsoft.Spark.Interop.Ipc.JvmObjectId)~System.String">
            <summary>
            Implicit conversion to string.
            </summary>
            <param name="jvmObjectId">JvmObjectId to convert from</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.ToString">
            <summary>
            Returns the string version of this object which is the unique id
            of the JVM object.
            </summary>
            <returns>Id of the JVM object</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object by comparing the id.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectId.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.IJvmObjectReferenceProvider">
            <summary>
            Implemented by objects that contain a <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/>.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.IJvmObjectReferenceProvider.Reference">
            <summary>
            Gets the <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/> wrapped by the object.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference">
            <summary>
            Reference to object created in JVM.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.JvmObjectReference._creationTime">
            <summary>
            The time when this reference was created.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.#ctor(System.String,Microsoft.Spark.Interop.Ipc.IJvmBridge)">
            <summary>
            Constructor for the JvmObjectReference class.
            </summary>
            <param name="id">Id for the JVM object</param>
            <param name="jvm">IJvmBridge instance that created the JVM object</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Copy constructor.
            </summary>
            <param name="other">Other JvmObjectReference object to copy from.</param>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Id">
            <summary>
            An unique identifier for an object created on the JVM.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Jvm">
            <summary>
            IJvmBridge instance that created the JVM object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(System.String,System.Object)">
            <summary>
            Invokes a method on the JVM object that this JvmObjectReference references to.
            </summary>
            <param name="arg0">Parameter for the method.</param>
            <param name="methodName">Method name to invoke</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(System.String,System.Object,System.Object)">
            <summary>
            Invokes a method on the JVM object that this JvmObjectReference references to.
            </summary>
            <param name="arg0">First parameter for the method.</param>
            <param name="arg1">Second parameter for the method.</param>
            <param name="methodName">Method name to invoke</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Invoke(System.String,System.Object[])">
            <summary>
            Invokes a method on the JVM object that this JvmObjectReference references to.
            </summary>
            <param name="methodName">Method name to invoke</param>
            <param name="args">Parameters for the method</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.ToString">
            <summary>
            Returns the string version of this object which is the unique id
            of the JVM object.
            </summary>
            <returns>Id of the JVM object</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object by comparing the ids.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.JvmObjectReference.GetDebugInfo">
            <summary>
            Gets the debug info on the JVM object that the current object refers to.
            </summary>
            <returns>The debug info of the JVM object</returns>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.PayloadHelper">
            <summary>
            Helper to build the IPC payload for JVM calls from CLR.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.SpecialLengths">
            <summary>
            Enums with which Worker communicates with Spark.
            See spark/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.END_OF_DATA_SECTION">
            <summary>
            Flag to indicate the end of data section
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.PYTHON_EXCEPTION_THROWN">
            <summary>
            Flag to indicate an exception thrown from .NET side
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.TIMING_DATA">
            <summary>
            Flag to indicate a timing data
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.END_OF_STREAM">
            <summary>
            Flag to indicate the end of stream
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.NULL">
            <summary>
            Flag to indicate non-defined type
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Interop.Ipc.SpecialLengths.START_ARROW_STREAM">
            <summary>
            Flag used by PySpark only.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Interop.Ipc.SerDe">
            <summary>
            Serialization and Deserialization of data types between JVM and CLR
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadBool(System.IO.Stream)">
            <summary>
            Reads a boolean from a stream.
            </summary>
            <param name="s">The stream to read</param>
            <returns>The boolean value read from the stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadInt32(System.IO.Stream)">
            <summary>
            Reads an integer from a stream.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The integer read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadInt64(System.IO.Stream)">
            <summary>
            Reads a long integer from a stream.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The long integer read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadDouble(System.IO.Stream)">
            <summary>
            Reads a double from a stream.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The double read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadString(System.IO.Stream)">
            <summary>
            Reads a string from a stream
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The string read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadString(System.IO.Stream,System.Int32)">
            <summary>
            Reads a string with a given length from a stream
            </summary>
            <param name="s">The stream to be read</param>
            <param name="length">The length to be read</param>
            <returns>The string read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadBytes(System.IO.Stream,System.Int32)">
            <summary>
            Reads a byte array with a given length from a stream
            </summary>
            <param name="s">The stream to be read</param>
            <param name="length">The length to be read</param>
            <returns>The a byte array read from stream</returns>
            <exception cref="T:System.ArgumentOutOfRangeException">
            An ArgumentOutOfRangeException thrown if the given length is negative
            </exception>
            <exception cref="T:System.ArgumentException">
            An ArgumentException if the actual read length is less than the given length
            </exception>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.ReadBytes(System.IO.Stream)">
            <summary>
            Reads a byte array from a stream. The first 4 bytes indicate the length of a byte array.
            </summary>
            <param name="s">The stream to be read</param>
            <returns>The byte array read from stream</returns>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Byte)">
            <summary>
            Writes a byte to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The byte to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Byte[])">
            <summary>
            Writes a byte array to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The byte array to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Byte[],System.Int32)">
            <summary>
            Writes a byte array to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The byte array to write</param>
            <param name="count">The number of bytes in the array to write.</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Boolean)">
            <summary>
            Writes a boolean to a stream
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The boolean value to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Int32)">
            <summary>
            Writes an integer to a stream (big-endian).
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The integer to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Int64)">
            <summary>
            Writes a long integer to a stream (big-endian).
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The long integer to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.Double)">
            <summary>
            Writes a double to a stream (big-endian).
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The double to write</param>
        </member>
        <member name="M:Microsoft.Spark.Interop.Ipc.SerDe.Write(System.IO.Stream,System.String)">
            <summary>
            Writes a string to a stream.
            </summary>
            <param name="s">The stream to write</param>
            <param name="value">The string to write</param>
        </member>
        <member name="T:Microsoft.Spark.Interop.SparkEnvironment">
            <summary>
            Contains everything needed to setup an environment for using .NET with Spark.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.JvmException">
            <summary>
            Contains the message returned from the <see cref="T:Microsoft.Spark.Interop.Ipc.JvmBridge"/> on an error.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.ML.Feature.Bucketizer">
            <summary>
            <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> maps a column of continuous features to a column of feature
            buckets.
            
            <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> can map multiple columns at once by setting the inputCols
            parameter. Note that when both the inputCol and inputCols parameters are set, an Exception
            will be thrown. The splits parameter is only used for single column usage, and splitsArray
            is for multiple columns.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> without any parameters
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> a unique ID
            </summary>
            <param name="uid">An immutable unique ID for the object and its derivatives.</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetSplits">
            <summary>
            Gets the splits that were set using SetSplits
            </summary>
            <returns>double[], the splits to be used to bucket the input column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetSplits(System.Double[])">
            <summary>
            Split points for splitting a single column into buckets. To split multiple columns use
            SetSplitsArray. You cannot use both SetSplits and SetSplitsArray at the same time
            </summary>
            <param name="value">
            Split points for mapping continuous features into buckets. With n+1 splits, there are n
            buckets. A bucket defined by splits x,y holds values in the range [x,y) except the last
            bucket, which also includes y. The splits should be of length &gt;= 3 and strictly
            increasing. Values outside the splits specified will be treated as errors.
            </param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetSplitsArray">
            <summary>
            Gets the splits that were set by SetSplitsArray
            </summary>
            <returns>double[][], the splits to be used to bucket the input columns</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetSplitsArray(System.Double[][])">
            <summary>
            Split points fot splitting multiple columns into buckets. To split a single column use
            SetSplits. You cannot use both SetSplits and SetSplitsArray at the same time.
            </summary>
            <param name="value">
            The array of split points for mapping continuous features into buckets for multiple 
            columns. For each input column, with n+1 splits, there are n buckets. A bucket defined
            by splits x,y holds values in the range [x,y) except the last bucket, which also
            includes y. The splits should be of length &gt;= 3 and strictly increasing.
            Values outside the splits specified will be treated as errors.</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetInputCol">
            <summary>
            Gets the column that the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> should read from and convert into
            buckets. This would have been set by SetInputCol
            </summary>
            <returns>string, the input column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetInputCol(System.String)">
            <summary>
            Sets the column that the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> should read from and convert into
            buckets
            </summary>
            <param name="value">The name of the column to as the source of the buckets</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetInputCols">
            <summary>
            Gets the columns that <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> should read from and convert into
            buckets. This is set by SetInputCol
            </summary>
            <returns>IEnumerable&lt;string&gt;, list of input columns</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetInputCols(System.Collections.Generic.IEnumerable{System.String})">
             <summary>
             Sets the columns that <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> should read from and convert into
             buckets.
            
             Each column is one set of buckets so if you have two input columns you can have two
             sets of buckets and two output columns.
             </summary>
             <param name="value">List of input columns to use as sources for buckets</param>
             <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetOutputCol">
            <summary>
            Gets the name of the column the output data will be written to. This is set by
            SetInputCol
            </summary>
            <returns>string, the output column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetOutputCol(System.String)">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <param name="value">The name of the new column which contains the bucket ID</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetOutputCols">
            <summary>
            The list of columns that the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> will create in the DataFrame.
            This is set by SetOutputCols
            </summary>
            <returns>IEnumerable&lt;string&gt;, list of output columns</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetOutputCols(System.Collections.Generic.List{System.String})">
            <summary>
            The list of columns that the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> will create in the DataFrame.
            </summary>
            <param name="value">List of column names which will contain the bucket ID</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> that was previously saved using Save
            </summary>
            <param name="path">The path the previous <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> was saved to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> so that it can be loaded later using Load
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.Transform(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Executes the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> and transforms the DataFrame to include the new
            column or columns with the bucketed data.
            </summary>
            <param name="source">The DataFrame to add the bucketed data to</param>
            <returns>
            <see cref="T:Microsoft.Spark.Sql.DataFrame"/> containing the original data and the new bucketed columns
            </returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.Uid">
            <summary>
            The uid that was used to create the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/>. If no UID is passed in
            when creating the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> then a random UID is created when the
            <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> is created.
            </summary>
            <returns>string UID identifying the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.GetHandleInvalid">
            <summary>
            How should the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> handle invalid data, choices are "skip",
            "error" or "keep"
            </summary>
            <returns>string showing the way Spark will handle invalid data</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Bucketizer.SetHandleInvalid(System.String)">
             <summary>
             Tells the <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> what to do with invalid data.
            
             Choices are "skip", "error" or "keep". Default is "error"
             </summary>
             <param name="value">"skip", "error" or "keep"</param>
             <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Bucketizer"/> object</returns>
        </member>
        <member name="T:Microsoft.Spark.ML.Feature.HashingTF">
            <summary>
            A <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> Maps a sequence of terms to their term frequencies using the
            hashing trick. Currently we use Austin Appleby's MurmurHash 3 algorithm
            (MurmurHash3_x86_32) to calculate the hash code value for the term object. Since a simple
            modulo is used to transform the hash function to a column index, it is advisable to use a
            power of two as the numFeatures parameter; otherwise the features will not be mapped evenly
            to the columns.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> without any parameters
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> a unique ID
            <param name="uid">unique identifier</param>
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> that was previously saved using Save
            </summary>
            <param name="path">The path the previous <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> was saved to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> so that it can be loaded later using Load
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.GetBinary">
            <summary>
            Gets the binary toggle that controls term frequency counts
            </summary>
            <returns>Flag showing whether the binary toggle is on or off</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.SetBinary(System.Boolean)">
             <summary>
             Binary toggle to control term frequency counts.
             If true, all non-zero counts are set to 1.  This is useful for discrete probabilistic
             models that model binary events rather than integer counts
            </summary>
             <param name="value">binary toggle, default is false</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.GetInputCol">
            <summary>
            Gets the column that the <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> should read from
            </summary>
            <returns>string, the name of the input column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.SetInputCol(System.String)">
            <summary>
            Sets the column that the <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> should read from
            </summary>
            <param name="value">The name of the column to as the source</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.GetOutputCol">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> will create a new column in the <see cref="T:Microsoft.Spark.Sql.DataFrame"/>,
            this is the name of the new column.
            </summary>
            <returns>string, the name of the output col</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.SetOutputCol(System.String)">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> will create a new column in the <see cref="T:Microsoft.Spark.Sql.DataFrame"/>,
            this is the name of the new column.
            </summary>
            <param name="value">The name of the new column</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.GetNumFeatures">
            <summary>
            Gets the number of features that should be used. Since a simple modulo is used to
            transform the hash function to a column index, it is advisable to use a power of two
            as the numFeatures parameter; otherwise the features will not be mapped evenly to the
            columns.
            </summary>
            <returns>The number of features to be used</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.SetNumFeatures(System.Int32)">
            <summary>
            Sets the number of features that should be used. Since a simple modulo is used to
            transform the hash function to a column index, it is advisable to use a power of two as
            the numFeatures parameter; otherwise the features will not be mapped evenly to the
            columns.
            </summary>
            <param name="value">int</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.Uid">
            <summary>
            An immutable unique ID for the object and its derivatives.
            </summary>
            <returns>string, unique ID for the object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.HashingTF.Transform(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Executes the <see cref="T:Microsoft.Spark.ML.Feature.HashingTF"/> and transforms the DataFrame to include the new
            column or columns with the tokens.
            </summary>
            <param name="source">The <see cref="T:Microsoft.Spark.Sql.DataFrame"/> to add the tokens to</param>
            <returns><see cref="T:Microsoft.Spark.Sql.DataFrame"/> containing the original data and the tokens</returns>
        </member>
        <member name="T:Microsoft.Spark.ML.Feature.IDF">
            <summary>
            Inverse document frequency (IDF). The standard formulation is used:
            idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is
            the number of documents that contain term t.
            
            This implementation supports filtering out terms which do not appear in a minimum number
            of documents (controlled by the variable minDocFreq). For terms that are not in at least
            minDocFreq documents, the IDF is found as 0, resulting in TF-IDFs of 0.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> without any parameters
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> a unique ID
            </summary>
            <param name="uid">An immutable unique ID for the object and its derivatives.</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.GetInputCol">
            <summary>
            Gets the column that the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> should read from
            </summary>
            <returns>string, input column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.SetInputCol(System.String)">
            <summary>
            Sets the column that the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> should read from
            </summary>
            <param name="value">The name of the column to as the source</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.GetOutputCol">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <returns>string, the output column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.SetOutputCol(System.String)">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <param name="value">The name of the new column</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.GetMinDocFreq">
            <summary>
            Minimum of documents in which a term should appear for filtering
            </summary>
            <returns>int, minimum number of documents in which a term should appear</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.SetMinDocFreq(System.Int32)">
            <summary>
            Minimum of documents in which a term should appear for filtering
            </summary>
            <param name="value">int, the minimum of documents a term should appear in</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.Fit(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Fits a model to the input data.
            </summary>
            <param name="source">The <see cref="T:Microsoft.Spark.Sql.DataFrame"/> to fit the model to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.Uid">
            <summary>
            The uid that was used to create the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/>. If no UID is passed in
            when creating the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> then a random UID is created when the
            <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> is created.
            </summary>
            <returns>string UID identifying the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> that was previously saved using Save
            </summary>
            <param name="path">The path the previous <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> was saved to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> object, loaded from path</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDF.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> so that it can be loaded later using Load
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDF"/> object</returns>
        </member>
        <member name="T:Microsoft.Spark.ML.Feature.IDFModel">
            <summary>
            A <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> that converts the input string to lowercase and then splits it by
            white spaces.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> without any parameters
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> a unique ID
            </summary>
            <param name="uid">An immutable unique ID for the object and its derivatives.</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.GetInputCol">
            <summary>
            Gets the column that the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> should read from
            </summary>
            <returns>string, input column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.SetInputCol(System.String)">
            <summary>
            Sets the column that the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> should read from and convert into
            buckets
            </summary>
            <param name="value">The name of the column to as the source</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.GetOutputCol">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> will create a new column in the <see cref="T:Microsoft.Spark.Sql.DataFrame"/>,
            this is the name of the new column.
            </summary>
            <returns>string, the output column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.SetOutputCol(System.String)">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <param name="value">The name of the new column which contains the tokens
            </param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.GetMinDocFreq">
            <summary>
            Minimum of documents in which a term should appear for filtering
            </summary>
            <returns>Minimum number of documents a term should appear</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.Transform(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Executes the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> and transforms the <see cref="T:Microsoft.Spark.Sql.DataFrame"/> to
            include the new column or columns with the tokens.
            </summary>
            <param name="source">The <see cref="T:Microsoft.Spark.Sql.DataFrame"/> to add the tokens to</param>
            <returns><see cref="T:Microsoft.Spark.Sql.DataFrame"/> containing the original data and the tokens</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.Uid">
            <summary>
            The uid that was used to create the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/>. If no UID is passed in
            when creating the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> then a random UID is created when the
            <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> is created.
            </summary>
            <returns>string UID identifying the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> that was previously saved using Save
            </summary>
            <param name="path">The path the previous <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> was saved to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> object, loaded from path</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.IDFModel.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> so that it can be loaded later using Load
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.IDFModel"/> object</returns>
        </member>
        <member name="T:Microsoft.Spark.ML.Feature.Tokenizer">
            <summary>
            A <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> that converts the input string to lowercase and then splits it by
            white spaces.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> without any parameters
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> a unique ID
            </summary>
            <param name="uid">An immutable unique ID for the object and its derivatives.</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.GetInputCol">
            <summary>
            Gets the column that the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> should read from
            </summary>
            <returns>string, input column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.SetInputCol(System.String)">
            <summary>
            Sets the column that the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> should read from
            </summary>
            <param name="value">The name of the column to as the source</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.GetOutputCol">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <returns>string, the output column</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.SetOutputCol(System.String)">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <param name="value">The name of the new column</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.Transform(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Executes the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> and transforms the DataFrame to include the new
            column
            </summary>
            <param name="source">The DataFrame to transform</param>
            <returns>
            New <see cref="T:Microsoft.Spark.Sql.DataFrame"/> object with the source <see cref="T:Microsoft.Spark.Sql.DataFrame"/> transformed
            </returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.Uid">
            <summary>
            The uid that was used to create the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/>. If no UID is passed in
            when creating the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> then a random UID is created when the
            <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> is created.
            </summary>
            <returns>string UID identifying the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> that was previously saved using Save
            </summary>
            <param name="path">The path the previous <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> was saved to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> object, loaded from path</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Tokenizer.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> so that it can be loaded later using Load
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Tokenizer"/> object</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> without any parameters. Once you have created a
            <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> you must call <see cref="M:Microsoft.Spark.ML.Feature.Word2Vec.SetInputCol(System.String)"/>,
            <see cref="M:Microsoft.Spark.ML.Feature.Word2Vec.SetOutputCol(System.String)"/>, and <see cref="M:Microsoft.Spark.ML.Feature.Word2Vec.SetMinCount(System.Int32)"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> a unique ID.
            </summary>
            <param name="uid">An immutable unique ID for the object and its derivatives.</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetInputCol">
            <summary>
            Gets the column that the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> should read from.
            </summary>
            <returns>The name of the input column.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetInputCol(System.String)">
            <summary>
            Sets the column that the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> should read from.
            </summary>
            <param name="value">The name of the column to as the source.</param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetOutputCol">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <returns>The name of the output column.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetOutputCol(System.String)">
            <summary>
            The <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> will create a new column in the DataFrame, this is the
            name of the new column.
            </summary>
            <param name="value">The name of the output column which will be created.</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetVectorSize">
            <summary>
            Gets the vector size, the dimension of the code that you want to transform from words.
            </summary>
            <returns>
            The vector size, the dimension of the code that you want to transform from words.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetVectorSize(System.Int32)">
            <summary>
            Sets the vector size, the dimension of the code that you want to transform from words.
            </summary>
            <param name="value">
            The dimension of the code that you want to transform from words.
            </param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetMinCount">
            <summary>
            Gets the minimum number of times a token must appear to be included in the word2vec
            model's vocabulary.
            </summary>
            <returns>
            The minimum number of times a token must appear to be included in the word2vec model's
            vocabulary.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetMinCount(System.Int32)">
            <summary>
            The minimum number of times a token must appear to be included in the word2vec model's
            vocabulary.
            </summary>
            <param name="value">
            The minimum number of times a token must appear to be included in the word2vec model's
            vocabulary, the default is 5.
            </param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetMaxIter">
            <summary>Gets the maximum number of iterations.</summary>
            <returns>The maximum number of iterations.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetMaxIter(System.Int32)">
            <summary>Maximum number of iterations (&gt;= 0).</summary>
            <param name="value">The number of iterations.</param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetMaxSentenceLength">
            <summary>
            Gets the maximum length (in words) of each sentence in the input data.
            </summary>
            <returns>The maximum length (in words) of each sentence in the input data.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetMaxSentenceLength(System.Int32)">
            <summary>
            Sets the maximum length (in words) of each sentence in the input data.
            </summary>
            <param name="value">
            The maximum length (in words) of each sentence in the input data.
            </param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetNumPartitions">
            <summary>Gets the number of partitions for sentences of words.</summary>
            <returns>The number of partitions for sentences of words.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetNumPartitions(System.Int32)">
            <summary>Sets the number of partitions for sentences of words.</summary>
            <param name="value">
            The number of partitions for sentences of words, default is 1.
            </param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetSeed">
            <summary>Gets the value that is used for the random seed.</summary>
            <returns>The value that is used for the random seed.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetSeed(System.Int64)">
            <summary>Random seed.</summary>
            <param name="value">The value to use for the random seed.</param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetStepSize">
            <summary>Gets the size to be used for each iteration of optimization.</summary>
            <returns>The size to be used for each iteration of optimization.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetStepSize(System.Double)">
            <summary>Step size to be used for each iteration of optimization (&gt; 0).</summary>
            <param name="value">Value to use for the step size.</param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.GetWindowSize">
            <summary>Gets the window size (context words from [-window, window]).</summary>
            <returns>The window size.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.SetWindowSize(System.Int32)">
            <summary>The window size (context words from [-window, window]).</summary>
            <param name="value">
            The window size (context words from [-window, window]), default is 5.
            </param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.Fit(Microsoft.Spark.Sql.DataFrame)">
            <summary>Fits a model to the input data.</summary>
            <param name="dataFrame">The <see cref="T:Microsoft.Spark.Sql.DataFrame"/> to fit the model to.</param>
            <returns><see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/></returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.Uid">
            <summary>
            The uid that was used to create the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/>. If no UID is passed in
            when creating the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> then a random UID is created when the
            <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> is created.
            </summary>
            <returns>string UID identifying the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/>.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> that was previously saved using
            <see cref="M:Microsoft.Spark.ML.Feature.Word2Vec.Save(System.String)"/>.
            </summary>
            <param name="path">The path the previous <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> was saved to</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> object, loaded from path.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2Vec.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> so that it can be loaded later using
            <see cref="M:Microsoft.Spark.ML.Feature.Word2Vec.Load(System.String)"/>.
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> to.</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> object.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.#ctor">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> without any parameters
            </summary>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.#ctor(System.String)">
            <summary>
            Create a <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> with a UID that is used to give the
            <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> a unique ID
            </summary>
            <param name="uid">An immutable unique ID for the object and its derivatives.</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.Transform(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Transform a sentence column to a vector column to represent the whole sentence.
            </summary>
            <param name="documentDF"><see cref="T:Microsoft.Spark.Sql.DataFrame"/> to transform</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.FindSynonyms(System.String,System.Int32)">
            <summary>
            Find <paramref name="num"/> number of words whose vector representation most similar to
            the supplied vector. If the supplied vector is the vector representation of a word in
            the model's vocabulary, that word will be in the results. Returns a dataframe with the
            words and the cosine similarities between the synonyms and the given word vector.
            </summary>
            <param name="word">The "word" to find similarities for, this can be a string or a
            vector representation.</param>
            <param name="num">The number of words to find that are similar to "word"</param>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.Load(System.String)">
            <summary>
            Loads the <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> that was previously saved using
            <see cref="M:Microsoft.Spark.ML.Feature.Word2VecModel.Save(System.String)"/>.
            </summary>
            <param name="path">
            The path the previous <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> was saved to
            </param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> object, loaded from path.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.Save(System.String)">
            <summary>
            Saves the <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> so that it can be loaded later using
            <see cref="M:Microsoft.Spark.ML.Feature.Word2VecModel.Load(System.String)"/>.
            </summary>
            <param name="path">The path to save the <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> to.</param>
            <returns>New <see cref="T:Microsoft.Spark.ML.Feature.Word2VecModel"/> object.</returns>
        </member>
        <member name="M:Microsoft.Spark.ML.Feature.Word2VecModel.Uid">
            <summary>
            The UID that was used to create the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/>. If no UID is passed in
            when creating the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> then a random UID is created when the
            <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/> is created.
            </summary>
            <returns>string UID identifying the <see cref="T:Microsoft.Spark.ML.Feature.Word2Vec"/>.</returns>
        </member>
        <member name="T:Microsoft.Spark.Network.DefaultSocketWrapper">
            <summary>
            A simple wrapper of System.Net.Sockets.Socket class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.#ctor">
            <summary>
            Default constructor that creates a new instance of DefaultSocket class which represents
            a traditional socket (System.Net.Socket.Socket).
            
            This socket is bound to Loopback with port 0.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.#ctor(System.Net.Sockets.Socket)">
            <summary>
            Initializes a instance of DefaultSocket class using the specified
            System.Net.Socket.Socket object.
            </summary>
            <param name="socket">The existing socket</param>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Dispose">
            <summary>
            Releases all resources used by the current instance of the DefaultSocket class.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Accept">
            <summary>
            Accepts a incoming connection request.
            </summary>
            <returns>A DefaultSocket instance used to send and receive data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Connect(System.Net.IPAddress,System.Int32,System.String)">
            <summary>
            Establishes a connection to a remote host that is specified by an IP address and
            a port number.
            </summary>
            <param name="remoteaddr">The IP address of the remote host</param>
            <param name="port">The port number of the remote host</param>
            <param name="secret">Secret string to use for connection</param>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.CreateNetworkStream">
            <summary>
            Returns the NetworkStream used to send and receive data.
            </summary>
            <remarks>
            GetStream returns a NetworkStream that you can use to send and receive data.
            You must close/dispose the NetworkStream by yourself. Closing DefaultSocketWrapper
            does not release the NetworkStream.
            </remarks>
            <returns>The underlying Stream instance that be used to send and receive data</returns>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.InputStream">
            <summary>
            Returns a stream used to receive data only.
            </summary>
            <returns>The underlying Stream instance that be used to receive data</returns>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.OutputStream">
            <summary>
            Returns a stream used to send data only.
            </summary>
            <returns>The underlying Stream instance that be used to send data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.DefaultSocketWrapper.Listen(System.Int32)">
            <summary>
            Starts listening for incoming connections requests
            </summary>
            <param name="backlog">The maximum length of the pending connections queue. </param>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.LocalEndPoint">
            <summary>
            Returns the local endpoint.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Network.DefaultSocketWrapper.RemoteEndPoint">
            <summary>
            Returns the remote endpoint.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Network.ISocketWrapper">
            <summary>
            ISocketWrapper interface defines the common methods to operate a socket.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.ISocketWrapper.Accept">
            <summary>
            Accepts a incoming connection request.
            </summary>
            <returns>A ISocket instance used to send and receive data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.ISocketWrapper.Connect(System.Net.IPAddress,System.Int32,System.String)">
            <summary>
            Establishes a connection to a remote host that is specified by an IP address and
            a port number.
            </summary>
            <param name="remoteaddr">The IP address of the remote host</param>
            <param name="port">The port number of the remote host</param>
            <param name="secret">Optional secret string to use for connection</param>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.InputStream">
            <summary>
            Returns a stream used to receive data only.
            </summary>
            <returns>The underlying Stream instance that be used to receive data</returns>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.OutputStream">
            <summary>
            Returns a stream used to send data only.
            </summary>
            <returns>The underlying Stream instance that be used to send data</returns>
        </member>
        <member name="M:Microsoft.Spark.Network.ISocketWrapper.Listen(System.Int32)">
            <summary>
            Starts listening for incoming connections requests
            </summary>
            <param name="backlog">The maximum length of the pending connections queue</param>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.LocalEndPoint">
            <summary>
            Returns the local endpoint.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Network.ISocketWrapper.RemoteEndPoint">
            <summary>
            Returns the remote endpoint.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Network.SocketFactory">
            <summary>
            SocketFactory is used to create ISocketWrapper instance.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Network.SocketFactory.CreateSocket">
            <summary>
            Creates an ISocket instance based on the socket type set.
            </summary>
            <returns>
            ISocketWrapper instance.
            </returns>
        </member>
        <member name="T:Microsoft.Spark.PairRDDFunctions">
            <summary>
            Extra functions available on RDDs of (key, value) pairs through extension methods.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.PairRDDFunctions.CollectAsMap``2(Microsoft.Spark.RDD{System.Tuple{``0,``1}})">
            <summary>
            Returns the key-value pairs in this RDD as a dictionary.
            </summary>
            <typeparam name="TKey">Type of the key</typeparam>
            <typeparam name="TValue">Type of the value</typeparam>
            <param name="self">RDD object to apply</param>
            <returns>Dictionary of RDD content</returns>
        </member>
        <member name="M:Microsoft.Spark.PairRDDFunctions.Keys``2(Microsoft.Spark.RDD{System.Tuple{``0,``1}})">
            <summary>
            Return an RDD with the keys of each tuple.
            </summary>
            <typeparam name="TKey">Type of the key</typeparam>
            <typeparam name="TValue">Type of the value</typeparam>
            <param name="self">RDD object to apply</param>
            <returns>RDD with the keys of each tuple</returns>
        </member>
        <member name="M:Microsoft.Spark.PairRDDFunctions.Values``2(Microsoft.Spark.RDD{System.Tuple{``0,``1}})">
            <summary>
            Return an RDD with the values of each tuple.
            </summary>
            <typeparam name="TKey">Type of the key</typeparam>
            <typeparam name="TValue">Type of the value</typeparam>
            <param name="self">RDD object to apply</param>
            <returns>RDD with the values of each tuple</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD`1">
            <summary>
            A Resilient Distributed Dataset(RDD), the basic abstraction in Spark,
            represents an immutable, partitioned collection of elements that can be
            operated on in parallel. This class contains the basic operations available
            on all RDDs.
            </summary>
            <typeparam name="T">Type of the elements in the RDD</typeparam>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._jvmObject">
            <summary>
            The JVM object for this RDD. This can be null if the current
            RDD is not materialized.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._prevRddJvmObjRef">
            <summary>
            The previous RDD object that this RDD references to. This is
            used by <see cref="T:Microsoft.Spark.PipelinedRDD`1"/> to chain RDD operations.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._sparkContext">
            <summary>
            SparkContext object associated with the RDD.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._isCached">
            <summary>
            Flag that checks if <see cref="M:Microsoft.Spark.RDD`1.Cache"/> is called.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._isCheckpointed">
            <summary>
            Flag that checks if <see cref="M:Microsoft.Spark.RDD`1.Checkpoint"/> is called.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._serializedMode">
            <summary>
            Serialization mode for the current RDD. This will be
            translated into serialization mode while creating a serialized command.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.RDD`1._prevSerializedMode">
            <summary>
            Serialization mode for the previously pipelined RDD. This will be
            translated into deserialization mode while creating a serialized command.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference,Microsoft.Spark.SparkContext,Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Constructor mainly called by SparkContext for creating the first RDD
            via <see cref="M:Microsoft.Spark.SparkContext.Parallelize``1(System.Collections.Generic.IEnumerable{``0},System.Nullable{System.Int32})"/>, etc.
            </summary>
            <param name="jvmObject">The reference to the RDD JVM object</param>
            <param name="sparkContext">SparkContext object</param>
            <param name="serializedMode">Serialization mode for the current RDD</param>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference,Microsoft.Spark.SparkContext,Microsoft.Spark.Utils.CommandSerDe.SerializedMode,Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Constructor mainly called by <see cref="T:Microsoft.Spark.PipelinedRDD`1"/>.
            </summary>
            <param name="prevRddJvmObjRef">
            The reference to the RDD JVM object from which pipeline is created
            </param>
            <param name="sparkContext">SparkContext object</param>
            <param name="serializedMode">Serialization mode for the current RDD</param>
            <param name="prevSerializedMode">Serialization mode for the previous RDD</param>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Cache">
            <summary>
            Persist this RDD with the default storage level (MEMORY_ONLY).
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Checkpoint">
            <summary>
            Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
            directory set with <see cref="M:Microsoft.Spark.SparkContext.SetCheckpointDir(System.String)"/> and all
            references to its parent RDDs will be removed. This function must be called before
            any job has been executed on this RDD. It is strongly recommended that this RDD is
            persisted in memory, otherwise saving it in a file will require re-computation.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Map``1(System.Func{`0,``0},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to all elements of this RDD.
            </summary>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.FlatMap``1(System.Func{`0,System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by first applying a function to all elements of this RDD,
            and then flattening the results.
            </summary>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.MapPartitions``1(System.Func{System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD.
            </summary>
            
            <remarks>
            "preservesPartitioning" indicates whether the input function preserves the
            partitioner, which should be false unless this is a pair RDD and the input
            function doesn't modify the keys.
            </remarks>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.MapPartitionsWithIndex``1(System.Func{System.Int32,System.Collections.Generic.IEnumerable{`0},System.Collections.Generic.IEnumerable{``0}},System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD,
            while tracking the index of the original partition.
            </summary>
            <remarks>
            "preservesPartitioning" indicates whether the input function preserves the
            partitioner, which should be false unless this is a pair RDD and the input
            function doesn't modify the keys.
            </remarks>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.GetNumPartitions">
            <summary>
            Return the number of partitions in this RDD.
            </summary>
            <returns>The number of partitions in this RDD</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Filter(System.Func{`0,System.Boolean})">
            <summary>
            Return a new RDD containing only the elements that satisfy a predicate.
            </summary>
            <param name="func">Predicate function to apply</param>
            <returns>A new RDD with elements that satisfy a predicate</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Sample(System.Boolean,System.Double,System.Nullable{System.Int64})">
            <summary>
            Return a sampled subset of this RDD with a seed.
            </summary>
            <remarks>
            This is NOT guaranteed to provide exactly the fraction of the count
            of the given RDD.
            </remarks>
            <param name="withReplacement">True if elements be sampled multiple times</param>
            <param name="fraction">
            Expected size of the sample as a fraction of this RDD's size without replacement
            </param>
            <param name="seed">Optional user-supplied seed (random seed if not provided)</param>
            <returns>A sampled subset of this RDD</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.Collect">
            <summary>
            Return an enumerable collection that contains all of the elements in this RDD.
            </summary>
            <remarks>
            This method should only be used if the resulting array is expected to be small,
            as all the data is loaded into the driver's memory.
            </remarks>
            <returns>An enumerable collection of all the elements.</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.MapPartitionsWithIndexInternal``1(Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate,System.Boolean)">
            <summary>
            Helper function for creating PipelinedRDD.
            </summary>
            <typeparam name="U">Type of the new RDD elements</typeparam>
            <param name="func">Function to apply</param>
            <param name="preservesPartitioning">Flag to preserve partitioning</param>
            <returns>New RDD by applying a given function</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.CollectAndServe">
            <summary>
            Returns the socket info by calling collectAndServe on the RDD object.
            </summary>
            <returns>Socket info</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD`1.GetJvmRef">
            <summary>
            Returns the JvmObjectReference object of this RDD.
            </summary>
            <remarks>
            It is possible that the JvmObjectReference object is null depending
            on how the RDD object is instantiated (e.g., <see cref="T:Microsoft.Spark.PipelinedRDD`1"/>). Thus,
            this function should be used instead of directly accessing _jvmObject because the
            derived class (e.g., <see cref="T:Microsoft.Spark.PipelinedRDD`1"/>) can override the behavior
            when _jvmObject is null.
            </remarks>
            <returns>JvmObjetReference object for this RDD</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.MapUdfWrapper`2">
            <summary>
            Helper to map the UDF for Map() to <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.FlatMapUdfWrapper`2">
            <summary>
            Helper to map the UDF for FlatMap() to <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.MapPartitionsUdfWrapper`2">
            <summary>
            Helper to map the UDF for MapPartitions() to
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.MapPartitionsWithIndexUdfWrapper`2">
            <summary>
            Helper to map the UDF for MapPartitionsWithIndex() to
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
            <typeparam name="TArg">Input type</typeparam>
            <typeparam name="TResult">Output type</typeparam>
        </member>
        <member name="T:Microsoft.Spark.RDD`1.FilterUdfWrapper">
            <summary>
            Helper to map the UDF for Filter() to
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.PipelinedRDD`1">
            <summary>
            PipelinedRDD is used to pipeline functions applied to RDD.
            </summary>
            <typeparam name="T">Type of the elements in the RDD</typeparam>
        </member>
        <member name="M:Microsoft.Spark.PipelinedRDD`1.MapPartitionsWithIndexInternal``1(Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate,System.Boolean)">
            <summary>
            Return a new RDD by applying a function to each partition of this RDD,
            while tracking the index of the original partition.
            </summary>
            <typeparam name="U">The element type of new RDD</typeparam>
            <param name="newFunc">The function to be applied to each partition</param>
            <param name="preservesPartitioning">
            Indicates if it preserves partition parameters
            </param>
            <returns>A new RDD</returns>
        </member>
        <member name="P:Microsoft.Spark.PipelinedRDD`1.Microsoft#Spark#Interop#Ipc#IJvmObjectReferenceProvider#Reference">
            <summary>
            Returns the JVM reference for this RDD. It also initializes the reference
            if it is not yet initialized.
            
            Note that PipelineRDD uses the JavaRDD internally.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.PipelinedRDD`1.IsPipelinable">
            <summary>
            Checks whether worker functions can be pipelined.
            </summary>
            <returns>True if worker functions can be pipelined.</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector">
            <summary>
            Collector collects objects from a socket.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.RDD.Collector.Collect(System.IO.Stream,Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Collects pickled row objects from the given socket.
            </summary>
            <param name="stream">Stream object to read from</param>
            <param name="serializedMode">Serialized mode for each element</param>
            <returns>Collection of row objects</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD.Collector.GetDeserializer(Microsoft.Spark.Utils.CommandSerDe.SerializedMode)">
            <summary>
            Returns a deserializer based on the given serialization mode.
            </summary>
            <param name="mode">Serialization mode</param>
            <returns>A deserializer object</returns>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.IDeserializer">
            <summary>
            Interface to deserialize an object from a given stream.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.BinaryDeserializer">
            <summary>
            Deserializer using the BinaryFormatter.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.StringDeserializer">
            <summary>
            Deserializer for UTF-8 strings.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.Collector.RowDeserializer">
            <summary>
            Deserializer for Pickled Rows.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.WorkerFunction">
            <summary>
            WorkerFunction provides the delegate type that is used for unifying
            UDFs used for RDD. It also provides functionality to chain delegates.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate">
            <summary>
            Delegate type to which each RDD UDF is transformed.
            </summary>
            <param name="splitId">split id for the current task</param>
            <param name="input">enumerable collection of objects</param>
            <returns>enumerable collection of objects after applying UDF</returns>
        </member>
        <member name="M:Microsoft.Spark.RDD.WorkerFunction.Chain(Microsoft.Spark.RDD.WorkerFunction,Microsoft.Spark.RDD.WorkerFunction)">
            <summary>
            Used to chain two function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.RDD.WorkerFunction.WorkerFuncChainHelper">
            <summary>
            Helper to chain two delegates.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Services.ConfigurationService">
            <summary>
            Implementation of configuration service that helps getting config settings
            to be used in .NET backend.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ConfigurationService.GetBackendPortNumber">
            <summary>
            Returns the port number for socket communication between JVM and CLR.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ConfigurationService.GetWorkerExePath">
            <summary>
            Returns the worker executable path.
            </summary>
            <returns>Worker executable path</returns>
        </member>
        <member name="T:Microsoft.Spark.Services.ConsoleLoggerService">
            <summary>
            This logger service will be used if the .NET driver app did not configure a logger.
            Right now it just prints out the messages to Console
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Services.ConsoleLoggerService.IsDebugEnabled">
            <summary>
            Gets a value indicating whether logging is enabled for the Debug level.
            Always return true for the DefaultLoggerService object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.GetLoggerInstance(System.Type)">
            <summary>
            Get an instance of ILoggerService by a given type of logger
            </summary>
            <param name="type">The type of a logger to return</param>
            <returns>An instance of ILoggerService</returns>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogDebug(System.String)">
            <summary>
            Logs a message at debug level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogDebug(System.String,System.Object[])">
            <summary>
            Logs a message at debug level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogInfo(System.String)">
            <summary>
            Logs a message at info level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogInfo(System.String,System.Object[])">
            <summary>
            Logs a message at info level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogWarn(System.String)">
            <summary>
            Logs a message at warning level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogWarn(System.String,System.Object[])">
            <summary>
            Logs a message at warning level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogFatal(System.String)">
            <summary>
            Logs a fatal message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogFatal(System.String,System.Object[])">
            <summary>
            Logs a fatal message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogError(System.String)">
            <summary>
            Logs a error message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogError(System.String,System.Object[])">
            <summary>
            Logs a error message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ConsoleLoggerService.LogException(System.Exception)">
            <summary>
            Logs an exception
            </summary>
            <param name="e">The exception to be logged</param>
        </member>
        <member name="T:Microsoft.Spark.Services.IConfigurationService">
            <summary>
            Helps getting config settings to be used in .NET runtime
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.IConfigurationService.GetBackendPortNumber">
            <summary>
            The port number used for communicating with the .NET backend process.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.IConfigurationService.GetWorkerExePath">
            <summary>
            The full path to the .NET worker executable.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Services.ILoggerService">
            <summary>
            Defines a logger what be used in service
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Services.ILoggerService.IsDebugEnabled">
            <summary>
            Gets a value indicating whether logging is enabled for the Debug level.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.GetLoggerInstance(System.Type)">
            <summary>
            Get an instance of ILoggerService by a given type of logger
            </summary>
            <param name="type">The type of a logger to return</param>
            <returns>An instance of ILoggerService</returns>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogDebug(System.String)">
            <summary>
            Logs a message at debug level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogDebug(System.String,System.Object[])">
            <summary>
            Logs a message at debug level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogInfo(System.String)">
            <summary>
            Logs a message at info level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogInfo(System.String,System.Object[])">
            <summary>
            Logs a message at info level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogWarn(System.String)">
            <summary>
            Logs a message at warning level.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogWarn(System.String,System.Object[])">
            <summary>
            Logs a message at warning level with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogFatal(System.String)">
            <summary>
            Logs a fatal message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogFatal(System.String,System.Object[])">
            <summary>
            Logs a fatal message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogError(System.String)">
            <summary>
            Logs a error message.
            </summary>
            <param name="message">The message to be logged</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogError(System.String,System.Object[])">
            <summary>
            Logs a error message with a format string.
            </summary>
            <param name="messageFormat">The format string</param>
            <param name="messageParameters">The array of arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Services.ILoggerService.LogException(System.Exception)">
            <summary>
            Logs an exception
            </summary>
            <param name="e">The exception to be logged</param>
        </member>
        <member name="T:Microsoft.Spark.Services.LoggerServiceFactory">
            <summary>
            Used to get logger service instances for different types
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Services.LoggerServiceFactory.SetLoggerService(Microsoft.Spark.Services.ILoggerService)">
            <summary>
            Overrides an existing logger by a given logger service instance
            </summary>
            <param name="loggerServiceOverride">
            The logger service instance used to overrides
            </param>
        </member>
        <member name="M:Microsoft.Spark.Services.LoggerServiceFactory.GetLogger(System.Type)">
            <summary>
            Gets an instance of logger service for a given type.
            </summary>
            <param name="type">The type of logger service to get</param>
            <returns>An instance of logger service</returns>
        </member>
        <member name="M:Microsoft.Spark.Services.LoggerServiceFactory.GetDefaultLogger">
            <summary>
            if there exists xxx.exe.config file and log4net settings, then use log4net
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.SparkConf">
            <summary>
            Configuration for a Spark application. Used to set various Spark parameters as key-value
            pairs.
            </summary>
            <remarks>
            Note that once a SparkConf object is passed to Spark, it is cloned and can no longer be
            modified by the user. Spark does not support modifying the configuration at runtime.
            </remarks>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.#ctor(System.Boolean)">
            <summary>
            Constructor.
            </summary>
            <param name="loadDefaults">
            Indicates whether to load values from Java system properties
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.SetMaster(System.String)">
            <summary>
            The master URL to connect to, such as "local" to run locally with one thread,
            "local[4]" to run locally with 4 cores, or "spark://master:7077" to run on a Spark
            standalone cluster.
            </summary>
            <param name="master">Spark master</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.SetAppName(System.String)">
            <summary>
            Set a name for your application. Shown in the Spark web UI.
            </summary>
            <param name="appName">Name of the app</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.SetSparkHome(System.String)">
            <summary>
            Set the location where Spark is installed on worker nodes.
            </summary>
            <param name="sparkHome"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.Set(System.String,System.String)">
            <summary>
            Set the value of a string config
            </summary>
            <param name="key">Config name</param>
            <param name="value">Config value</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.GetInt(System.String,System.Int32)">
            <summary>
            Get a int parameter value, falling back to a default if not set
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.Get(System.String,System.String)">
            <summary>
            Get a string parameter value, falling back to a default if not set
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.SparkConf.GetAll">
            <summary>
            Get all parameters as a list of pairs.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.SparkContext">
            <summary>
            Main entry point for Spark functionality. A SparkContext represents the connection
            to a Spark cluster, and can be used to create RDDs, accumulators and broadcast
            variables on that cluster.
            
            Only one `SparkContext` should be active per JVM. You must `stop()` the
            active `SparkContext` before creating a new one.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(Microsoft.Spark.SparkConf)">
            <summary>
            Create a SparkContext object with the given config.
            </summary>
            <param name="conf">a Spark config object describing the application configuration.
            Any settings in this config overrides the default configs as well as system properties.
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor">
            <summary>
            Create a SparkContext that loads settings from system properties (for instance,
            when launching with spark-submit).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(System.String,System.String,Microsoft.Spark.SparkConf)">
            <summary>
            Alternative constructor that allows setting common Spark properties directly.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
            <param name="conf">
            A <see cref="T:Microsoft.Spark.SparkConf"/> object specifying other Spark parameters
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(System.String,System.String)">
            <summary>
            Initializes a SparkContext instance with a specific master and application name.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(System.String,System.String,System.String)">
            <summary>
            Alternative constructor that allows setting common Spark properties directly.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
            <param name="sparkHome">The path that holds spark bits</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor where SparkContext object is already created.
            </summary>
            <param name="jvmObject">JVM object reference for this SparkContext object</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.GetConf">
            <summary>
            Returns SparkConf object associated with this SparkContext object.
            Note that modifying the SparkConf object will not have any impact.
            </summary>
            <returns>SparkConf object</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.GetOrCreate(Microsoft.Spark.SparkConf)">
            <summary>
            This function may be used to get or instantiate a SparkContext and register it as a
            singleton object. Because we can only have one active SparkContext per JVM,
            this is useful when applications may wish to share a SparkContext.
            </summary>
            <param name="conf"><see cref="T:Microsoft.Spark.SparkConf"/> that will be used for creating SparkContext
            </param>
            <returns>
            Current SparkContext (or a new one if it wasn't created before the function call)
            </returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetLogLevel(System.String)">
            <summary>
            Control our logLevel. This overrides any user-defined log settings.
            </summary>
            <remarks>
            Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
            </remarks>
            <param name="logLevel">The desired log level as a string.</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.Stop">
            <summary>
            Shut down the SparkContext.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.SparkContext.DefaultParallelism">
            <summary>
            Default level of parallelism to use when not given by user (e.g. Parallelize()).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.GetUpdatedConf(System.String,System.String,System.String,Microsoft.Spark.SparkConf)">
            <summary>
            Creates a modified version of <see cref="T:Microsoft.Spark.SparkConf"/> with the parameters that can be
            passed separately to SparkContext, to make it easier to write SparkContext's
            constructors.
            </summary>
            <param name="master">Cluster URL to connect to (e.g. spark://host:port, local)</param>
            <param name="appName">A name for the application</param>
            <param name="sparkHome">The path that holds spark bits</param>
            <param name="conf">
            A <see cref="T:Microsoft.Spark.SparkConf"/> object specifying other Spark parameters
            </param>
            <returns>Modified <see cref="T:Microsoft.Spark.SparkConf"/> object.</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetJobDescription(System.String)">
            <summary>
            Sets a human readable description of the current job.
            </summary>
            <param name="value">Description of the current job</param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetJobGroup(System.String,System.String,System.Boolean)">
            <summary>
            Assigns a group ID to all the jobs started by this thread until the group ID is set to
            a different value or cleared.
            </summary>
            <remarks>
            Often, a unit of execution in an application consists of multiple Spark actions or
            jobs. Application programmers can use this method to group all those jobs together
            and give a group description. Once set, the Spark web UI will associate such jobs
            with this group.
            </remarks>
            <param name="groupId">Group Id</param>
            <param name="description">Description on the job group</param>
            <param name="interruptOnCancel">
            If true, then job cancellation will result in `Thread.interrupt()` being called on the
            job's executor threads. 
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.ClearJobGroup">
            <summary>
            Clear the current thread's job group ID and its description.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.Parallelize``1(System.Collections.Generic.IEnumerable{``0},System.Nullable{System.Int32})">
            <summary>
            Distribute a local collection to form an RDD.
            </summary>
            <typeparam name="T">Type of the elements in the collection</typeparam>
            <param name="seq">Collection to distribute</param>
            <param name="numSlices">Number of partitions to divide the collection into</param>
            <returns>RDD representing distributed collection</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.TextFile(System.String,System.Nullable{System.Int32})">
            <summary>
            Read a text file from HDFS, a local file system (available on all nodes), or any
            Hadoop-supported file system URI, and return it as an RDD of strings.
            </summary>
            <param name="path">path to the text file on a supported file system</param>
            <param name="minPartitions">minimum number of partitions for the resulting RDD</param>
            <returns>RDD of lines of the text file</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)">
            <summary>
            Add a file to be downloaded with this Spark job on every node.
            </summary>
            <remarks>
            If a file is added during execution, it will not be available until the next
            TaskSet starts.
            </remarks>
            <param name="path">
            File path can be either a local file, a file in HDFS (or other Hadoop-supported
            filesystems), or an HTTP, HTTPS or FTP URI.
            </param>
            <param name="recursive">
            If true, a directory can be given in `path`. Currently directories are supported
            only for Hadoop-supported filesystems.
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.SetCheckpointDir(System.String)">
            <summary>
            Sets the directory under which RDDs are going to be checkpointed.
            </summary>
            <param name="directory">
            path to the directory where checkpoint files will be stored
            </param>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.Broadcast``1(``0)">
            <summary>
            Broadcast a read-only variable to the cluster, returning a Microsoft.Spark.Broadcast
            object for reading it in distributed functions. The variable will be sent to each 
            executor only once.
            </summary>
            <typeparam name="T">Type of the variable being broadcast</typeparam>
            <param name="value">Value of the broadcast variable</param>
            <returns>A Broadcast object of type <see cref="M:Microsoft.Spark.SparkContext.Broadcast``1(``0)"/></returns>
        </member>
        <member name="M:Microsoft.Spark.SparkContext.WrapAsJavaRDD(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Returns JVM object reference to JavaRDD object transformed
            from a Scala RDD object.
            </summary>
            <remarks>
            The transformation is for easy reflection on the JVM side.
            </remarks>
            <param name="rdd">JVM object reference to Scala RDD</param>
            <returns>JVM object reference to JavaRDD object</returns>
        </member>
        <member name="T:Microsoft.Spark.SparkFiles">
            <summary>
            Resolves paths to files added through <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.SparkFiles.Get(System.String)">
            <summary>
            Get the absolute path of a file added through
            <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            </summary>
            <param name="fileName">The name of the file added through
            <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            </param>
            <returns>The absolute path of the file.</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkFiles.GetRootDirectory">
            <summary>
            Get the root directory that contains files added through
            <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            </summary>
            <returns>The root directory that contains the files.</returns>
        </member>
        <member name="M:Microsoft.Spark.SparkFiles.SetRootDirectory(System.String)">
            <summary>
            Set the root directory that contains files added through
            <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            <remarks>
            This should only be called from the Microsoft.Spark.Worker.
            </remarks>
            </summary>
            <param name="path">Root directory that contains files added
            through <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            </param>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowArrayHelpers">
            <summary>
            Helper methods to work with Apache Arrow arrays.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowGroupedMapUdfWrapper">
            <summary>
            Wraps the given Func object, which represents a Grouped Map UDF.
            </summary>
            <remarks>
            UDF serialization requires a "wrapper" object in order to serialize/deserialize.
            </remarks>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`2">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`3">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`4">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`5">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`6">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`7">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`8">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`9">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`10">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowUdfWrapper`11">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.Builder">
            <summary>
            The entry point to programming Spark with the Dataset and DataFrame API.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Master(System.String)">
            <summary>
            Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
            run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone
            cluster.
            </summary>
            <param name="master">Master URL</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.AppName(System.String)">
            <summary>
            Sets a name for the application, which will be shown in the Spark web UI.
            If no application name is set, a randomly generated name will be used.
            </summary>
            <param name="appName">Name of the app</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.String)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.Boolean)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.Double)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(System.String,System.Int64)">
            <summary>
            Sets a config option. Options set using this method are automatically propagated to
            both SparkConf and SparkSession's own configuration.
            </summary>
            <param name="key">Key for the configuration</param>
            <param name="value">value of the configuration</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.Config(Microsoft.Spark.SparkConf)">
            <summary>
            Sets a list of config options based on the given SparkConf
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.EnableHiveSupport">
            <summary>
            Enables Hive support, including connectivity to a persistent Hive metastore, support
            for Hive serdes, and Hive user-defined functions.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Builder.GetOrCreate">
            <summary>
            Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
            one based on the options set in this builder.
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Catalog.Catalog">
            <summary>
            Catalog interface for Spark. To access this, use SparkSession.Catalog.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.CacheTable(System.String)">
             <summary>
             Caches the specified table in-memory.
            
             Spark SQL can cache tables using an in-memory columnar format by calling
             `CacheTable("tableName")` or `DataFrame.Cache()`. Spark SQL will scan only required
             columns and will automatically tune compression to minimize memory usage and GC
             pressure. You can call `UncacheTable("tableName")` to remove the table from memory.
             </summary>
             <param name="tableName">Is either a qualified or unqualified name that designates a
             table. If no database identifier is provided, it refers to a table in the current
             database.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ClearCache">
            <summary>
            Removes all cached tables from the in-memory cache. You can either clear all cached
            tables at once using this or clear each table individually using
            `UncacheTable("tableName")`.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.CreateTable(System.String,System.String)">
            <summary>
            Creates a table, in the hive warehouse, from the given path and returns the
            corresponding DataFrame. The table will contain the contents of the parquet
            file that is in the `path` parameter. The default data source type is parquet. This can
            be changed using `CreateTable(tableName, path, source)` or setting the configuration
            option `spark.sql.sources.default` when creating the spark session using
            `Config("spark.sql.sources.default", "csv")` or after you have created the session using
            `Conf().Set("spark.sql.sources.default", "csv")`.
            </summary>
            <param name="tableName">The name of the table to create.</param>
            <param name="path">Path to use to create the table.</param>
            <returns>The contents of the files in the path parameter as a `DataFrame`.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.CreateTable(System.String,System.String,System.String)">
             <summary>
             Creates a table, in the hive warehouse, from the given path based from a data
             source and returns the corresponding DataFrame.
            
             The type of file type (csv, parquet, etc.) is specified using the `source` parameter.
             </summary>
             <param name="tableName">Is either a qualified or unqualified name that designates a
             table. If no database identifier is provided, it refers to a table in the current
             database.</param>
             <param name="path">Path to use to create the table.</param>
             <param name="source">Data source to use to create the table such as parquet, csv, etc.
             </param>
             <returns>The results of reading the files in path as a `DataFrame`.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.CurrentDatabase">
            <summary>
            Returns the current database in this session. By default your session will be
            connected to the "default" database (named "default") and to change database
            either use `SetCurrentDatabase("databaseName")` or
            `SparkSession.Sql("USE DATABASE databaseName")`.
            </summary>
            <returns>The database name as a string.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.DatabaseExists(System.String)">
            <summary>
            Check if the database with the specified name exists. This will check the list
            of hive databases in the current session to see if the database exists.
            </summary>
            <param name="dbName">Name of the database to check.</param>
            <returns>bool, true if the database exists and false if it does not exist.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.DropGlobalTempView(System.String)">
             <summary>
             Drops the global temporary view with the given view name in the catalog.
            
             You can create global temporary views by taking a DataFrame and calling
             `DataFrame.CreateOrReplaceGlobalTempView`.
             </summary>
             <param name="viewName">The unqualified name of the temporary view to be dropped.
             </param>
             <returns>bool, true if the view was dropped and false if it was not dropped.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.DropTempView(System.String)">
             <summary>
             Drops the local temporary view with the given view name in the catalog.
             Local temporary view is session-scoped. Its lifetime is the lifetime of the session
             that created it, i.e. it will be automatically dropped when the session terminates.
             It's not tied to any databases, i.e. we can't use db1.view1 to reference a local
             temporary view.
            
             You can create temporary views by taking a DataFrame and calling
             `DataFrame.CreateOrReplaceTempView`.
             </summary>
             <param name="viewName">The unqualified name of the temporary view to be dropped.
             </param>
             <returns>bool, true if the view was dropped and false if it was not dropped.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.FunctionExists(System.String)">
            <summary>
            Check if the function with the specified name exists. `FunctionsExists` includes in-built
            functions such as `abs`. To see if a built-in function exists you must use the
            unqualified name. If you create a function you can use the qualified name.
            </summary>
            <param name="functionName">Is either a qualified or unqualified name that designates a
            function. If no database identifier is provided, it refers to a function in the
            current database.</param>
            <returns>bool, true if the function exists and false it is does not.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.FunctionExists(System.String,System.String)">
            <summary>
            Check if the function with the specified name exists in the specified database. If you
            want to check if a built-in function exists specify the dbName as null or use
            `FunctionExists(functionName)`.
            </summary>
            <param name="dbName">Is a name that designates a database.</param>
            <param name="functionName">Is an unqualified name that designates a function.</param>
            <returns>bool, true if the function exists and false it is does not.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.GetDatabase(System.String)">
             <summary>
             Get the database with the specified name.
            
             Calling `GetDatabase` gives you access to the hive database name, description and
             location.
             </summary>
             <param name="dbName">Name of the database to get.</param>
             <returns>`Database` object which includes the name, description and locationUri of
             the database.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.GetFunction(System.String)">
            <summary>
            Get the function with the specified name. If you are trying to get an in-built
            function then use the unqualified name.
            </summary>
            <param name="functionName">Is either a qualified or unqualified name that designates a
            function. If no database identifier is provided, it refers to a temporary function or
            a function in the current database.</param>
            <returns>`Function` object which includes the class name, database, description,
            whether it is temporary and the name of the function.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.GetFunction(System.String,System.String)">
            <summary>
            Get the function with the specified name. If you are trying to get an in-built function
            then pass null as the dbName.
            </summary>
            <param name="dbName">Is a name that designates a database. Built-in functions will be
            in database null rather than default.</param>
            <param name="functionName">Is an unqualified name that designates a function in the
            specified database.</param>
            <returns>`Function` object which includes the class name, database, description,
            whether it is temporary and the name of the function.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.GetTable(System.String)">
            <summary>
            Get the table or view with the specified name. You can use this to find the tables
            description, database, type and whether it is a temporary table or not.
            </summary>
            <param name="tableName">Is either a qualified or unqualified name that designates a
            table. If no database identifier is provided, it refers to a table in the current
            database.</param>
            <returns>`Table` object which includes name, database, description, table type and
            whether the table is temporary or not.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.GetTable(System.String,System.String)">
            <summary>
            Get the table or view with the specified name in the specified database. You can use
            this to find the tables description, database, type and whether it is a temporary
            table or not.
            </summary>
            <param name="dbName">Is a name that designates a database.</param>
            <param name="tableName">Is an unqualified name that designates a table in the specified
            database.</param>
            <returns>`Table` object which includes name, database, description, table type and
            whether the table is temporary or not.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.IsCached(System.String)">
            <summary>
            Returns true if the table is currently cached in-memory. If the table is cached then it
            will consume memory. To remove the table from cache use `UncacheTable` or `ClearCache`
            </summary>
            <param name="tableName">Is either a qualified or unqualified name that designates a
            table. If no database identifier is provided, it refers to a table in the current
            database.</param>
            <returns>bool, true if the table is cahced and false if it is not cached</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListColumns(System.String)">
            <summary>
            Returns a list of columns for the given table/view or temporary view. The DataFrame
            includes the name, description, dataType, whether it is nullable or if it is
            partitioned and if it is broken in buckets.
            </summary>
            <param name="tableName">Is either a qualified or unqualified name that designates a
            table. If no database identifier is provided, it refers to a table in the current
            database.</param>
            <returns>`DataFrame` with the name, description, dataType, whether each column is
            nullable, if the column is partitioned and if the column is broken in buckets.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListColumns(System.String,System.String)">
            <summary>
            Returns a list of columns for the given table/view in the specified database.
            The `DataFrame` includes the name, description, dataType, whether it is nullable or if it
            is partitioned and if it is broken in buckets.
            </summary>
            <param name="dbName">Is a name that designates a database.</param>
            <param name="tableName">Is an unqualified name that designates a table in the specified
            database.</param>
            <returns>`DataFrame` with the name, description, dataType, whether each column is
            nullable, if the column is partitioned and if the column is broken in buckets.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListDatabases">
            <summary>
            Returns a list of databases available across all sessions. The `DataFrame` contains
            the name, description and locationUri of each database.
            </summary>
            <returns>`DataFrame` with the  name, description and locationUri of each database.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListFunctions">
            <summary>
            Returns a list of functions registered in the current database. This includes all
            temporary functions. The `DataFrame` contains the class name, database, description,
            whether it is temporary and the name of each function.
            </summary>
            <returns>`DataFrame` with the class name, database, description, whether it is
            temporary and the name of each function.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListFunctions(System.String)">
            <summary>
            Returns a list of functions registered in the specified database. This includes all
            temporary functions. The `DataFrame` contains the class name, database, description,
            whether it is temporary and the name of the function.
            </summary>
            <param name="dbName">Is a name that designates a database.</param>
            <returns>`DataFrame` with the class name, database, description, whether it is
            temporary and the name of each function.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListTables">
            <summary>
            Returns a list of tables/views in the current database. The `DataFrame` includes the
            name, database, description, table type and whether the table is temporary or not.
            </summary>
            <returns>`DataFrame` with the name, database, description, table type and whether the
            table is temporary or not for each table in the default database</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.ListTables(System.String)">
            <summary>
            Returns a list of tables/views in the specified database. The `DataFrame` includes the
            name, database, description, table type and whether the table is temporary or not.
            </summary>
            <param name="dbName">Is a name that designates a database.</param>
            <returns>`DataFrame` with the name, database, description, table type and whether the
            table is temporary or not for each table in the named database</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.RecoverPartitions(System.String)">
            <summary>
            Recovers all the partitions in the directory of a table and update the catalog. This
            only works for partitioned tables and not un-partitioned tables or views.
            </summary>
            <param name="tableName">Is either a qualified or unqualified name that designates a
            table. If no database identifier is provided, it refers to a table in the current
            database.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.RefreshByPath(System.String)">
            <summary>
            Invalidates and refreshes all the cached data (and the associated metadata) for any
            Dataset that contains the given data source path. Path matching is by prefix,
            i.e. "/" would invalidate everything that is cached.
            </summary>
            <param name="path">Path to refresh</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.RefreshTable(System.String)">
            <summary>
            Invalidates and refreshes all the cached data and metadata of the given table. For
            performance reasons, Spark SQL or the external data source library it uses might cache
            certain metadata about a table, such as the location of blocks. When those change
            outside of Spark SQL, users should call this function to invalidate the cache. If this
            table is cached as an InMemoryRelation, drop the original cached version and make the
            new version cached lazily.
            </summary>
            <param name="tableName">Is either a qualified or unqualified name that designates a
            table. If no database identifier is provided, it refers to a table in the current
            database.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.SetCurrentDatabase(System.String)">
            <summary>
            Sets the current default database in this session.
            </summary>
            <param name="dbName">The name of the database to set.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.TableExists(System.String)">
            <summary>
            Check if the table or view with the specified name exists. This can either be a
            temporary view or a table/view.
            </summary>
            <param name="tableName">Is either a qualified or unqualified name that designates a
            table. If no database identifier is provided, it refers to a table in the current
            database.</param>
            <returns>bool, true if the table exists and false if it does not exist</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.TableExists(System.String,System.String)">
            <summary>
            Check if the table or view with the specified name exists in the specified database.
            </summary>
            <param name="dbName">Is a name that designates a database.</param>
            <param name="tableName">Is an unqualified name that designates a table.</param>
            <returns>bool, true if the table exists in the specified database and false if it does
            not exist</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Catalog.Catalog.UncacheTable(System.String)">
             <summary>
             Removes the specified table from the in-memory cache.
             </summary>
             <param name="tableName">Is either a qualified or unqualified name that designates a
             table. If no database identifier is provided, it refers to a table in the current
             database.
            
             To cache a table use `CacheTable(tableName)`.
             </param>
        </member>
        <member name="T:Microsoft.Spark.Sql.Catalog.Database">
            <summary>
            A database in Spark, as returned by the `ListDatabases` method defined in `Catalog`.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Database.Description">
            <summary>
            Description of the database.
            </summary>
            <returns>string, the description of the database.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Database.LocationUri">
            <summary>
            Path (in the form of a uri) to data files
            </summary>
            <returns>string, the location of the database.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Database.Name">
            <summary>
            Name of the database.
            </summary>
            <returns>string, the name of the database.</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Catalog.Function">
            <summary>
            A user-defined function in Spark, as returned by `ListFunctions` method in `Catalog`.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Function.Database">
            <summary>
            Name of the database the function belongs to.
            </summary>
            <returns>string, the name of the database that the function is in.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Function.Description">
            <summary>
            Description of the function.
            </summary>
            <returns>string, the description of the function.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Function.IsTemporary">
            <summary>
            Whether the function is temporary or not
            </summary>
            <returns>bool, true if the function is temporary and false if it is not temporary.
            </returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Function.Name">
            <summary>
            Name of the function
            </summary>
            <returns>string, the name of the function.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Function.ClassName">
            <summary>
            The fully qualified class name of the function
            </summary>
            <returns>string, the name of the class that implements the function.</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Catalog.Table">
            <summary>
            A table in Spark, as returned by the `ListTables` method in `Catalog`.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Table.Database">
            <summary>
            Name of the database the table belongs to.
            </summary>
            <returns>string, the name of the database the table is in.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Table.Description">
            <summary>
            Description of the table.
            </summary>
            <returns>string, the description of the table.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Table.IsTemporary">
            <summary>
            Whether the table is temporary or not.
            </summary>
            <returns>bool, true if the table is temporary, false if it is not.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Table.Name">
            <summary>
            The name of the table.
            </summary>
            <returns>string, the name of the table.</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Catalog.Table.TableType">
            <summary>
            The type of table (e.g. view/table)
            </summary>
            <returns>string, will return either `view` or `table` </returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Column">
            <summary>
            Column class represents a column that will be computed based on the data in a DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor for Column class.
            </summary>
            <param name="jvmObject">JVM object reference</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_UnaryNegation(Microsoft.Spark.Sql.Column)">
            <summary>
            Negate the given column.
            </summary>
            <param name="self">Column to negate</param>
            <returns>New column after applying negation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_LogicalNot(Microsoft.Spark.Sql.Column)">
            <summary>
            Apply inversion of boolean expression, i.e. NOT.
            </summary>
            <param name="self">Column to apply inversion</param>
            <returns>New column after applying inversion</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Equality(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply equality test on the given two columns.
            </summary>
            <param name="lhs">Column on the left side of equality test</param>
            <param name="rhs">Column on the right side of equality test</param>
            <returns>New column after applying equality test</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EqualTo(System.Object)">
            <summary>
            Equality test.
            </summary>
            <param name="rhs">The right hand side of expression being tested for equality</param>
            <returns>New column after applying the equal to operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Inequality(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply inequality test.
            </summary>
            <param name="lhs">Column on the left side of inequality test</param>
            <param name="rhs">Column on the right side of inequality test</param>
            <returns>New column after applying inequality test</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.NotEqual(System.Object)">
            <summary>
            Inequality test.
            </summary>
            <param name="rhs">
            The right hand side of expression being tested for inequality.
            </param>
            <returns>New column after applying not equal operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_GreaterThan(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "greater than" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Gt(System.Object)">
            <summary>
            Greater than.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is greater.
            </param>
            <returns>New column after applying the greater than operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_LessThan(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "less than" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Lt(System.Object)">
            <summary>
            Less than.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is lesser.
            </param>
            <returns>New column after applying the less than operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_LessThanOrEqual(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "less than or equal to" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Leq(System.Object)">
            <summary>
            Less than or equal to.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is less or equal to.
            </param>
            <returns>New column after applying the less than or equal to operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_GreaterThanOrEqual(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply "greater than or equal to" operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Geq(System.Object)">
            <summary>
            Greater or equal to.
            </summary>
            <param name="rhs">
            The object that is in comparison to test if the left hand side is greater or equal to
            </param>
            <returns>New column after applying the greater or equal to operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EqNullSafe(System.Object)">
            <summary>
            Apply equality test that is safe for null values.
            </summary>
            <param name="obj">Object to apply equality test</param>
            <returns>New column after applying the equality test</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.When(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Evaluates a condition and returns one of multiple possible result expressions.
            If Otherwise(object) is not defined at the end, null is returned for
            unmatched conditions. This method can be chained with other 'when' invocations in case
            multiple matches are required.
            </summary>
            <param name="condition">The condition to check</param>
            <param name="value">The value to set if the condition is true</param>
            <returns>New column after applying the when method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Otherwise(System.Object)">
            <summary>
            Evaluates a list of conditions and returns one of multiple possible result expressions.
            If otherwise is not defined at the end, null is returned for unmatched conditions.
            This is used when the When(Column, object) method is applied.
            </summary>
            <param name="value">The value to set</param>
            <returns>New column after applying otherwise method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Between(System.Object,System.Object)">
            <summary>
            True if the current column is between the lower bound and upper bound, inclusive.
            </summary>
            <param name="lowerBound">The lower bound</param>
            <param name="upperBound">The upper bound</param>
            <returns>New column after applying the between method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsNaN">
            <summary>
            True if the current expression is NaN.
            </summary>
            <returns>
            New column with values true if the preceding column had a NaN
            value in the same index, and false otherwise.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsNull">
            <summary>
            True if the current expression is null.
            </summary>
            <returns>
            New column with values true if the preceding column had a null
            value in the same index, and false otherwise.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsNotNull">
            <summary>
            True if the current expression is NOT null.
            </summary>
            <returns>
            New column with values true if the preceding column had a non-null
            value in the same index, and false otherwise.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_BitwiseOr(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean OR operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Or(Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean OR operator with the given column.
            </summary>
            <param name="other">Column to apply OR operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_BitwiseAnd(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean AND operator for the given two columns.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Column on the right side of the operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.And(Microsoft.Spark.Sql.Column)">
            <summary>
            Apply boolean AND operator with the given column.
            </summary>
            <param name="other">Column to apply AND operator</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Addition(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply sum of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the sum operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Plus(System.Object)">
            <summary>
            Sum of this expression and another expression.
            </summary>
            <param name="rhs">The expression to be summed with</param>
            <returns>New column after applying the plus operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Subtraction(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply subtraction of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the subtraction operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Minus(System.Object)">
            <summary>
            Subtraction. Subtract the other expression from this expression.
            </summary>
            <param name="rhs">The expression to be subtracted with</param>
            <returns>New column after applying the minus operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Multiply(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply multiplication of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the multiplication operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Multiply(System.Object)">
            <summary>
            Multiplication of this expression and another expression.
            </summary>
            <param name="rhs">The expression to be multiplied with</param>
            <returns>New column after applying the multiply operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Division(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply division of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the division operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Divide(System.Object)">
            <summary>
            Division of this expression by another expression.
            </summary>
            <param name="rhs">The expression to be divided by</param>
            <returns>New column after applying the divide operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.op_Modulus(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Apply division of two expressions.
            </summary>
            <param name="lhs">Column on the left side of the operator</param>
            <param name="rhs">Object on the right side of the operator</param>
            <returns>New column after applying the division operation</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Mod(System.Object)">
            <summary>
            Modulo (a.k.a remainder) expression.
            </summary>
            <param name="rhs">
            The expression to be divided by to get the remainder for.
            </param>
            <returns>New column after applying the mod operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Like(System.String)">
            <summary>
            SQL like expression. Returns a boolean column based on a SQL LIKE match.
            </summary>
            <param name="literal">The literal that is used to compute the SQL LIKE match</param>
            <returns>New column after applying the SQL LIKE match</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.RLike(System.String)">
            <summary>
            SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
            match.
            </summary>
            <param name="literal">The literal that is used to compute the Regex match</param>
            <returns>New column after applying the regex LIKE method</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.GetItem(System.Object)">
            <summary>
            An expression that gets an item at position `ordinal` out of an array,
            or gets a value by key `key` in a `MapType`.
            </summary>
            <param name="key">The key with which to identify the item</param>
            <returns>New column after getting an item given a specific key</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.GetField(System.String)">
            <summary>
            An expression that gets a field by name in a `StructType`.
            </summary>
            <param name="fieldName">The name of the field</param>
            <returns>New column after getting a field for a specific key</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.SubStr(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            An expression that returns a substring.
            </summary>
            <param name="startPos">Expression for the starting position</param>
            <param name="len">Expression for the length of the substring</param>
            <returns>
            New column that is bound by the start position provided, and the length.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.SubStr(System.Int32,System.Int32)">
            <summary>
            An expression that returns a substring.
            </summary>
            <param name="startPos">Starting position</param>
            <param name="len">Length of the substring</param>
            <returns>
            New column that is bound by the start position provided, and the length.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Contains(System.Object)">
            <summary>
            Contains the other element. Returns a boolean column based on a string match.
            </summary>
            <param name="other">
            The object that is used to check for existence in the current column.
            </param>
            <returns>New column after checking if the column contains object other</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.StartsWith(Microsoft.Spark.Sql.Column)">
            <summary>
            String starts with. Returns a boolean column based on a string match.
            </summary>
            <param name="other">
            The other column containing strings with which to check how values
            in this column starts.
            </param>
            <returns>
            A boolean column where entries are true if values in the current
            column does indeed start with the values in the given column.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.StartsWith(System.String)">
            <summary>
            String starts with another string literal.
            Returns a boolean column based on a string match.
            </summary>
            <param name="literal">
            The string literal used to check how values in a column starts.
            </param>
            <returns>
            A boolean column where entries are true if values in the current column
            does indeed start with the given string literal.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EndsWith(Microsoft.Spark.Sql.Column)">
            <summary>
            String ends with. Returns a boolean column based on a string match.
            </summary>
            <param name="other">
            The other column containing strings with which to check how values
            in this column ends.
            </param>
            <returns>
            A boolean column where entries are true if values in the current
            column does indeed end with the values in the given column.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.EndsWith(System.String)">
            <summary>
            String ends with another string literal. Returns a boolean column based
            on a string match.
            </summary>
            <param name="literal">
            The string literal used to check how values in a column ends.
            </param>
            <returns>
            A boolean column where entries are true if values in the current column
            does indeed end with the given string literal.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Alias(System.String)">
            <summary>
            Gives the column an alias. Same as `As()`.
            </summary>
            <param name="alias">The alias that is given</param>
            <returns>New column after applying an alias</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.As(System.String)">
            <summary>
            Gives the column an alias.
            </summary>
            <param name="alias">The alias that is given</param>
            <returns>New column after applying the as alias operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.As(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Assigns the given aliases to the results of a table generating function.
            </summary>
            <param name="alias">A list of aliases</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Apply(System.Object)">
             <summary>
             Extracts a value or values from a complex type.
             The following types of extraction are supported:
            
             1. Given an Array, an integer ordinal can be used to retrieve a single value.
             2. Given a Map, a key of the correct type can be used to retrieve an individual value.
             3. Given a Struct, a string fieldName can be used to extract that field.
             4. Given an Array of Structs, a string fieldName can be used to extract field
             of every struct in that array, and return an Array of fields.
            
             </summary>
             <param name="extraction">Object used to extract value(s) from the column</param>
             <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Name(System.String)">
            <summary>
            Gives the column a name (alias).
            </summary>
            <param name="alias">Alias column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Cast(System.String)">
            <summary>
            Casts the column to a different data type, using the canonical string
            representation of the type.
            </summary>
            <remarks>
            The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
            `float`, `double`, `decimal`, `date`, `timestamp`.
            </remarks>
            <param name="to">String version of datatype</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Desc">
            <summary>
            Returns a sort expression based on ascending order of the column,
            and null values return before non-null values.
            </summary>
            <returns>New column after applying the descending order operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.DescNullsFirst">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values appear before non-null values.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.DescNullsLast">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values appear after non-null values.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Asc">
            <summary>
            Returns a sort expression based on ascending order of the column.
            </summary>
            <returns>New column after applying the ascending order operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.AscNullsFirst">
            <summary>
            Returns a sort expression based on ascending order of the column,
            and null values return before non-null values.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.AscNullsLast">
            <summary>
            Returns a sort expression based on ascending order of the column,
            and null values appear after non-null values.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Explain(System.Boolean)">
            <summary>
            Prints the expression to the console for debugging purposes.
            </summary>
            <param name="extended">To print extended version or not</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.BitwiseOR(System.Object)">
            <summary>
            Compute bitwise OR of this expression with another expression.
            </summary>
            <param name="other">
            The other column that will be used to compute the bitwise OR.
            </param>
            <returns>New column after applying bitwise OR operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.BitwiseAND(System.Object)">
            <summary>
            Compute bitwise AND of this expression with another expression.
            </summary>
            <param name="other">
            The other column that will be used to compute the bitwise AND.
            </param>
            <returns>New column after applying the bitwise AND operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.BitwiseXOR(System.Object)">
            <summary>
            Compute bitwise XOR of this expression with another expression.
            </summary>
            <param name="other">
            The other column that will be used to compute the bitwise XOR.
            </param>
            <returns>New column after applying bitwise XOR operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Over(Microsoft.Spark.Sql.Expressions.WindowSpec)">
            <summary>
            Defines a windowing column.
            </summary>
            <param name="window">
            A window specification that defines the partitioning, ordering, and frame boundaries.
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Over">
            <summary>
            Defines an empty analytic clause. In this case the analytic function is applied
            and presented for all rows in the result set.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.String[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Int32[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Int64[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Boolean[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Int16[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Single[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Double[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.IsIn(System.Decimal[])">
            <summary>
             A boolean expression that is evaluated to true if the value of this expression 
             is contained by the evaluated values of the arguments.  
            </summary>
            <param name="list">List of values to check the column against</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Expr">
            <summary>
            Gets the underlying Expression object of the <see cref="T:Microsoft.Spark.Sql.Column"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.Equals(System.Object)">
            <summary>
            Checks if the given object is equal to this object.
            </summary>
            <param name="obj">Object to compare to</param>
            <returns>True if the given object is equal to this object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.GetHashCode">
            <summary>
            Calculates the hash code for this object.
            </summary>
            <returns>Hash code for this object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.ApplyFunction(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Invokes a method under "org.apache.spark.sql.functions" with the given column.
            </summary>
            <param name="column">Column to apply function</param>
            <param name="name">Name of the function</param>
            <returns>New column after applying the function</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.ApplyMethod(System.String)">
            <summary>
            Invokes an operator (method name) with the current column.
            </summary>
            <param name="op">Operator to invoke</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.ApplyMethod(System.String,System.Object)">
            <summary>
            Invokes an operator (method name) with the current column with other object.
            </summary>
            <param name="op">Operator to invoke</param>
            <param name="other">Object to apply the operator with</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Column.ApplyMethod(System.String,System.Object,System.Object)">
            <summary>
            Invokes a method name with the current column with two other objects as parameters.
            </summary>
            <param name="op">Method to invoke</param>
            <param name="other1">Object to apply the method with</param>
            <param name="other2">Object to apply the method with</param>
            <returns>New column after applying the operator</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.RowPickler">
            <summary>
            Custom pickler for Row objects.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.GenericRowPickler">
            <summary>
            Custom pickler for GenericRow objects.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.DatePickler">
            <summary>
            Custom pickler for Date objects.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.TimestampPickler">
            <summary>
            Custom pickler for Timestamp objects.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrame">
            TODO:
            Missing APIs:
            Persist() with "StorageLevel"
            <summary>
             A distributed collection of data organized into named columns.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.DataFrame.Item(System.String)">
            <summary>
            Selects column based on the column name.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ToDF">
            <summary>
            Converts this strongly typed collection of data to generic `DataFrame`.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ToDF(System.String[])">
            <summary>
            Converts this strongly typed collection of data to generic `DataFrame`
            with columns renamed.
            </summary>
            <param name="colNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Schema">
            <summary>
            Returns the schema associated with this `DataFrame`.
            </summary>
            <returns>Schema associated with this data frame</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.PrintSchema">
            <summary>
            Prints the schema to the console in a nice tree format.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Explain(System.Boolean)">
            <summary>
            Prints the plans (logical and physical) to the console for debugging purposes.
            </summary>
            <param name="extended">prints only physical if set to false</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Columns">
            <summary>
            Returns all column names.
            </summary>
            <returns>Column names</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IsLocal">
            <summary>
            Returns true if the Collect() and Take() methods can be run locally without any
            Spark executors.
            </summary>
            <returns>True if Collect() and Take() can be run locally</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IsEmpty">
            <summary>
            Returns true if this DataFrame is empty.
            </summary>
            <returns>True if empty</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IsStreaming">
            <summary>
            Returns true if this `DataFrame` contains one or more sources that continuously
            return data as it arrives.
            </summary>
            <returns>True if streaming DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Checkpoint(System.Boolean)">
            <summary>
            Returns a checkpointed version of this `DataFrame`.
            </summary>
            <remarks>
            Checkpointing can be used to truncate the logical plan of this `DataFrame`, which is
            especially useful in iterative algorithms where the plan may grow exponentially.
            It will be saved to files inside the checkpoint directory set with
            <see cref="M:Microsoft.Spark.SparkContext.SetCheckpointDir(System.String)"/>.
            </remarks>
            <param name="eager">Whether to checkpoint this `DataFrame` immediately</param>
            <returns>Checkpointed DataFrame</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.LocalCheckpoint(System.Boolean)">
            <summary>
            Returns a locally checkpointed version of this `DataFrame`.
            </summary>
            <remarks>
            Checkpointing can be used to truncate the logical plan of this `DataFrame`, which is
            especially useful in iterative algorithms where the plan may grow exponentially.
            Local checkpoints are written to executor storage and despite potentially faster
            they are unreliable and may compromise job completion.
            </remarks>
            <param name="eager">Whether to checkpoint this `DataFrame` immediately</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WithWatermark(System.String,System.String)">
            <summary>
            Defines an event time watermark for this DataFrame. A watermark tracks a point in time
            before which we assume no more late data is going to arrive.
            </summary>
            <param name="eventTime">
            The name of the column that contains the event time of the row.
            </param>
            <param name="delayThreshold">
            The minimum delay to wait to data to arrive late, relative to the latest record that
            has been processed in the form of an interval (e.g. "1 minute" or "5 hours").
            </param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Show(System.Int32,System.Int32,System.Boolean)">
            <summary>
            Displays rows of the `DataFrame` in tabular form.
            </summary>
            <param name="numRows">Number of rows to show</param>
            <param name="truncate">If set to more than 0, truncates strings to `truncate`
                                   characters and all cells will be aligned right.</param>
            <param name="vertical">If set to true, prints output rows vertically
                                   (one line per column value).</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Na">
            <summary>
            Returns a `DataFrameNaFunctions` for working with missing data.
            </summary>
            <returns>DataFrameNaFunctions object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Stat">
             <summary>
            Returns a `DataFrameStatFunctions` for working statistic functions support.
             </summary>
             <returns>DataFrameNaFunctions object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Join with another `DataFrame`.
            </summary>
            <remarks>
            Behaves as an INNER JOIN and requires a subsequent join predicate.
            </remarks>
            <param name="right">Right side of the join operator</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame,System.String)">
            <summary>
            Inner equi-join with another `DataFrame` using the given column.
            </summary>
            <param name="right">Right side of the join operator</param>
            <param name="usingColumn">
            Name of the column to join on. This column must exist on both sides.
            </param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame,System.Collections.Generic.IEnumerable{System.String},System.String)">
            <summary>
            Equi-join with another `DataFrame` using the given columns. A cross join with
            a predicate is specified as an inner join. If you would explicitly like to
            perform a cross join use the `crossJoin` method.
            </summary>
            <param name="right">Right side of the join operator</param>
            <param name="usingColumns">Name of columns to join on</param>
            <param name="joinType">Type of join to perform. Default `inner`. Must be one of:
            `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`,
            `right_outer`, `left_semi`, `left_anti`</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Join(Microsoft.Spark.Sql.DataFrame,Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Join with another `DataFrame`, using the given join expression.
            </summary>
            <param name="right">Right side of the join operator</param>
            <param name="joinExpr">Join expression</param>
            <param name="joinType">Type of join to perform. Default `inner`. Must be one of:
            `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`, `right`,
            `right_outer`, `left_semi`, `left_anti`.</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CrossJoin(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Explicit Cartesian join with another `DataFrame`.
            </summary>
            <remarks>
            Cartesian joins are very expensive without an extra filter that can be pushed down.
            </remarks>
            <param name="right">Right side of the join operator</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.SortWithinPartitions(System.String,System.String[])">
            <summary>
            Returns a new `DataFrame` with each partition sorted by the given expressions.
            </summary>
            <remarks>
            This is the same operation as "SORT BY" in SQL (Hive QL).
            </remarks>
            <param name="column">Column name to sort by</param>
            <param name="columns">Additional column names to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.SortWithinPartitions(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` with each partition sorted by the given expressions.
            </summary>
            <remarks>
            This is the same operation as "SORT BY" in SQL (Hive QL).
            </remarks>
            <param name="columns">Column expressions to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Sort(System.String,System.String[])">
            <summary>
            Returns a new `DataFrame` sorted by the specified column, all in ascending order.
            </summary>
            <param name="column">Column name to sort by</param>
            <param name="columns">Additional column names to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Sort(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` sorted by the given expressions.
            </summary>
            <param name="columns">Column expressions to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.OrderBy(System.String,System.String[])">
            <summary>
            Returns a new Dataset sorted by the given expressions.
            </summary>
            <remarks>
            This is an alias of the Sort() function.
            </remarks>
            <param name="column">Column name to sort by</param>
            <param name="columns">Additional column names to sort by</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.OrderBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new Dataset sorted by the given expressions.
            </summary>
            <remarks>
            This is an alias of the Sort() function.
            </remarks>
            <param name="columns">Column expressions to sort by</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Hint(System.String,System.Object[])">
            <summary>
            Specifies some hint on the current `DataFrame`.
            </summary>
            <remarks>
            Due to the limitation of the type conversion between CLR and JVM,
            the type of object in `parameters` should be the same.
            </remarks>
            <param name="name">Name of the hint</param>
            <param name="parameters">Parameters of the hint</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Col(System.String)">
            <summary>
            Selects column based on the column name.
            </summary>
            <param name="colName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ColRegex(System.String)">
            <summary>
            Selects column based on the column name specified as a regex.
            </summary>
            <param name="colName">Column name as a regex</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.As(System.String)">
            <summary>
            Returns a new `DataFrame` with an alias set.
            </summary>
            <param name="alias">Alias name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Alias(System.String)">
            <summary>
            Returns a new `DataFrame` with an alias set. Same as As().
            </summary>
            <param name="alias">Alias name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Select(Microsoft.Spark.Sql.Column[])">
            <summary>
            Selects a set of column based expressions.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Select(System.String,System.String[])">
            <summary>
            Selects a set of columns. This is a variant of Select() that can only select
            existing columns using column names (i.e. cannot construct expressions).
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.SelectExpr(System.String[])">
            <summary>
            Selects a set of SQL expressions. This is a variant of Select() that
            accepts SQL expressions.
            </summary>
            <param name="expressions"></param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Filter(Microsoft.Spark.Sql.Column)">
            <summary>
            Filters rows using the given condition.
            </summary>
            <param name="condition">Condition expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Filter(System.String)">
            <summary>
            Filters rows using the given SQL expression.
            </summary>
            <param name="conditionExpr">SQL expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Where(Microsoft.Spark.Sql.Column)">
            <summary>
            Filters rows using the given condition. This is an alias for Filter().
            </summary>
            <param name="condition">Condition expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Where(System.String)">
            <summary>
            Filters rows using the given SQL expression. This is an alias for Filter().
            </summary>
            <param name="conditionExpr">SQL expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GroupBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Groups the DataFrame using the specified columns, so we can run aggregation on them.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GroupBy(System.String,System.String[])">
            <summary>
            Groups the DataFrame using the specified columns.
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Rollup(Microsoft.Spark.Sql.Column[])">
            <summary>
            Create a multi-dimensional rollup for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Rollup(System.String,System.String[])">
            <summary>
            Create a multi-dimensional rollup for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Cube(Microsoft.Spark.Sql.Column[])">
            <summary>
            Create a multi-dimensional cube for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Cube(System.String,System.String[])">
            <summary>
            Create a multi-dimensional cube for the current `DataFrame` using the
            specified columns.
            </summary>
            <param name="column">Column name</param>
            <param name="columns">Additional column names</param>
            <returns>RelationalGroupedDataset object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Agg(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column[])">
            <summary>
            Aggregates on the entire `DataFrame` without groups.
            </summary>
            <param name="expr">Column expression to aggregate</param>
            <param name="exprs">Additional column expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Limit(System.Int32)">
            <summary>
            Returns a new `DataFrame` by taking the first `number` rows.
            </summary>
            <param name="n">Number of rows to take</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Union(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing union of rows in this `DataFrame`
            and another `DataFrame`.
            </summary>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.UnionByName(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing union of rows in this `DataFrame`
            and another `DataFrame`, resolving columns by name.
            </summary>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Intersect(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows only in both this `DataFrame`
            and another `DataFrame`.
            </summary>
            <remarks>
            This is equivalent to `INTERSECT` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.IntersectAll(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows only in both this `DataFrame`
            and another `DataFrame` while preserving the duplicates.
            </summary>
            <remarks>
            This is equivalent to `INTERSECT ALL` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Except(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows in this `DataFrame` but
            not in another `DataFrame`.
            </summary>
            <remarks>
            This is equivalent to `EXCEPT DISTINCT` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ExceptAll(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Returns a new `DataFrame` containing rows in this `DataFrame` but
            not in another `DataFrame` while preserving the duplicates.
            </summary>
            <remarks>
            This is equivalent to `EXCEPT ALL` in SQL.
            </remarks>
            <param name="other">Other DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Sample(System.Double,System.Boolean,System.Nullable{System.Int64})">
            <summary>
            Returns a new `DataFrame` by sampling a fraction of rows (without replacement),
            using a user-supplied seed.
            </summary>
            <param name="fraction">Fraction of rows</param>
            <param name="withReplacement">Sample with replacement or not</param>
            <param name="seed">Optional random seed</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.RandomSplit(System.Double[],System.Nullable{System.Int64})">
            <summary>
            Randomly splits this `DataFrame` with the provided weights.
            </summary>
            <param name="weights">Weights for splits</param>
            <param name="seed">Optional random seed</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WithColumn(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a new `DataFrame` by adding a column or replacing the existing column that
            has the same name.
            </summary>
            <param name="colName">Name of the new column</param>
            <param name="col">Column expression for the new column</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WithColumnRenamed(System.String,System.String)">
            <summary>
            Returns a new Dataset with a column renamed.
            This is a no-op if schema doesn't contain `existingName`.
            </summary>
            <param name="existingName">Existing column name</param>
            <param name="newName">New column name to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Drop(System.String[])">
            <summary>
            Returns a new `DataFrame` with columns dropped.
            This is a no-op if schema doesn't contain column name(s).
            </summary>
            <param name="colNames">Name of columns to drop</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Drop(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a new `DataFrame` with a column dropped.
            This is a no-op if the `DataFrame` doesn't have a column with an equivalent expression.
            </summary>
            <param name="col">Column expression</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.DropDuplicates">
            <summary>
            Returns a new `DataFrame` that contains only the unique rows from this `DataFrame`.
            This is an alias for Distinct().
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.DropDuplicates(System.String,System.String[])">
            <summary>
            Returns a new `DataFrame` with duplicate rows removed, considering only
            the subset of columns.
            </summary>
            <param name="col">Column name</param>
            <param name="cols">Additional column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Describe(System.String[])">
            <summary>
            Computes basic statistics for numeric and string columns, including count, mean,
            stddev, min, and max. If no columns are given, this function computes statistics for
            all numerical or string columns.
            </summary>
            <remarks>
            This function is meant for exploratory data analysis, as we make no guarantee about
            the backward compatibility of the schema of the resulting DataFrame. If you want to
            programmatically compute summary statistics, use the `agg` function instead.
            </remarks>
            <param name="cols">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Summary(System.String[])">
             <summary>
             Computes specified statistics for numeric and string columns.
             </summary>
             <remarks>
             Available statistics are:
             - count
             - mean
             - stddev
             - min
             - max
             - arbitrary approximate percentiles specified as a percentage(e.g., 75%)
            
             If no statistics are given, this function computes count, mean, stddev, min,
             approximate quartiles(percentiles at 25%, 50%, and 75%), and max.
             </remarks>
             <param name="statistics">Statistics to compute</param>
             <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Head(System.Int32)">
            <summary>
            Returns the first `n` rows.
            </summary>
            <param name="n">Number of rows</param>
            <returns>First `n` rows</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Head">
            <summary>
            Returns the first row.
            </summary>
            <returns>First row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.First">
            <summary>
            Returns the first row. Alis for Head().
            </summary>
            <returns>First row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Take(System.Int32)">
            <summary>
            Returns the first `n` rows in the `DataFrame`.
            </summary>
            <param name="n">Number of rows</param>
            <returns>First `n` rows</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Collect">
            <summary>
            Returns an array that contains all rows in this `DataFrame`.
            </summary>
            <remarks>
            This requires moving all the data into the application's driver process, and
            doing so on a very large dataset can crash the driver process with OutOfMemoryError.
            </remarks>
            <returns>Row objects</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.ToLocalIterator">
            <summary>
            Returns an iterator that contains all of the rows in this `DataFrame`.
            The iterator will consume as much memory as the largest partition in this `DataFrame`.
            </summary>
            <returns>Row objects</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Count">
            <summary>
            Returns the number of rows in the `DataFrame`.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Repartition(System.Int32)">
            <summary>
            Returns a new `DataFrame` that has exactly `numPartitions` partitions.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Repartition(System.Int32,Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions into
            `numPartitions`. The resulting `DataFrame` is hash partitioned.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Repartition(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions, using
            `spark.sql.shuffle.partitions` as number of partitions.
            </summary>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.RepartitionByRange(System.Int32,Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions into
            `numPartitions`. The resulting `DataFrame` is range partitioned.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.RepartitionByRange(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a new `DataFrame` partitioned by the given partitioning expressions, using
            `spark.sql.shuffle.partitions` as number of partitions.
            The resulting Dataset is range partitioned.
            </summary>
            <param name="partitionExprs">Partitioning expressions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Coalesce(System.Int32)">
            <summary>
            Returns a new `DataFrame` that has exactly `numPartitions` partitions, when the
            fewer partitions are requested. If a larger number of partitions is requested,
            it will stay at the current number of partitions.
            </summary>
            <param name="numPartitions">Number of partitions</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Distinct">
            <summary>
            Returns a new Dataset that contains only the unique rows from this `DataFrame`.
            This is an alias for DropDuplicates().
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Persist">
            <summary>
            Persist this `DataFrame` with the default storage level (`MEMORY_AND_DISK`).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Cache">
            <summary>
            Persist this `DataFrame` with the default storage level (`MEMORY_AND_DISK`).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Unpersist(System.Boolean)">
            <summary>
            Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
            </summary>
            <remarks>
            This will not un-persist any cached data that is built upon this `DataFrame`.
            </remarks>
            <param name="blocking"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateTempView(System.String)">
            <summary>
            Creates a local temporary view using the given name. The lifetime of this
            temporary view is tied to the SparkSession that created this `DataFrame`.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateOrReplaceTempView(System.String)">
            <summary>
            Creates or replaces a local temporary view using the given name. The lifetime of this
            temporary view is tied to the SparkSession that created this `DataFrame`.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateGlobalTempView(System.String)">
            <summary>
            Creates a global temporary view using the given name. The lifetime of this
            temporary view is tied to this Spark application.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.CreateOrReplaceGlobalTempView(System.String)">
            <summary>
            Creates or replaces a global temporary view using the given name. The lifetime of this
            temporary view is tied to this Spark application.
            </summary>
            <param name="viewName">Name of the view</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.Write">
            <summary>
            Interface for saving the content of the non-streaming Dataset out
            into external storage.
            </summary>
            <returns>DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.WriteStream">
            <summary>
            Interface for saving the content of the streaming Dataset out into external storage.
            </summary>
            <returns>DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GetRows(System.String)">
            <summary>
            Returns row objects based on the function (either "toPythonIterator" or
            "collectToPython").
            </summary>
            <param name="funcName"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrame.GetConnectionInfo(System.String)">
            <summary>
            Returns a tuple of port number and secret string which are
            used for connecting with Spark to receive rows for this `DataFrame`.
            </summary>
            <returns>A tuple of port number and secret string</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameGroupedMapUdfWrapper">
            <summary>
            Wraps the given Func object, which represents a Grouped Map UDF.
            </summary>
            <remarks>
            UDF serialization requires a "wrapper" object in order to serialize/deserialize.
            </remarks>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameNaFunctions">
            <summary>
            Provides functionalities for working with missing data in <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop">
            <summary>
            Returns a new `DataFrame` that drops rows containing any null or NaN values.
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.String)">
            <summary>
            Returns a new `DataFrame` that drops rows containing null or NaN values.
            </summary>
            <remarks>
            If `how` is "any", then drop rows containing any null or NaN values.
            If `how` is "all", then drop rows only if every column is null or NaN for that row.
            </remarks>
            <param name="how">Determines the behavior of dropping rows</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that drops rows containing any null or NaN values
            in the specified columns.
            </summary>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.String,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that drops rows containing any null or NaN values
            in the specified columns.
            </summary>
            <remarks>
            If `how` is "any", then drop rows containing any null or NaN values.
            If `how` is "all", then drop rows only if every column is null or NaN for that row.
            </remarks>
            <param name="how">Determines the behavior of dropping rows</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.Int32)">
            <summary>
            Returns a new `DataFrame` that drops rows containing less than `minNonNulls`
            non-null and non-NaN values.
            </summary>
            <param name="minNonNulls"></param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Drop(System.Int32,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that drops rows containing less than `minNonNulls`
            non-null and non-NaN values in the specified columns.
            </summary>
            <param name="minNonNulls"></param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Int64)">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in numeric columns
            with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Double)">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in numeric columns
            with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.String)">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in numeric columns
            with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Int64,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified numeric
            columns. If a specified column is not a numeric column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Double,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified numeric
            columns. If a specified column is not a numeric column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.String,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified string
            columns. If a specified column is not a string column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Boolean)">
            <summary>
            Returns a new `DataFrame` that replaces null values in boolean columns with `value`.
            </summary>
            <param name="value">Value to replace with</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Boolean,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null or NaN values in specified boolean
            columns. If a specified column is not a boolean column, it is ignored.
            </summary>
            <param name="value">Value to replace with</param>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Int32})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Int64})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Double})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Fill(System.Collections.Generic.IDictionary{System.String,System.Boolean})">
            <summary>
            Returns a new `DataFrame` that replaces null values.
            </summary>
            <remarks>
            The key of the map is the column name, and the value of the map is the
            replacement value.
            </remarks>
            <param name="valueMap">Values to replace null values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.String,System.Collections.Generic.IDictionary{System.Double,System.Double})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnName">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.String,System.Collections.Generic.IDictionary{System.Boolean,System.Boolean})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnName">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.String,System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnName">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.IDictionary{System.Double,System.Double})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnNames">
            Name of the column to apply the value replacement. If `col` is "*", replacement
            is applied on all string, numeric or boolean columns.
            </param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.IDictionary{System.Boolean,System.Boolean})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnNames">list of columns to apply the value replacement.</param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameNaFunctions.Replace(System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.IDictionary{System.String,System.String})">
            <summary>
            Replaces values matching keys in `replacement` map with the corresponding values.
            </summary>
            <param name="columnNames">list of columns to apply the value replacement.</param>
            <param name="replacement">Map that stores the replacement values</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameReader">
            <summary>
            DataFrameReader provides functionality to load a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>
            from external storage systems (e.g. file systems, key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Format(System.String)">
            <summary>
            Specifies the input data source format.
            </summary>
            <param name="source">Name of the data source</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Schema(Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Specifies the schema by using <see cref="T:Microsoft.Spark.Sql.Types.StructType"/>.
            </summary>
            <remarks>
            Some data sources (e.g. JSON) can infer the input schema automatically
            from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </remarks>
            <param name="schema">The input schema</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Schema(System.String)">
            <summary>
            Specifies the schema by using the given DDL-formatted string.
            </summary>
            <remarks>
            Some data sources (e.g. JSON) can infer the input schema automatically
            from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </remarks>
            <param name="schemaString">DDL-formatted string</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.String)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.Boolean)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.Int64)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Option(System.String,System.Double)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds input options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Load">
            <summary>
            Loads input in as a DataFrame, for data sources that don't require a path
            (e.g. external key-value stores).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Load(System.String)">
            <summary>
            Loads input in as a DataFrame, for data sources that require a path 
            (e.g. data backed by a local or distributed file system).
            </summary>
            <param name="path">Input path</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Load(System.String[])">
            <summary>
            Loads input in as a DataFrame from the given paths.
            </summary>
            <remarks>
            Paths can be empty if data sources don't require a path (e.g. external
            key-value stores).
            </remarks>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Jdbc(System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Construct a DataFrame representing the database table accessible via JDBC URL
            url named table and connection properties.
            </summary>
            <param name="url">JDBC database url of the form "jdbc:subprotocol:subname"</param>
            <param name="table">Name of the table in the external database</param>
            <param name="properties">JDBC database connection arguments</param>
            <returns>DataFrame representing the database table accessible via JDBC</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Jdbc(System.String,System.String,System.String,System.Int64,System.Int64,System.Int32,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Construct a DataFrame representing the database table accessible via JDBC URL
            url named table. Partitions of the table will be retrieved in parallel based
            on the parameters passed to this function.
            </summary>
            <param name="url">JDBC database url of the form "jdbc:subprotocol:subname"</param>
            <param name="table">Name of the table in the external database</param>
            <param name="columnName">The name of a column of integral type that will be used
            for partitioning</param>
            <param name="lowerBound">The minimum value of columnName used to decide partition
            stride.</param>
            <param name="upperBound">The maximum value of columnName used to decide partition
            stride</param>
            <param name="numPartitions">
            The number of partitions. This, along with lowerBound (inclusive),
            upperBound(exclusive), form partition strides for generated WHERE
            clause expressions used to split the column columnName evenly.When
            the input is less than 1, the number is set to 1.
            </param>
            <param name="properties">JDBC database connection arguments</param>
            <returns>DataFrame representing the database table accessible via JDBC</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Jdbc(System.String,System.String,System.Collections.Generic.IEnumerable{System.String},System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Construct a DataFrame representing the database table accessible via JDBC URL
            url named table and connection properties. The predicates parameter gives a list
            expressions suitable for inclusion in WHERE clauses; each one defines one partition
            of the DataFrame.
            </summary>
            <param name="url">JDBC database url of the form "jdbc:subprotocol:subname"</param>
            <param name="table">Name of the table in the external database</param>
            <param name="predicates">Condition in the WHERE clause for each partition</param>
            <param name="properties">JDBC database connection arguments</param>
            <returns>DataFrame representing the database table accessible via JDBC</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Json(System.String[])">
            <summary>
            Loads a JSON file (one object per line) and returns the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Csv(System.String[])">
            <summary>
            Loads CSV files and returns the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Parquet(System.String[])">
            <summary>
            Loads a Parquet file, returning the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Orc(System.String[])">
            <summary>
            Loads an ORC file and returns the result as a DataFrame.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.Text(System.String[])">
            <summary>
            Loads text files and returns a DataFrame whose schema starts with a string column
            named "value", and followed by partitioned columns if there are any.
            </summary>
            <param name="paths">Input paths</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameReader.LoadSource(System.String,System.String[])">
            <summary>
            Helper function to create a DataFrame with the given source and paths.
            </summary>
            <param name="source">Name of the data source</param>
            <param name="paths">Input paths</param>
            <returns>A DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameStatFunctions">
            <summary>
            Provides statistic functions for <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.ApproxQuantile(System.String,System.Collections.Generic.IEnumerable{System.Double},System.Double)">
            <summary>
            Calculates the approximate quantiles of a numerical column of a DataFrame.
            </summary>
            <remarks>
            This method implements a variation of the Greenwald-Khanna algorithm
            (with some speed optimizations).
            </remarks>
            <param name="columnName">Column name</param>
            <param name="probabilities">A list of quantile probabilities</param>
            <param name="relativeError">
            The relative target precision to achieve (greater than or equal to 0)
            </param>
            <returns>The approximate quantiles at the given probabilities</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Cov(System.String,System.String)">
            <summary>
            Calculate the sample covariance of two numerical columns of a DataFrame.
            </summary>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <returns>The covariance of the two columns</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Corr(System.String,System.String,System.String)">
            <summary>
            Calculates the correlation of two columns of a DataFrame.
            </summary>
            <remarks>
            Currently only the Pearson Correlation Coefficient is supported.
            </remarks>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <param name="method">Method name for calculating correlation</param>
            <returns>The Pearson Correlation Coefficient</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Corr(System.String,System.String)">
            <summary>
            Calculates the Pearson Correlation Coefficient of two columns of a DataFrame.
            </summary>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <returns>The Pearson Correlation Coefficient</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.Crosstab(System.String,System.String)">
            <summary>
            Computes a pair-wise frequency table of the given columns, also known as 
            a contingency table.
            </summary>
            <param name="colName1">First column name</param>
            <param name="colName2">Second column name</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.FreqItems(System.Collections.Generic.IEnumerable{System.String},System.Double)">
            <summary>
            Finding frequent items for columns, possibly with false positives.
            </summary>
            <param name="columnNames">Column names</param>
            <param name="support">
            The minimum frequency for an item to be considered frequent.
            Should be greater than 1e-4.
            </param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.FreqItems(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Finding frequent items for columns, possibly with false positives with
            a default support of 1%.
            </summary>
            <param name="columnNames">Column names</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameStatFunctions.SampleBy``1(System.String,System.Collections.Generic.IDictionary{``0,System.Double},System.Int64)">
            <summary>
            Returns a stratified sample without replacement based on the fraction given
            on each stratum.
            </summary>
            <typeparam name="T">Stratum type</typeparam>
            <param name="columnName">Column name that defines strata</param>
            <param name="fractions">
            Sampling fraction for each stratum. If a stratum is not specified, we treat
            its fraction as zero.
            </param>
            <param name="seed">Random seed</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper">
            <summary>
            An abstract class to detect DataFrameUdfWrapper derivatives at runtime
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`2">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`3">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`4">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`5">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`6">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`7">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`8">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`9">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`10">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameUdfWrapper`11">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameWorkerFunction">
            <summary>
            Function that will be executed in the worker using a <see cref="T:Microsoft.Data.Analysis.DataFrame"/> that supports the Apache Arrow format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameWorkerFunction.ExecuteDelegate">
            <summary>
            Type of the UDF to run. Refer to <see cref="T:Microsoft.Spark.Sql.ArrowUdfWrapper`2"/>.Execute.
            </summary>
            <param name="input">unpickled data, representing a row</param>
            <param name="argOffsets">offsets to access input</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWorkerFunction.Chain(Microsoft.Spark.Sql.DataFrameWorkerFunction,Microsoft.Spark.Sql.DataFrameWorkerFunction)">
            <summary>
            Used to chain functions.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.DataFrameWorkerFunction.WorkerFuncChainHelper.s_outerFuncArgOffsets">
            <summary>
            The outer function will always take 0 as an offset since there is only one
            return value from an inner function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameGroupedMapWorkerFunction">
            <summary>
            Function for Grouped Map Vector UDFs using a <see cref="T:Microsoft.Data.Analysis.DataFrame"/> that supports the Apache Arrow format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameGroupedMapWorkerFunction.ExecuteDelegate">
            <summary>
            A delegate to invoke a Grouped Map Vector UDF.
            </summary>
            <param name="input">The input data frame.</param>
            <returns>The resultant data frame.</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.DataFrameWriter">
            <summary>
            Interface used to write a DataFrame to external storage systems (e.g. file systems,
            key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Mode(Microsoft.Spark.Sql.SaveMode)">
            <summary>
            Specifies the behavior when data or table already exists.
            </summary>
            <remarks>
            Options include:
              - SaveMode.Overwrite: overwrite the existing data.
              - SaveMode.Append: append the data.
              - SaveMode.Ignore: ignore the operation (i.e. no-op).
              - SaveMode.ErrorIfExists: default option, throw an exception at runtime.
            </remarks>
            <param name="saveMode">Save mode enum</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Mode(System.String)">
            <summary>
            Specifies the behavior when data or table already exists.
            </summary>
            <remarks>
            Options include:
              - "overwrite": overwrite the existing data.
              - "append": append the data.
              - "ignore": ignore the operation (i.e.no-op).
              - "error" or "errorifexists": default option, throw an exception at runtime.
            </remarks>
            <param name="saveMode">Save mode string</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Format(System.String)">
            <summary>
            Specifies the underlying output data source. Built-in options include
            "parquet", "json", etc.
            </summary>
            <param name="source">Data source name</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.String)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.Boolean)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.Int64)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Option(System.String,System.Double)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds output options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.PartitionBy(System.String[])">
            <summary>
            Partitions the output by the given columns on the file system. If specified,
            the output is laid out on the file system similar to Hive's partitioning scheme.
            </summary>
            <param name="colNames">Column names to partition by</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.BucketBy(System.Int32,System.String,System.String[])">
            <summary>
            Buckets the output by the given columns. If specified, the output is laid out
            on the file system similar to Hive's bucketing scheme.
            </summary>
            <param name="numBuckets">Number of buckets to save</param>
            <param name="colName">A column name</param>
            <param name="colNames">Additional column names</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.SortBy(System.String,System.String[])">
            <summary>
            Sorts the output in each bucket by the given columns.
            </summary>
            <param name="colName">A name of a column</param>
            <param name="colNames">Additional column names</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Save(System.String)">
            <summary>
            Saves the content of the DataFrame at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Save">
            <summary>
            Saves the content of the DataFrame as the specified table.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.InsertInto(System.String)">
            <summary>
            Inserts the content of the DataFrame to the specified table. It requires that
            the schema of the DataFrame is the same as the schema of the table.
            </summary>
            <param name="tableName">Name of the table</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.SaveAsTable(System.String)">
            <summary>
            Saves the content of the DataFrame as the specified table.
            </summary>
            <param name="tableName">Name of the table</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Jdbc(System.String,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Saves the content of the DataFrame to a external database table via JDBC
            </summary>
            <param name="url">JDBC database URL of the form "jdbc:subprotocol:subname"</param>
            <param name="table">Name of the table in the external database</param>
            <param name="properties">JDBC database connection arguments</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Json(System.String)">
            <summary>
            Saves the content of the DataFrame in JSON format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Parquet(System.String)">
            <summary>
            Saves the content of the DataFrame in Parquet format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Orc(System.String)">
            <summary>
            Saves the content of the DataFrame in ORC format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Text(System.String)">
            <summary>
            Saves the content of the DataFrame in a text file at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.Csv(System.String)">
            <summary>
            Saves the content of the DataFrame in CSV format at the specified path.
            </summary>
            <param name="path">Path to save the content</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.DataFrameWriter.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameWriter object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Expressions.UserDefinedFunction">
            <summary>
            UserDefinedFunction is not exposed to the user directly.
            Use Functions.Udf to create this object indirectly.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Expressions.Window">
            <summary>
            Utility functions for defining window in DataFrames.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Expressions.Window.UnboundedPreceding">
            <summary>
            Value representing the first row in the partition, equivalent to
            "UNBOUNDED PRECEDING" in SQL.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Expressions.Window.UnboundedFollowing">
            <summary>
            Value representing the last row in the partition, equivalent to
            "UNBOUNDED FOLLOWING" in SQL.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Expressions.Window.CurrentRow">
            <summary>
            Value representing the current row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.PartitionBy(System.String,System.String[])">
            <summary>
            Creates a `WindowSpec` with the partitioning defined.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.PartitionBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a `WindowSpec` with the partitioning defined.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.OrderBy(System.String,System.String[])">
            <summary>
            Creates a `WindowSpec` with the ordering defined.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.OrderBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a `WindowSpec` with the ordering defined.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.RowsBetween(System.Int64,System.Int64)">
            <summary>
            Creates a `WindowSpec` with the frame boundaries defined,
            from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is the
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.RangeBetween(System.Int64,System.Int64)">
            <summary>
            Creates a `WindowSpec` with the frame boundaries defined,
            from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.Window.RangeBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a `WindowSpec` with the frame boundaries defined,
            from `start` (inclusive) to `end` (inclusive).
            </summary>
            <remarks>
            This API is deprecated in Spark 2.4 and removed in Spark 3.0.
            </remarks>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedPreceding()`
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedFollowing()`
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Expressions.WindowSpec">
            <summary>
            A window specification that defines the partitioning, ordering, and frame boundaries.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.PartitionBy(System.String,System.String[])">
            <summary>
            Defines the partitioning columns in a `WindowSpec`.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.PartitionBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Defines the partitioning columns in a `WindowSpec`.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.OrderBy(System.String,System.String[])">
            <summary>
            Defines the ordering columns in a `WindowSpec`.
            </summary>
            <param name="colName">Name of column</param>
            <param name="colNames">Additional column names</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.OrderBy(Microsoft.Spark.Sql.Column[])">
            <summary>
            Defines the ordering columns in a `WindowSpec`.
            </summary>
            <param name="columns">Column expressions</param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.RowsBetween(System.Int64,System.Int64)">
            <summary>
            Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is the
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.RangeBetween(System.Int64,System.Int64)">
            <summary>
            Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).
            </summary>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if this is
            the minimum long value `Window.s_unboundedPreceding`.
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if this is
            maximum long value `Window.s_unboundedFollowing`.
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Expressions.WindowSpec.RangeBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).
            </summary>
            <remarks>
            This API is deprecated in Spark 2.4 and removed in Spark 3.0.
            </remarks>
            <param name="start">
            Boundary start, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedPreceding()`
            </param>
            <param name="end">
            Boundary end, inclusive. The frame is unbounded if the expression is
            `Microsoft.Spark.Sql.Functions.UnboundedFollowing()`
            </param>
            <returns>WindowSpec object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.IForeachWriter">
             <summary>
             Interface for writing custom logic to process data generated by a query. This is
             often used to write the output of a streaming query to arbitrary storage systems.
             </summary>
             <remarks>
             <para>
             Any implementation of this interface will be used by Spark in the following way:
             <list type="bullet">
             <item>
             <description>
             A single instance of this class is responsible of all the data generated by a single task
             in a query. In other words, one instance is responsible for processing one partition of the
             data generated in a distributed manner.
             </description>
             </item>
             <item>
             <description>
             Any implementation of this class must be <see cref="T:System.SerializableAttribute"/> because each
             task will get a fresh serialized-deserialized copy of the provided object. Hence, it is
             strongly recommended that any initialization for writing data (e.g.opening a connection or
             starting a transaction) is done after the <see cref="M:Microsoft.Spark.Sql.IForeachWriter.Open(System.Int64,System.Int64)"/> method has been
             called, which signifies that the task is ready to generate data.
             </description>
             </item>
             <item>
             <description>
             The lifecycle of the methods are as follows:
             <example>
             <para/>For each partition with <c>partitionId</c>:
             <para/>... For each batch/epoch of streaming data(if its streaming query) with <c>epochId</c>:
             <para/>....... Method <c>Open(partitionId, epochId)</c> is called.
             <para/>....... If <c>Open</c> returns true:
             <para/>........... For each row in the partition and batch/epoch, method <c>Process(row)</c> is called.
             <para/>....... Method <c>Close(errorOrNull)</c> is called with error(if any) seen while processing rows.
             </example>
             </description>
             </item>
             </list>
             </para>
            
             <para>
             Important points to note:
             <list type="bullet">
             <item>
             <description>
             The <c>partitionId</c> and <c>epochId</c> can be used to deduplicate generated data
             when failures cause reprocessing of some input data. This depends on the execution
             mode of the query. If the streaming query is being executed in the micro-batch
             mode, then every partition represented by a unique tuple(partition_id, epoch_id)
             is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used
             to deduplicate and/or transactionally commit data and achieve exactly-once
             guarantees. However, if the streaming query is being executed in the continuous
             mode, then this guarantee does not hold and therefore should not be used for
             deduplication.
             </description>
             </item>
             </list>
             </para>
             </remarks>
        </member>
        <member name="M:Microsoft.Spark.Sql.IForeachWriter.Open(System.Int64,System.Int64)">
            <summary>
            Called when starting to process one partition of new data in the executor.
            </summary>
            <param name="partitionId">The partition id.</param>
            <param name="epochId">A unique id for data deduplication.</param>
            <returns>True if successful, false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.IForeachWriter.Process(Microsoft.Spark.Sql.Row)">
            <summary>
            Called to process each <see cref="T:Microsoft.Spark.Sql.Row"/> in the executor side. This method
            will be called only if <c>Open</c> returns <c>true</c>.
            </summary>
            <param name="row">The row to process.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.IForeachWriter.Close(System.Exception)">
            <summary>
            Called when stopping to process one partition of new data in the executor side. This is
            guaranteed to be called either <see cref="M:Microsoft.Spark.Sql.IForeachWriter.Open(System.Int64,System.Int64)"/> returns <c>true</c> or
            <c>false</c>. However, <see cref="M:Microsoft.Spark.Sql.IForeachWriter.Close(System.Exception)"/> won't be called in the following
            cases:
            <list type="bullet">
            <item>
            <description>
            CLR/JVM crashes without throwing a <see cref="T:System.Exception"/>.
            </description>
            </item>
            <item>
            <description>
            <see cref="M:Microsoft.Spark.Sql.IForeachWriter.Open(System.Int64,System.Int64)"/> throws an <see cref="T:System.Exception"/>.
            </description>
            </item>
            </list>
            </summary>
            <param name="errorOrNull">
            The <see cref="T:System.Exception"/> thrown during processing or null if there was no error.
            </param>
        </member>
        <member name="T:Microsoft.Spark.Sql.ForeachWriterWrapper">
            <summary>
            Wraps a <see cref="T:Microsoft.Spark.Sql.IForeachWriter"/> and calls the appropriate methods as decribed in
            the lifecycle documentation for the interface.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ForeachWriterWrapperUdfWrapper">
            <summary>
            Wraps the given Func object, which represents a <see cref="T:Microsoft.Spark.Sql.ForeachWriterWrapper"/> UDF.
            When this UdfWrapper is processed, the PythonEvalType is
            <see cref="F:Microsoft.Spark.Utils.UdfUtils.PythonEvalType.NON_UDF"/>. The CommandExecutor expects the 
            <see cref="M:Microsoft.Spark.Sql.ForeachWriterWrapperUdfWrapper.Execute(System.Int32,System.Collections.Generic.IEnumerable{System.Object})"/> method to match the
            <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/> delegate. This UdfWrapper helps map 
            the UDF for <see cref="T:Microsoft.Spark.Sql.ForeachWriterWrapper"/> to <see cref="T:Microsoft.Spark.RDD.WorkerFunction.ExecuteDelegate"/>.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Functions">
            <summary>
            Functions available for DataFrame operations.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Column(System.String)">
            <summary>
            Returns a Column based on the given column name.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Col(System.String)">
            <summary>
            Returns a Column based on the given column name. Alias for Column().
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lit(System.Object)">
            <summary>
            Creates a Column of literal value.
            </summary>
            <param name="literal">Literal value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Asc(System.String)">
            <summary>
            Returns a sort expression based on the ascending order of the column.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.AscNullsFirst(System.String)">
            <summary>
            Returns a sort expression based on the ascending order of the column,
            and null values return before non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.AscNullsLast(System.String)">
            <summary>
            Returns a sort expression based on the ascending order of the column,
            and null values appear after non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Desc(System.String)">
            <summary>
            Returns a sort expression based on the descending order of the column.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DescNullsFirst(System.String)">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values return before non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DescNullsLast(System.String)">
            <summary>
            Returns a sort expression based on the descending order of the column,
            and null values appear after non-null values.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(System.String)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <param name="rsd">Maximum estimation error allowed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ApproxCountDistinct(System.String,System.Double)">
            <summary>
            Returns the approximate number of distinct items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <param name="rsd">Maximum estimation error allowed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Avg(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the average of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Avg(System.String)">
            <summary>
            Returns the average of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectList(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a list of objects with duplicates.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectList(System.String)">
            <summary>
            Returns a list of objects with duplicates.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectSet(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a set of objects with duplicate elements eliminated.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CollectSet(System.String)">
            <summary>
            Returns a set of objects with duplicate elements eliminated.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Corr(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the Pearson Correlation Coefficient for two columns.
            </summary>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Corr(System.String,System.String)">
            <summary>
            Returns the Pearson Correlation Coefficient for two columns.
            </summary>
            <param name="columnName1">Column one name</param>
            <param name="columnName2">Column two name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Count(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the number of items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Count(System.String)">
            <summary>
            Returns the number of items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CountDistinct(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <param name="column">Column to apply</param>
            <param name="columns">Additional columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CountDistinct(System.String,System.String[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarPop(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the population covariance for two columns.
            </summary>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarPop(System.String,System.String)">
            <summary>
            Returns the population covariance for two columns.
            </summary>
            <param name="columnName1">Column one name</param>
            <param name="columnName2">Column two name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarSamp(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sample covariance for two columns.
            </summary>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CovarSamp(System.String,System.String)">
            <summary>
            Returns the sample covariance for two columns.
            </summary>
            <param name="columnName1">Column one name</param>
            <param name="columnName2">Column two name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.First(Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Returns the first value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the first values it sees. It will return
            the first non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.First(System.String,System.Boolean)">
            <summary>
            Returns the first value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the first values it sees. It will return
            the first non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="columnName">Column name</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Grouping(Microsoft.Spark.Sql.Column)">
            <summary>
            Indicates whether a specified column in a GROUP BY list is aggregated
            or not, returning 1 for aggregated or 0 for not aggregated in the result set.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Grouping(System.String)">
            <summary>
            Indicates whether a specified column in a GROUP BY list is aggregated
            or not, returning 1 for aggregated or 0 for not aggregated in the result set.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.GroupingId(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <remarks>
            The list of columns should match with grouping columns exactly, or empty
            (meaning all the grouping columns).
            </remarks>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.GroupingId(System.String,System.String[])">
            <summary>
            Returns the number of distinct items in a group.
            </summary>
            <remarks>
            The list of columns should match with grouping columns exactly.
            </remarks>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Kurtosis(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the kurtosis of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Kurtosis(System.String)">
            <summary>
            Returns the kurtosis of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Last(Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Returns the last value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the last values it sees. It will return
            the last non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Last(System.String,System.Boolean)">
            <summary>
            Returns the last value of a column in a group.
            </summary>
            <remarks>
            The function by default returns the last values it sees. It will return
            the last non-null value it sees when ignoreNulls is set to true.
            If all values are null, then null is returned.
            </remarks>
            <param name="columnName">Column name</param>
            <param name="ignoreNulls">To ignore null or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Max(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the maximum value of the column in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Max(System.String)">
            <summary>
            Returns the maximum value of the column in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Mean(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the average value of the column in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Mean(System.String)">
            <summary>
            Returns the average value of the column in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Min(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the minimum value of the column in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Min(System.String)">
            <summary>
            Returns the minimum value of the column in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Skewness(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the skewness of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Skewness(System.String)">
            <summary>
            Returns the skewness of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Stddev(Microsoft.Spark.Sql.Column)">
            <summary>
            Alias for StddevSamp().
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Stddev(System.String)">
            <summary>
            Alias for StddevSamp().
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevSamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sample standard deviation of the expression in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevSamp(System.String)">
            <summary>
            Returns the sample standard deviation of the expression in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevPop(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the population standard deviation of the expression in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.StddevPop(System.String)">
            <summary>
            Returns the population standard deviation of the expression in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sum(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sum of all values in the expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sum(System.String)">
            <summary>
            Returns the sum of all values in the expression.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SumDistinct(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the sum of distinct values in the expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SumDistinct(System.String)">
            <summary>
            Returns the sum of distinct values in the expression.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Variance(Microsoft.Spark.Sql.Column)">
            <summary>
            Alias for VarSamp().
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Variance(System.String)">
            <summary>
            Alias for VarSamp().
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarSamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the unbiased variance of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarSamp(System.String)">
            <summary>
            Returns the unbiased variance of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarPop(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the population variance of the values in a group.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VarPop(System.String)">
            <summary>
            Returns the population variance of the values in a group.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnboundedPreceding">
            <summary>
            Window function: returns the special frame boundary that represents the first
            row in the window partition.
            </summary>
            <remarks>
            This API is deprecated in Spark 2.4 and removed in Spark 3.0.
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnboundedFollowing">
            <summary>
            Window function: returns the special frame boundary that represents the last
            row in the window partition.
            </summary>
            <remarks>
            This API is deprecated in Spark 2.4 and removed in Spark 3.0.
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CurrentRow">
            <summary>
            Window function: returns the special frame boundary that represents the current
            row in the window partition.
            </summary>
            <remarks>
            This API is deprecated in Spark 2.4 and removed in Spark 3.0.
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CumeDist">
            <summary>
            Window function: returns the cumulative distribution of values within a window
            partition, i.e. the fraction of rows that are below the current row.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DenseRank">
            <summary>
            Window function: returns the rank of rows within a window partition, without any gaps.
            </summary>
            <remarks>This is equivalent to the DENSE_RANK function in SQL.</remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lag(Microsoft.Spark.Sql.Column,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows before the current row,
            and null if there is less than 'offset' rows before the current row.
            For example, an 'offset' of one will return the previous row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LAG function in SQL.</remarks>
            <param name="column">Column to apply</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lag(System.String,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows before the current row,
            and null if there is less than 'offset' rows before the current row.
            For example, an 'offset' of one will return the previous row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LAG function in SQL.</remarks>
            <param name="columnName">Column name</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lead(Microsoft.Spark.Sql.Column,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows after the current row,
            and null if there is less than 'offset' rows after the current row.
            For example, an 'offset' of one will return the next row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LEAD function in SQL.</remarks>
            <param name="column">Column to apply</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lead(System.String,System.Int32,System.Object)">
            <summary>
            Window function: returns the value that is 'offset' rows after the current row,
            and null if there is less than 'offset' rows after the current row.
            For example, an 'offset' of one will return the next row at any given point
            in the window partition.
            </summary>
            <remarks>This is equivalent to the LEAD function in SQL.</remarks>
            <param name="columnName">Column name</param>
            <param name="offset">Offset from the current row</param>
            <param name="defaultValue">Default value when the offset row doesn't exist</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ntile(System.Int32)">
            <summary>
            Window function: returns the ntile group id (from 1 to `n` inclusive) in an ordered
            window partition. For example, if `n` is 4, the first quarter of the rows will get
            value 1, the second quarter will get 2, the third quarter will get 3, and the last
            quarter will get 4.
            </summary>
            <remarks>This is equivalent to the NTILE function in SQL.</remarks>
            <param name="n">Number of buckets</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.PercentRank">
            <summary>
            Window function: returns the relative rank (i.e. percentile) of rows within
            a window partition.
            </summary>
            <remarks>This is equivalent to the PERCENT_RANK function in SQL.</remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rank">
            <summary>
            Window function: returns the rank of rows within a window partition.
            </summary>
            <remarks>This is equivalent to the RANK function in SQL.</remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RowNumber">
            <summary>
            Window function: returns a sequential number starting at 1 within a window partition.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Abs(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the absolute value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Array(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a new array column. The input columns must all have the same data type.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Array(System.String,System.String[])">
            <summary>
            Creates a new array column. The input columns must all have the same data type.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Map(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a new map column.
            </summary>
            <remarks>
            The input columns must be grouped as key-value pairs, e.g.
            (key1, value1, key2, value2, ...). The key columns must all have the same data type,
            and can't be null. The value columns must all have the same data type.
            </remarks>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapFromArrays(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new map column. The array in the first column is used for keys. The array
            in the second column is used for values. All elements in the array for key should
            not be null.
            </summary>
            <param name="key">Column expression for key</param>
            <param name="values">Column expression for values</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Broadcast(Microsoft.Spark.Sql.DataFrame)">
            <summary>
            Marks a DataFrame as small enough for use in broadcast joins.
            </summary>
            <param name="df">DataFrame to apply</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Coalesce(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the first column that is not null, or null if all inputs are null.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.InputFileName">
            <summary>
            Creates a string column for the file name of the current Spark task.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.IsNaN(Microsoft.Spark.Sql.Column)">
            <summary>
            Return true iff the column is NaN.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.IsNull(Microsoft.Spark.Sql.Column)">
            <summary>
            Return true iff the column is null.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MonotonicallyIncreasingId">
            <summary>
            A column expression that generates monotonically increasing 64-bit integers.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.NaNvl(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns col1 if it is not NaN, or col2 if col1 is NaN.
            </summary>
            <remarks>
            Both inputs should be floating point columns (DoubleType or FloatType).
            </remarks>
            <param name="column1">Column one to apply</param>
            <param name="column2">Column two to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Negate(Microsoft.Spark.Sql.Column)">
            <summary>
            Unary minus, i.e. negate the expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Not(Microsoft.Spark.Sql.Column)">
            <summary>
            Inversion of boolean expression, i.e. NOT.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rand(System.Int64)">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from U[0.0, 1.0].
            </summary>
            <remarks>
            This is non-deterministic when data partitions are not fixed.
            </remarks>
            <param name="seed">Random seed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rand">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from U[0.0, 1.0].
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Randn(System.Int64)">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from the standard normal distribution.
            </summary>
            <remarks>
            This is non-deterministic when data partitions are not fixed.
            </remarks>
            <param name="seed">Random seed</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Randn">
            <summary>
            Generate a random column with independent and identically distributed (i.i.d.)
            samples from the standard normal distribution.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SparkPartitionId">
            <summary>
            Partition ID.
            </summary>
            <remarks>
            This is non-deterministic because it depends on data partitioning and task scheduling.
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sqrt(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the square root of the specified float value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sqrt(System.String)">
            <summary>
            Computes the square root of the specified float value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Struct(Microsoft.Spark.Sql.Column[])">
            <summary>
            Creates a new struct column that composes multiple input columns.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Struct(System.String,System.String[])">
            <summary>
            Creates a new struct column that composes multiple input columns.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.When(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Evaluates a condition and returns one of multiple possible result expressions.
            If otherwise is not defined at the end, null is returned for
            unmatched conditions.
            </summary>
            <param name="condition">The condition to check.</param>
            <param name="value">The value to set if the condition is true.</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.BitwiseNOT(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes bitwise NOT.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Expr(System.String)">
            <summary>
            Parses the expression string into the column that it represents.
            </summary>
            <param name="expr">Expression string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Acos(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse cosine of `column` in radians, as if computed by `java.lang.Math.acos`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Acos(System.String)">
            <summary>
            Inverse cosine of `columnName` in radians, as if computed by `java.lang.Math.acos`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Asin(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse sine of `column` in radians, as if computed by `java.lang.Math.asin`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Asin(System.String)">
            <summary>
            Inverse sine of `columnName` in radians, as if computed by `java.lang.Math.asin`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse tangent of `column` in radians, as if computed by `java.lang.Math.atan`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan(System.String)">
            <summary>
            Inverse tangent of `columnName` in radians, as if computed by `java.lang.Math.atan`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="y">Coordinate on y-axis</param>
            <param name="x">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="y">Coordinate on y-axis</param>
            <param name="xName">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yName">Coordinate on y-axis</param>
            <param name="x">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.String,System.String)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yName">Coordinate on y-axis</param>
            <param name="xName">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="y">Coordinate on y-axis</param>
            <param name="xValue">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.String,System.Double)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yName">Coordinate on y-axis</param>
            <param name="xValue">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yValue">Coordinate on y-axis</param>
            <param name="x">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Atan2(System.Double,System.String)">
            <summary>
            Computes atan2 for the given `x` and `y`.
            </summary>
            <param name="yValue">Coordinate on y-axis</param>
            <param name="xName">Coordinate on x-axis</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bin(Microsoft.Spark.Sql.Column)">
            <summary>
            An expression that returns the string representation of the binary value
            of the given long column. For example, bin("12") returns "1100".
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bin(System.String)">
            <summary>
            An expression that returns the string representation of the binary value
            of the given long column. For example, bin("12") returns "1100".
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cbrt(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the cube-root of the given column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cbrt(System.String)">
            <summary>
            Computes the cube-root of the given column.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ceil(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the ceiling of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ceil(System.String)">
            <summary>
            Computes the ceiling of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Conv(Microsoft.Spark.Sql.Column,System.Int32,System.Int32)">
            <summary>
            Convert a number in a string column from one base to another.
            </summary>
            <param name="column">Column to apply</param>
            <param name="fromBase">Source base number</param>
            <param name="toBase">Target base number</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cos(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes cosine of the angle, as if computed by `java.lang.Math.cos`
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cos(System.String)">
            <summary>
            Computes cosine of the angle, as if computed by `java.lang.Math.cos`
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cosh(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Cosh(System.String)">
            <summary>
            Computes hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Exp(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the exponential of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Exp(System.String)">
            <summary>
            Computes the exponential of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Expm1(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the exponential of the given value minus one.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Expm1(System.String)">
            <summary>
            Computes the exponential of the given value minus one.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Factorial(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the factorial of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Floor(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the floor of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Floor(System.String)">
            <summary>
            Computes the floor of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Greatest(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the greatest value of the list of values, skipping null values.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Greatest(System.String,System.String[])">
            <summary>
            Returns the greatest value of the list of column names, skipping null values.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hex(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hex value of the given column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Unhex(Microsoft.Spark.Sql.Column)">
            <summary>
            Inverse of hex. Interprets each pair of characters as a hexadecimal number
            and converts to the byte representation of number.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.String,System.String)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.String,System.Double)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side value</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hypot(System.Double,System.String)">
            <summary>
            Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
            </summary>
            <param name="left">Left side value</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Least(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the least value of the list of values, skipping null values.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Least(System.String,System.String[])">
            <summary>
            Returns the least value of the list of values, skipping null values.
            </summary>
            <param name="columnName">Column name</param>
            <param name="columnNames">Additional column names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the natural logarithm of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(System.String)">
            <summary>
            Computes the natural logarithm of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the first argument-base logarithm of the second argument.
            </summary>
            <param name="logBase">Base for logarithm</param>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log(System.Double,System.String)">
            <summary>
            Computes the first argument-base logarithm of the second argument.
            </summary>
            <param name="logBase">Base for logarithm</param>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log10(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the logarithm of the given value in base 10.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log10(System.String)">
            <summary>
            Computes the logarithm of the given value in base 10.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log1p(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the natural logarithm of the given value plus one.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log1p(System.String)">
            <summary>
            Computes the natural logarithm of the given value plus one.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log2(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the logarithm of the given column in base 2.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Log2(System.String)">
            <summary>
            Computes the logarithm of the given column in base 2.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.String,System.String)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(Microsoft.Spark.Sql.Column,System.Double)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.String,System.Double)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="leftName">Left side column name</param>
            <param name="right">Right side value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.Double,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side value</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pow(System.Double,System.String)">
            <summary>
            Returns the value of the first argument raised to the power of the second argument.
            </summary>
            <param name="left">Left side value</param>
            <param name="rightName">Right side column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Pmod(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the positive value of dividend mod divisor.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rint(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the double value that is closest in value to the argument and
            is equal to a mathematical integer.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rint(System.String)">
            <summary>
            Returns the double value that is closest in value to the argument and
            is equal to a mathematical integer.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Round(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the `column` rounded to 0 decimal places with
            HALF_UP round mode.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Round(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the value of the `column` rounded to `scale` decimal places with
            HALF_UP round mode.
            </summary>
            <param name="column">Column to apply</param>
            <param name="scale">Scale factor</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bround(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the value of the `column` rounded to 0 decimal places with
            HALF_EVEN round mode.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Bround(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the value of the `column` rounded to `scale` decimal places with
            HALF_EVEN round mode.
            </summary>
            <param name="column">Column to apply</param>
            <param name="scale">Scale factor</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ShiftLeft(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Shift the given value `numBits` left.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">Number of bits to shift</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ShiftRight(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            (Signed) shift the given value `numBits` right.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">Number of bits to shift</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ShiftRightUnsigned(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Unsigned shift the given value `numBits` right.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">Number of bits to shift</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Signum(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the signum of the given value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Signum(System.String)">
            <summary>
            Computes the signum of the given value.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sin(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sin(System.String)">
            <summary>
            Computes sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sinh(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hyperbolic sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sinh(System.String)">
            <summary>
            Computes hyperbolic sine of the angle, as if computed by `java.lang.Math.sin`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tan(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes tangent of the given value, as if computed by `java.lang.Math.tan`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tan(System.String)">
            <summary>
            Computes tangent of the given value, as if computed by `java.lang.Math.tan`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tanh(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes hyperbolic tangent of the given value, as if computed by
            `java.lang.Math.tanh`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Tanh(System.String)">
            <summary>
            Computes hyperbolic tangent of the given value, as if computed by
            `java.lang.Math.tanh`.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Degrees(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts an angle measured in radians to an approximately equivalent angle
            measured in degrees.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Degrees(System.String)">
            <summary>
            Converts an angle measured in radians to an approximately equivalent angle
            measured in degrees.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Radians(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts an angle measured in degrees to an approximately equivalent angle
            measured in radians.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Radians(System.String)">
            <summary>
            Converts an angle measured in degrees to an approximately equivalent angle
            measured in radians.
            </summary>
            <param name="columnName">Column name</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Md5(Microsoft.Spark.Sql.Column)">
            <summary>
            Calculates the MD5 digest of a binary column and returns the value
            as a 32 character hex string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sha1(Microsoft.Spark.Sql.Column)">
            <summary>
            Calculates the SHA-1 digest of a binary column and returns the value
            as a 40 character hex string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sha2(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Calculates the SHA-2 family of hash functions of a binary column and
            returns the value as a hex string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="numBits">One of 224, 256, 384 or 512</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Crc32(Microsoft.Spark.Sql.Column)">
            <summary>
            Calculates the cyclic redundancy check value  (CRC32) of a binary column and
            returns the value as a bigint.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hash(Microsoft.Spark.Sql.Column[])">
            <summary>
            Calculates the hash code of given columns, and returns the result as an int column.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ascii(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the numeric value of the first character of the string column, and returns
            the result as an int column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Base64(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the BASE64 encoding of a binary column and returns it as a string column.
            </summary>
            <remarks>
            This is the reverse of unbase64.
            </remarks>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ConcatWs(System.String,Microsoft.Spark.Sql.Column[])">
            <summary>
            Concatenates multiple input string columns together into a single string column,
            using the given separator.
            </summary>
            <param name="sep">Separator used for string concatenation</param>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Decode(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes the first argument into a string from a binary using the provided
            character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE',
            'UTF-16')
            </summary>
            <param name="column">Column to apply</param>
            <param name="charset">Character set</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Encode(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Computes the first argument into a binary from a string using the provided
            character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE',
            'UTF-16')
            </summary>
            <param name="column">Column to apply</param>
            <param name="charset">Character set</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FormatNumber(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Formats the given numeric `column` to a format like '#,###,###.##',
            rounded to the given `d` decimal places with HALF_EVEN round mode,
            and returns the result as a string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="d">Decimal places for rounding</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FormatString(System.String,Microsoft.Spark.Sql.Column[])">
            <summary>
            Formats the arguments in printf-style and returns the result as a string column.
            </summary>
            <param name="format">Printf-style format</param>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.InitCap(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a new string column by converting the first letter of each word to uppercase.
            Words are delimited by whitespace.
            </summary>
            <remarks>
            </remarks>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Instr(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Locate the position of the first occurrence of the given substring.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index. Returns 0 if the given substring
            could not be found.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="substring">Substring to find</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Length(Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the character length of a given string or number of bytes of a binary string.
            </summary>
            <remarks>
            The length of character strings includes the trailing spaces. The length of binary
            strings includes binary zeros.
            </remarks>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lower(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts a string column to lower case.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Levenshtein(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Computes the Levenshtein distance of the two given string columns.
            </summary>
            <param name="left">Left side column to apply</param>
            <param name="right">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Locate(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Locate the position of the first occurrence of the given substring.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index. Returns 0 if the given substring
            could not be found.
            </remarks>
            <param name="substring">Substring to find</param>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Locate(System.String,Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Locate the position of the first occurrence of the given substring
            starting from the given position offset.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index. Returns 0 if the given substring
            could not be found.
            </remarks>
            <param name="substring">Substring to find</param>
            <param name="column">Column to apply</param>
            <param name="pos">Offset to start the search</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Lpad(Microsoft.Spark.Sql.Column,System.Int32,System.String)">
            <summary>
            Left-pad the string column with pad to the given length `len`. If the string column is
            longer than `len`, the return value is shortened to `len` characters.
            </summary>
            <param name="column">Column to apply</param>
            <param name="len">Length of padded string</param>
            <param name="pad">String used for padding</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ltrim(Microsoft.Spark.Sql.Column)">
            <summary>
            Trim the spaces from left end for the given string column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Ltrim(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Trim the specified character string from left end for the given string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="trimString">String to trim</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RegexpExtract(Microsoft.Spark.Sql.Column,System.String,System.Int32)">
            <summary>
            Extract a specific group matched by a Java regex, from the specified string column.
            </summary>
            <remarkes>
            If the regex did not match, or the specified group did not match,
            an empty string is returned.
            </remarkes>
            <param name="column">Column to apply</param>
            <param name="exp">Regular expression to match</param>
            <param name="groupIdx">Group index to extract</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RegexpReplace(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Replace all substrings of the specified string value that match the pattern with
            the given replacement string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="pattern">Regular expression to match</param>
            <param name="replacement">String to replace with</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.RegexpReplace(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Replace all substrings of the specified string value that match the pattern with
            the given replacement string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="pattern">Regular expression to match</param>
            <param name="replacement">String to replace with</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Unbase64(Microsoft.Spark.Sql.Column)">
            <summary>
            Decodes a BASE64 encoded string column and returns it as a binary column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rpad(Microsoft.Spark.Sql.Column,System.Int32,System.String)">
            <summary>
            Right-pad the string column with pad to the given length `len`. If the string column is
            longer than `len`, the return value is shortened to `len` characters.
            </summary>
            <param name="column">Column to apply</param>
            <param name="len">Length of padded string</param>
            <param name="pad">String used for padding</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Repeat(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Repeats a string column `n` times, and returns it as a new string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="n">Repeatation number</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rtrim(Microsoft.Spark.Sql.Column)">
            <summary>
            Trim the spaces from right end for the specified string value.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Rtrim(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Trim the specified character string from right end for the given string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="trimString">String to trim</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Soundex(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the soundex code for the specified expression.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Split(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Splits string with a regular expression pattern.
            </summary>
            <param name="column">Column to apply</param>
            <param name="pattern">Regular expression pattern</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Substring(Microsoft.Spark.Sql.Column,System.Int32,System.Int32)">
            <summary>
            Returns the substring (or slice of byte array) starting from the given
            position for the given length.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="pos">Starting position</param>
            <param name="len">Length of the substring</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SubstringIndex(Microsoft.Spark.Sql.Column,System.String,System.Int32)">
            <summary>
            Returns the substring from the given string before `count` occurrences of
            the given delimiter.
            </summary>
            <param name="column">Column to apply</param>
            <param name="delimiter">Delimiter to find</param>
            <param name="count">Number of occurrences of delimiter</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Translate(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Translate any characters that match with the given `matchingString` in the column
            by the given `replaceString`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="matchingString">String to match</param>
            <param name="replaceString">String to replace with</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Trim(Microsoft.Spark.Sql.Column)">
            <summary>
            Trim the spaces from both ends for the specified string column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Trim(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Trim the specified character from both ends for the specified string column.
            </summary>
            <param name="column">Column to apply</param>
            <param name="trimString">String to trim</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Upper(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts a string column to upper case.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.AddMonths(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the date that is `numMonths` after `startDate`.
            </summary>
            <param name="startDate">Start date</param>
            <param name="numMonths">Number of months to add to start date</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CurrentDate">
            <summary>
            Returns the current date as a date column.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CurrentTimestamp">
            <summary>
            Returns the current timestamp as a timestamp column.
            </summary>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateFormat(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts a date/timestamp/string to a value of string in the format specified
            by the date format given by the second argument.
            </summary>
            <param name="dateExpr">Date expression</param>
            <param name="format">Format string to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateAdd(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the date that is `days` days after `start`.
            </summary>
            <param name="start">Start date</param>
            <param name="days">Number of days to add to start data</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateSub(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Returns the date that is `days` days before `start`.
            </summary>
            <param name="start">Start date</param>
            <param name="days">Number of days to subtract from start data</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateDiff(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the number of days from `start` to `end`.
            </summary>
            <param name="start">Start date</param>
            <param name="end">End date</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Year(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the year as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Quarter(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the quarter as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Month(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the month as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DayOfWeek(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the day of the week as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DayOfMonth(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the day of the month as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DayOfYear(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the day of the year as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Hour(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the hours as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.LastDay(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the last day of the month which the given date belongs to.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Minute(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the minutes as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MonthsBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns number of months between dates `end` and `stasrt`.
            </summary>
            <param name="end">Date column</param>
            <param name="start">Date column</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MonthsBetween(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Returns number of months between dates `end` and `start`. If `roundOff` is set to true,
            the result is rounded off to 8 digits; it is not rounded otherwise.
            </summary>
            <param name="end">Date column</param>
            <param name="start">Date column</param>
            <param name="roundOff">To round or not</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.NextDay(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Given a date column, returns the first date which is later than the value of
            the date column that is on the specified day of the week.
            </summary>
            <param name="date">Date column</param>
            <param name="dayOfWeek">
            One of the following (case-insensitive):
              "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun".
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Second(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the seconds as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.WeekOfYear(Microsoft.Spark.Sql.Column)">
            <summary>
            Extracts the week number as an integer from a given date/timestamp/string.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUnixTime(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts the number of seconds from UNIX epoch (1970-01-01 00:00:00 UTC) to a string
            representing the timestamp of that moment in the current system time zone with
            a default format "yyyy-MM-dd HH:mm:ss".
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUnixTime(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts the number of seconds from UNIX epoch (1970-01-01 00:00:00 UTC) to a string
            representing the timestamp of that moment in the current system time zone with
            the given format.
            </summary>
            <param name="column">Column to apply</param>
            <param name="format">Format of the timestamp</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnixTimestamp">
            <summary>
            Returns the current Unix timestamp (in seconds).
            </summary>
            <remarks>
            All calls of `UnixTimestamp` within the same query return the same value
            (i.e. the current timestamp is calculated at the start of query evaluation).
            </remarks>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnixTimestamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds),
            using the default timezone and the default locale.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.UnixTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts time string with given format to Unix timestamp (in seconds).
            </summary>
            <remarks>
            Supported date format can be found:
            http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
            </remarks>
            <param name="column">Column to apply</param>
            <param name="format">Date format</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToTimestamp(Microsoft.Spark.Sql.Column)">
            <summary>
            Convert time string to a Unix timestamp (in seconds) by casting rules to
            `TimestampType`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Convert time string to a Unix timestamp (in seconds) with specified format.
            </summary>
            <remarks>
            Supported date format can be found:
            http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
            </remarks>
            <param name="column">Column to apply</param>
            <param name="format">Date format</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToDate(Microsoft.Spark.Sql.Column)">
            <summary>
            Converts the column into `DateType` by casting rules to `DateType`.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToDate(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Converts the column into a `DateType` with a specified format.
            </summary>
            <remarks>
            Supported date format can be found:
            http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html
            </remarks>
            <param name="column">Column to apply</param>
            <param name="format">Date format</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Trunc(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Returns date truncated to the unit specified by the format.
            </summary>
            <param name="column">Column to apply</param>
            <param name="format">
            'year', 'yyyy', 'yy' for truncate by year, or
            'month', 'mon', 'mm' for truncate by month
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.DateTrunc(System.String,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns timestamp truncated to the unit specified by the format.
            </summary>
            <param name="format">
            'year', 'yyyy', 'yy' for truncate by year, or
            'month', 'mon', 'mm' for truncate by month, or
            'day', 'dd' for truncate by day, or
            'second', 'minute', 'hour', 'week', 'month', 'quarter'
            </param>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUtcTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC,
            and renders that time as a timestamp in the given time zone. For example, 'GMT+1'
            would yield '2017-07-14 03:40:00.0'.
            </summary>
            <remarks>
            This API is deprecated in Spark 3.0.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromUtcTimestamp(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC,
            and renders that time as a timestamp in the given time zone. For example, 'GMT+1'
            would yield '2017-07-14 03:40:00.0'.
            </summary>
            <remarks>
            This API is deprecated in Spark 3.0.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToUtcTimestamp(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the
            given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1'
            would yield '2017-07-14 01:40:00.0'.
            </summary>
            <remarks>
            This API is deprecated in Spark 3.0.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToUtcTimestamp(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the
            given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1'
            would yield '2017-07-14 01:40:00.0'.
            </summary>
            <remarks>
            This API is deprecated in Spark 3.0.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="tz">Timezone expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Window(Microsoft.Spark.Sql.Column,System.String,System.String,System.String)">
            <summary>
            Bucketize rows into one or more time windows given a timestamp column.
            </summary>
            <remarks>
            Refer to org.apache.spark.unsafe.types.CalendarInterval for the duration strings.
            </remarks>
            <param name="column">The column to use as the timestamp for windowing by time</param>
            <param name="windowDuration">A string specifying the width of the window</param>
            <param name="slideDuration">
            A string specifying the sliding interval of the window
            </param>
            <param name="startTime">
            The offset with respect to 1970-01-01 00:00:00 UTC with which to start window intervals
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Window(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Bucketize rows into one or more time windows given a timestamp column.
            </summary>
            <remarks>
            Refer to org.apache.spark.unsafe.types.CalendarInterval for the duration strings.
            </remarks>
            <param name="column">The column to use as the timestamp for windowing by time</param>
            <param name="windowDuration">A string specifying the width of the window</param>
            <param name="slideDuration">
            A string specifying the sliding interval of the window
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Window(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Generates tumbling time windows given a timestamp specifying column.
            </summary>
            <param name="column">The column to use as the timestamp for windowing by time</param>
            <param name="windowDuration">
            A string specifying the width of the window.
            Refer to org.apache.spark.unsafe.types.CalendarInterval for the duration strings.
            </param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayContains(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Returns null if the array is null, true if the array contains `value`,
            and false otherwise.
            </summary>
            <param name="column">Column to apply</param>
            <param name="value">Value to check for existence</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArraysOverlap(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns true if `a1` and `a2` have at least one non-null element in common.
            If not and both arrays are non-empty and any of them contains a null,
            it returns null. It returns false otherwise.
            </summary>
            <param name="a1">Left side array</param>
            <param name="a2">Right side array</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Slice(Microsoft.Spark.Sql.Column,System.Int32,System.Int32)">
            <summary>
            Returns an array containing all the elements in `column` from index `start`
            (or starting from the end if `start` is negative) with the specified `length`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="start">Start position in the array</param>
            <param name="length">Length for slicing</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayJoin(Microsoft.Spark.Sql.Column,System.String,System.String)">
            <summary>
            Concatenates the elements of `column` using the `delimiter`.
            Null values are replaced with `nullReplacement`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="delimiter">Delimiter for join</param>
            <param name="nullReplacement">String to replace null value</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayJoin(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Concatenates the elements of `column` using the `delimiter`.
            </summary>
            <param name="column">Column to apply</param>
            <param name="delimiter">Delimiter for join</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Concat(Microsoft.Spark.Sql.Column[])">
            <summary>
            Concatenates multiple input columns together into a single column.
            </summary>
            <remarks>
            If all inputs are binary, concat returns an output as binary.
            Otherwise, it returns as string.
            </remarks>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayPosition(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Locates the position of the first occurrence of the value in the given array as long.
            Returns null if either of the arguments are null.
            </summary>
            <remarks>
            The position is not zero based, but 1 based index.
            Returns 0 if value could not be found in array.
            </remarks>
            <param name="column">Column to apply</param>
            <param name="value">Value to locate</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ElementAt(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Returns element of array at given index in `value` if column is array.
            Returns value for the given key in `value` if column is map.
            </summary>
            <param name="column">Column to apply</param>
            <param name="value">Value to locate</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArraySort(Microsoft.Spark.Sql.Column)">
            <summary>
            Sorts the input array in ascending order. The elements of the input array must
            be sortable. Null elements will be placed at the end of the returned array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayRemove(Microsoft.Spark.Sql.Column,System.Object)">
            <summary>
            Remove all elements that equal to element from the given array.
            </summary>
            <param name="column">Column to apply</param>
            <param name="element">Element to remove</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayDistinct(Microsoft.Spark.Sql.Column)">
            <summary>
            Removes duplicate values from the array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayIntersect(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an array of the elements in the intersection of the given two arrays,
            without duplicates.
            </summary>s
            <param name="col1">Left side column to apply</param>
            <param name="col2">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayUnion(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an array of the elements in the union of the given two arrays,
            without duplicates.
            </summary>
            <param name="col1">Left side column to apply</param>
            <param name="col2">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayExcept(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an array of the elements in the `col1` but not in the `col2`,
            without duplicates. The order of elements in the result is nondeterministic.
            </summary>
            <param name="col1">Left side column to apply</param>
            <param name="col2">Right side column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Explode(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element in the given array or map column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ExplodeOuter(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element in the given array or map column.
            Unlike Explode(), if the array/map is null or empty then null is produced.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.PosExplode(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element with position in the given array or map column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.PosExplodeOuter(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a new row for each element with position in the given array or map column.
            Unlike Posexplode(), if the array/map is null or empty then the row(null, null)
            is produced.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.GetJsonObject(Microsoft.Spark.Sql.Column,System.String)">
            <summary>
            Extracts JSON object from a JSON string based on path specified, and returns JSON
            string of the extracted JSON object.
            </summary>
            <param name="column">Column to apply</param>
            <param name="path">JSON file path</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.JsonTuple(Microsoft.Spark.Sql.Column,System.String[])">
            <summary>
            Creates a new row for a JSON column according to the given field names.
            </summary>
            <param name="column">Column to apply</param>
            <param name="fields">Field names</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromJson(Microsoft.Spark.Sql.Column,System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Parses a column containing a JSON string into a `StructType` or `ArrayType`
            of `StructType`s with the specified schema.
            </summary>
            <param name="column">Column to apply</param>
            <param name="schema">JSON format string or DDL-formatted string for a schema</param>
            <param name="options">Options for JSON parsing</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.FromJson(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Parses a column containing a JSON string into a `StructType` or `ArrayType`
            of `StructType`s with the specified schema.
            </summary>
            <param name="column">String column containing JSON data</param>
            <param name="schema">The schema to use when parsing the JSON string</param>
            <param name="options">Options for JSON parsing</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SchemaOfJson(System.String)">
            <summary>
            Parses a JSON string and infers its schema in DDL format.
            </summary>
            <param name="json">JSON string</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SchemaOfJson(Microsoft.Spark.Sql.Column)">
            <summary>
            Parses a JSON string and infers its schema in DDL format.
            </summary>
            <param name="json">String literal containing a JSON string.</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ToJson(Microsoft.Spark.Sql.Column,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Converts a column containing a `StructType`, `ArrayType` of `StructType`s,
            a `MapType` or `ArrayType` of `MapType`s into a JSON string.
            </summary>
            <param name="column">Column to apply</param>
            <param name="options">Options for JSON conversion</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Size(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns length of array or map.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.SortArray(Microsoft.Spark.Sql.Column,System.Boolean)">
            <summary>
            Sorts the input array for the given column in ascending (default) or
            descending order, the natural ordering of the array elements.
            </summary>
            <param name="column">Column to apply</param>
            <param name="asc">True for ascending order and false for descending order</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayMin(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the minimum value in the array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayMax(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns the maximum value in the array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Shuffle(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a random permutation of the given array.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Reverse(Microsoft.Spark.Sql.Column)">
            <summary>
            Reverses the string column and returns it as a new string column.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Flatten(Microsoft.Spark.Sql.Column)">
            <summary>
            Creates a single array from an array of arrays. If a structure of nested arrays
            is deeper than two levels, only one level of nesting is removed.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sequence(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Generate a sequence of integers from `start` to `stop`, incrementing by `step`.
            </summary>
            <param name="start">Start expression</param>
            <param name="stop">Stop expression</param>
            <param name="step">Step to increment</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Sequence(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Generate a sequence of integers from start to stop, incrementing by 1 if start is
            less than or equal to stop, otherwise -1.
            </summary>
            <param name="start">Start expression</param>
            <param name="stop">Stop expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayRepeat(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column)">
            <summary>
            Creates an array containing the `left` argument repeated the number of times given by
            the `right` argument.
            </summary>
            <param name="left">Left column expression</param>
            <param name="right">Right column expression</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArrayRepeat(Microsoft.Spark.Sql.Column,System.Int32)">
            <summary>
            Creates an array containing the `left` argument repeated the `count` number of times.
            </summary>
            <param name="left">Left column expression</param>
            <param name="count">Number of times to repeat</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapKeys(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an unordered array containing the keys of the map.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapValues(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns an unordered array containing the values of the map.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapFromEntries(Microsoft.Spark.Sql.Column)">
            <summary>
            Returns a map created from the given array of entries.
            </summary>
            <param name="column">Column to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.ArraysZip(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns a merged array of structs in which the N-th struct contains all
            N-th values of input arrays.
            </summary>
            <param name="columns">Columns to zip</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.MapConcat(Microsoft.Spark.Sql.Column[])">
            <summary>
            Returns the union of all the given maps.
            </summary>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``1(System.Func{``0})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``2(System.Func{``0,``1})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``3(System.Func{``0,``1,``2})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``4(System.Func{``0,``1,``2,``3})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``5(System.Func{``0,``1,``2,``3,``4})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``6(System.Func{``0,``1,``2,``3,``4,``5})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``7(System.Func{``0,``1,``2,``3,``4,``5,``6})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``8(System.Func{``0,``1,``2,``3,``4,``5,``6,``7})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``9(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``10(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>A delegate that when invoked will return a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``11(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,``10})">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf(System.Func{Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``1(System.Func{``0,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``2(System.Func{``0,``1,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``3(System.Func{``0,``1,``2,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``4(System.Func{``0,``1,``2,``3,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``5(System.Func{``0,``1,``2,``3,``4,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``6(System.Func{``0,``1,``2,``3,``4,``5,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``7(System.Func{``0,``1,``2,``3,``4,``5,``6,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``8(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``9(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>A delegate that when invoked will return a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.Udf``10(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>Creates a UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <param name="udf">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``2(System.Func{``0,``1})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``3(System.Func{``0,``1,``2})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``4(System.Func{``0,``1,``2,``3})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``5(System.Func{``0,``1,``2,``3,``4})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``6(System.Func{``0,``1,``2,``3,``4,``5})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``7(System.Func{``0,``1,``2,``3,``4,``5,``6})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``8(System.Func{``0,``1,``2,``3,``4,``5,``6,``7})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``9(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``10(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.VectorUdf``11(System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,``10})">
            <summary>Creates a Vector UDF from the specified delegate.</summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="udf">The Vector UDF function implementation.</param>
            <returns>
            A delegate that returns a <see cref="M:Microsoft.Spark.Sql.Functions.Column(System.String)"/> for the result of the Vector UDF.
            </returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Functions.CallUDF(System.String,Microsoft.Spark.Sql.Column[])">
            <summary>
            Call an user-defined function registered via SparkSession.Udf().Register().
            </summary>
            <param name="udfName">Name of the registered UDF</param>
            <param name="columns">Columns to apply</param>
            <returns>Column object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.GenericRow">
            <summary>
            Represents a row object in RDD, equivalent to GenericRow in Spark.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.#ctor(System.Object[])">
            <summary>
            Constructor for the GenericRow class.
            </summary>
            <param name="values">Column values for a row</param>        
        </member>
        <member name="P:Microsoft.Spark.Sql.GenericRow.Values">
            <summary>
            Values representing this row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.Size">
            <summary>
            Returns the number of columns in this row.
            </summary>
            <returns>Number of columns in this row</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.GenericRow.Item(System.Int32)">
            <summary>
            Returns the column value at the given index.
            </summary>
            <param name="index">Index to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.Get(System.Int32)">
            <summary>
            Returns the column value at the given index.
            </summary>
            <param name="index">Index to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.ToString">
            <summary>
            Returns the string version of this row.
            </summary>
            <returns>String version of this row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.GetAs``1(System.Int32)">
            <summary>
            Returns the column value at the given index, as a type T.
            TODO: If the original type is "long" and its value can be
            fit into the "int", Pickler will serialize the value as int.
            Since the value is boxed, <see cref="M:Microsoft.Spark.Sql.GenericRow.GetAs``1(System.Int32)"/> will throw an exception.
            </summary>
            <typeparam name="T">Type to convert to</typeparam>
            <param name="index">Index to look up</param>
            <returns>A column value as a type T</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.GenericRow.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`1">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`2">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`3">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`4">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`5">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`6">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`7">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`8">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`9">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`10">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingUdfWrapper`11">
            <summary>
            Wraps the given Func object, which represents a UDF.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
        </member>
        <member name="T:Microsoft.Spark.Sql.RelationalGroupedDataset">
            <summary>
            A set of methods for aggregations on a DataFrame.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Agg(Microsoft.Spark.Sql.Column,Microsoft.Spark.Sql.Column[])">
            <summary>
            Compute aggregates by specifying a series of aggregate columns.
            </summary>
            <param name="expr">Column to aggregate on</param>
            <param name="exprs">Additional columns to aggregate on</param>
            <returns>New DataFrame object with aggregation applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Count">
            <summary>
            Count the number of rows for each group.
            </summary>
            <returns>New DataFrame object with count applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Mean(System.String[])">
            <summary>
            Compute the mean value for each numeric columns for each group.
            </summary>
            <param name="colNames">Name of columns to compute mean on</param>
            <returns>New DataFrame object with mean applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Max(System.String[])">
            <summary>
            Compute the max value for each numeric columns for each group.
            </summary>
            <param name="colNames">Name of columns to compute max on</param>
            <returns>New DataFrame object with max applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Avg(System.String[])">
            <summary>
            Compute the average value for each numeric columns for each group.
            </summary>
            <param name="colNames">Name of columns to compute average on</param>
            <returns>New DataFrame object with average applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Min(System.String[])">
            <summary>
            Compute the min value for each numeric columns for each group.
            </summary>
            <param name="colNames">Name of columns to compute min on</param>
            <returns>New DataFrame object with min applied</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RelationalGroupedDataset.Sum(System.String[])">
            <summary>
            Compute the sum for each numeric columns for each group.
            </summary>
            <param name="colNames">Name of columns to compute sum on</param>
            <returns>New DataFrame object with sum applied</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Row">
            <summary>
            Represents a row object in RDD, equivalent to GenericRowWithSchema in Spark.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.#ctor(System.Object[],Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Constructor for the Row class.
            </summary>
            <param name="values">Column values for a row</param>
            <param name="schema">Schema associated with a row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.#ctor(Microsoft.Spark.Sql.GenericRow)">
            <summary>
            Constructor for the schema-less Row class used for chained UDFs.
            </summary>
            <param name="genericRow">GenericRow object</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.op_Implicit(Microsoft.Spark.Sql.GenericRow)~Microsoft.Spark.Sql.Row">
            <summary>
            Returns schema-less Row which can happen within chained UDFs (same behavior as PySpark).
            </summary>
            <remarks>
            The use of this conversion operator is discouraged except for the UDF that returns
            a Row object.
            </remarks>
            <returns>schema-less Row</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Row.Schema">
            <summary>
            Schema associated with this row.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Row.Values">
            <summary>
            Values representing this row.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Size">
            <summary>
            Returns the number of columns in this row.
            </summary>
            <returns>Number of columns in this row</returns>
        </member>
        <member name="P:Microsoft.Spark.Sql.Row.Item(System.Int32)">
            <summary>
            Returns the column value at the given index.
            </summary>
            <param name="index">Index to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Get(System.Int32)">
            <summary>
            Returns the column value at the given index.
            </summary>
            <param name="index">Index to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Get(System.String)">
            <summary>
            Returns the column value whose column name is given.
            </summary>
            <param name="columnName">Column name to look up</param>
            <returns>A column value</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.ToString">
            <summary>
            Returns the string version of this row.
            </summary>
            <returns>String version of this row</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.GetAs``1(System.Int32)">
            <summary>
            Returns the column value at the given index, as a type T.
            TODO: If the original type is "long" and its value can be
            fit into the "int", Pickler will serialize the value as int.
            Since the value is boxed, <see cref="M:Microsoft.Spark.Sql.Row.GetAs``1(System.Int32)"/> will throw an exception.
            </summary>
            <typeparam name="T">Type to convert to</typeparam>
            <param name="index">Index to look up</param>
            <returns>A column value as a type T</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.GetAs``1(System.String)">
            <summary>
            Returns the column value whose column name is given, as a type T.
            TODO: If the original type is "long" and its value can be
            fit into the "int", Pickler will serialize the value as int.
            Since the value is boxed, <see cref="M:Microsoft.Spark.Sql.Row.GetAs``1(System.String)"/> will throw an exception.
            </summary>
            <typeparam name="T">Type to convert to</typeparam>
            <param name="columnName">Column name to look up</param>
            <returns>A column value as a type T</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Row.Convert">
            <summary>
            Converts the values to .NET values. Currently, only the simple types such as
            int, string, etc. are supported (which are already converted correctly by
            the Pickler). Note that explicit type checks against the schema are not performed.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.RowCollector">
            <summary>
            RowCollector collects Row objects from a socket.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowCollector.Collect(Microsoft.Spark.Network.ISocketWrapper)">
            <summary>
            Collects pickled row objects from the given socket.
            </summary>
            <param name="socket">Socket the get the stream from</param>
            <returns>Collection of row objects</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.RowConstructor">
            <summary>
            RowConstructor is a custom unpickler for GenericRowWithSchema in Spark.
            Refer to spark/sql/core/src/main/scala/org/apache/spark/sql/execution/python/
            EvaluatePython.scala how GenericRowWithSchema is being pickled.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.RowConstructor.s_schemaCache">
            <summary>
            Cache the schemas of the rows being received. Multiple schemas may be
            sent per batch if there are nested rows contained in the row. Note that
            this is thread local variable because one RowConstructor object is
            registered to the Unpickler and there could be multiple threads unpickling
            the data using the same object registered.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.RowConstructor._parent">
            <summary>
            The RowConstructor that created this instance.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.RowConstructor._args">
            <summary>
            Stores the args passed from construct().
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.construct(System.Object[])">
            <summary>
            Used by Unpickler to pass unpickled data for handling.
            </summary>
            <param name="args">Unpickled data</param>
            <returns>New RowConstructor object capturing args data</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.GetRow">
            <summary>
            Construct a Row object from unpickled data. This is only to be called
            on a RowConstructor that contains the row data.
            </summary>
            <returns>A row object with unpickled data</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.Reset">
            <summary>
            Clears the schema cache. Spark sends rows in batches and for each
            row there is an accompanying set of schemas and row entries. If the
            schema was not cached, then it would need to be parsed and converted
            to a StructType for every row in the batch. A new batch may contain
            rows from a different table, so calling <c>Reset</c> after each
            batch would aid in preventing the cache from growing too large.
            Caching the schemas for each batch, ensures that each schema is
            only parsed and converted to a StructType once per batch.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RowConstructor.GetSchema">
            <summary>
            Get or cache the schema string contained in args. Calling this
            is only valid if the child args contain the row values.
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.RuntimeConfig">
            <summary>
            Runtime configuration interface for Spark.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.Set(System.String,System.String)">
            <summary>
            Sets the given Spark runtime configuration property.
            </summary>
            <param name="key">Config name</param>
            <param name="value">Config value</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.Set(System.String,System.Boolean)">
            <summary>
            Sets the given Spark runtime configuration property.
            </summary>
            <param name="key">Config name</param>
            <param name="value">Config value</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.Set(System.String,System.Int64)">
            <summary>
            Sets the given Spark runtime configuration property.
            </summary>
            <param name="key">Config name</param>
            <param name="value">Config value</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.Get(System.String)">
            <summary>
            Returns the value of Spark runtime configuration property for the given key.
            </summary>
            <param name="key">Key to use</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.Get(System.String,System.String)">
            <summary>
            Returns the value of Spark runtime configuration property for the given key.
            </summary>
            <param name="key">Key to use</param>
            <param name="defaultValue">Default value to use</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.Unset(System.String)">
            <summary>
            Resets the configuration property for the given key.
            </summary>
            <param name="key">Key to unset</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.RuntimeConfig.IsModifiable(System.String)">
            <summary>
            Indicates whether the configuration property with the given key
            is modifiable in the current session.
            </summary>
            <param name="key">Key to check</param>
            <returns>
            true if the configuration property is modifiable. For static SQL, Spark
            Core, invalid(not existing) and other non-modifiable configuration properties,
            the returned value is false.
            </returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.SaveMode">
            <summary>
            SaveMode is used to specify the expected behavior of saving a DataFrame to a data source.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.Append">
            <summary>
            Append mode means that when saving a DataFrame to a data source, if data/table already
            exists, contents of the DataFrame are expected to be appended to existing data.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.Overwrite">
            <summary>
            Overwrite mode means that when saving a DataFrame to a data source,
            if data/table already exists, existing data is expected to be overwritten by the
            contents of the DataFrame.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.ErrorIfExists">
            <summary>
            ErrorIfExists mode means that when saving a DataFrame to a data source, if data already
            exists, an exception is expected to be thrown.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.SaveMode.Ignore">
            <summary>
            Ignore mode means that when saving a DataFrame to a data source, if data already exists,
            the save operation is expected to not save the contents of the DataFrame and to not
            change the existing data.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.SparkSession">
            <summary>
            The entry point to programming Spark with the Dataset and DataFrame API.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor for SparkSession.
            </summary>
            <param name="jvmObject">Reference to the JVM SparkSession object</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.SparkSession.SparkContext">
            <summary>
            Returns SparkContext object associated with this SparkSession.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.SparkSession.Catalog">
            <summary>
            Interface through which the user may create, drop, alter or query underlying databases,
            tables, functions etc.
            </summary>
            <returns>Catalog object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Builder">
            <summary>
            Creates a Builder object for SparkSession.
            </summary>
            <returns>Builder object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.SetDefaultSession(Microsoft.Spark.Sql.SparkSession)">
            Note that *ActiveSession() APIs are not exposed because these APIs work with a
            thread-local variable, which stores the session variable. Since the Netty server
            that handles the requests is multi-threaded, any thread can invoke these APIs,
            resulting in unexpected behaviors if different threads are used.
            <summary>
            Sets the default SparkSession that is returned by the builder.
            </summary>
            <param name="session">SparkSession object</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.ClearDefaultSession">
            <summary>
            Clears the default SparkSession that is returned by the builder.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.GetDefaultSession">
            <summary>
            Returns the default SparkSession that is returned by the builder.
            </summary>
            <returns>SparkSession object or null if called on executors</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Active">
            <summary>
            Returns the currently active SparkSession, otherwise the default one.
            If there is no default SparkSession, throws an exception.
            </summary>
            <returns>SparkSession object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Dispose">
            <summary>
            Synonym for Stop().
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Conf">
            <summary>
            Runtime configuration interface for Spark.
            <remarks>
            This is the interface through which the user can get and set all Spark and Hadoop
            configurations that are relevant to Spark SQL. When getting the value of a config,
            this defaults to the value set in the underlying SparkContext, if any.
            </remarks>
            </summary>
            <returns>The RuntimeConfig object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.NewSession">
            <summary>
            Start a new session with isolated SQL configurations, temporary tables, registered
            functions are isolated, but sharing the underlying SparkContext and cached data.
            </summary>
            <remarks>
            Other than the SparkContext, all shared state is initialized lazily.
            This method will force the initialization of the shared state to ensure that parent
            and child sessions are set up with the same shared state. If the underlying catalog
            implementation is Hive, this will initialize the metastore, which may take some time.
            </remarks>
            <returns>New SparkSession object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Table(System.String)">
            <summary>
            Returns the specified table/view as a DataFrame.
            </summary>
            <param name="tableName">Name of a table or view</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{Microsoft.Spark.Sql.GenericRow},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Creates a <see cref="T:Microsoft.Spark.Sql.DataFrame"/> from an <see cref="T:System.Collections.IEnumerable"/> containing
            <see cref="T:Microsoft.Spark.Sql.GenericRow"/>s using the given schema.
            It is important to make sure that the structure of every <see cref="T:Microsoft.Spark.Sql.GenericRow"/> of
            the provided <see cref="T:System.Collections.IEnumerable"/> matches
            the provided schema. Otherwise, there will be runtime exception.
            </summary>
            <param name="data">List of Row objects</param>
            <param name="schema">Schema as StructType</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.Int32})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:System.Int32"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:System.Int32"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.Nullable{System.Int32}})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Nullable`1"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Nullable`1"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.String"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:System.String"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.Double})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Double"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:System.Double"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.Nullable{System.Double}})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Nullable`1"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Nullable`1"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.Boolean})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:System.Boolean"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:System.Boolean"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{System.Nullable{System.Boolean}})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Nullable`1"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:System.Nullable`1"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{Microsoft.Spark.Sql.Types.Date})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:Microsoft.Spark.Sql.Types.Date"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:Microsoft.Spark.Sql.Types.Date"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.CreateDataFrame(System.Collections.Generic.IEnumerable{Microsoft.Spark.Sql.Types.Timestamp})">
            <summary>
            Creates a Dataframe given data as <see cref="T:System.Collections.IEnumerable"/> of type
            <see cref="T:Microsoft.Spark.Sql.Types.Timestamp"/>
            </summary>
            <param name="data"><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:Microsoft.Spark.Sql.Types.Timestamp"/></param>
            <returns>Dataframe object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Sql(System.String)">
            <summary>
            Executes a SQL query using Spark, returning the result as a DataFrame.
            </summary>
            <param name="sqlText">SQL query text</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Read">
            <summary>
            Returns a DataFrameReader that can be used to read non-streaming data in
            as a DataFrame.
            </summary>
            <returns>DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Range(System.Int64)">
            <summary>
            Creates a DataFrame with a single column named id, containing elements in
            a range from 0 to end (exclusive) with step value 1.
            </summary>
            <param name="end">The end value (exclusive)</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Range(System.Int64,System.Int64)">
            <summary>
            Creates a DataFrame with a single column named id, containing elements in 
            a range from start to end (exclusive) with step value 1.
            </summary>
            <param name="start">The start value</param>
            <param name="end">The end value (exclusive)</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Range(System.Int64,System.Int64,System.Int64)">
            <summary>
            Creates a DataFrame with a single column named id, containing elements in
            a range from start to end (exclusive) with a step value.
            </summary>
            <param name="start">The start value</param>
            <param name="end">The end value (exclusive)</param>
            <param name="step">Step value to use when creating the range</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Range(System.Int64,System.Int64,System.Int64,System.Int32)">
            <summary>
            Creates a DataFrame with a single column named id, containing elements in
            a range from start to end (exclusive) with a step value, with partition
            number specified.
            </summary>
            <param name="start">The start value</param>
            <param name="end">The end value (exclusive)</param>
            <param name="step">Step value to use when creating the range</param>
            <param name="numPartitions">The number of partitions of the DataFrame</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.ReadStream">
            <summary>
            Returns a DataStreamReader that can be used to read streaming data in as a DataFrame.
            </summary>
            <returns>DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Udf">
            <summary>
            Returns UDFRegistraion object with which user-defined functions (UDF) can 
            be registered.
            </summary>
            <returns>UDFRegistration object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.Stop">
            <summary>
            Stops the underlying SparkContext.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.SchemaWithSingleColumn(Microsoft.Spark.Sql.Types.DataType,System.Boolean)">
            <summary>
            Returns a single column schema of the given datatype.
            </summary>
            <param name="dataType">Datatype of the column</param>
            <param name="isNullable">Indicates if values of the column can be null</param>
            <returns>Schema as StructType</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.SparkSession.ToGenericRows``1(System.Collections.Generic.IEnumerable{``0})">
            <summary>
            This method is transforming each element of IEnumerable of type T input into a single 
            columned GenericRow.
            </summary>
            <typeparam name="T">Datatype of values in rows</typeparam>
            <param name="rows">List of values of type T</param>
            <returns><see cref="T:System.Collections.IEnumerable"/> of type <see cref="T:Microsoft.Spark.Sql.GenericRow"/></returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.DataStreamReader">
            <summary>
            DataStreamReader provides functionality to load a streaming <see cref="T:Microsoft.Spark.Sql.DataFrame"/>
            from external storage systems (e.g. file systems, key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Format(System.String)">
            <summary>
            Specifies the input data source format.
            </summary>
            <param name="source">Name of the data source</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Schema(Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Specifies the schema by using <see cref="T:Microsoft.Spark.Sql.Types.StructType"/>.
            </summary>
            <remarks>
            Some data sources (e.g. JSON) can infer the input schema automatically
            from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </remarks>
            <param name="schema">The input schema</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Schema(System.String)">
            <summary>
            Specifies the schema by using the given DDL-formatted string.
            </summary>
            <remarks>
            Some data sources (e.g. JSON) can infer the input schema automatically
            from data. By specifying the schema here, the underlying data source can
            skip the schema inference step, and thus speed up data loading.
            </remarks>
            <param name="schemaString">DDL-formatted string</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.String)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.Boolean)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.Int64)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Option(System.String,System.Double)">
            <summary>
            Adds an input option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds input options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataStreamReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Load">
            <summary>
            Loads input data stream in as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>, for data streams
            that don't require a path (e.g.external key-value stores).
            </summary>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Load(System.String)">
            <summary>
            Loads input in as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>, for data streams that read
            from some path. 
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Json(System.String)">
            <summary>
            Loads a JSON file stream and returns the results as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Csv(System.String)">
            <summary>
            Loads a CSV file stream and returns the result as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Orc(System.String)">
            <summary>
            Loads a ORC file stream and returns the result as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Parquet(System.String)">
            <summary>
            Loads a Parquet file stream and returns the result as a <see cref="T:Microsoft.Spark.Sql.DataFrame"/>.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.Text(System.String)">
            <summary>
            Loads text files and returns a <see cref="T:Microsoft.Spark.Sql.DataFrame"/> whose schema starts
            with a string column named "value", and followed by partitioned columns
            if there are any.
            </summary>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataFrameReader object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamReader.LoadSource(System.String,System.String)">
            <summary>
            Helper function to load the source for a given path.
            </summary>
            <param name="source">Name of the source</param>
            <param name="path">File path for streaming</param>
            <returns>DataFrame object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.DataStreamWriter">
            <summary>
            DataStreamWriter provides functionality to write a streaming <see cref="T:Microsoft.Spark.Sql.DataFrame"/>
            to external storage systems (e.g. file systems, key-value stores, etc).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.OutputMode(System.String)">
            <summary>
            Specifies how data of a streaming DataFrame is written to a streaming sink.
            </summary>
            <remarks>
            The following mode is supported:
            "append": Only the new rows in the streaming DataFrame/Dataset will be written to
                      the sink.
            "complete": All the rows in the streaming DataFrame/Dataset will be written to the sink
                        every time there are some updates.
            "update": Only the rows that were updated in the streaming DataFrame will
                      be written to the sink every time there are some updates. If the query
                      doesn't contain aggregations, it will be equivalent to `append` mode.
            </remarks>
            <param name="outputMode">Output mode name</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.OutputMode(Microsoft.Spark.Sql.Streaming.OutputMode)">
            <summary>
            Specifies how data of a streaming DataFrame is written to a streaming sink.
            </summary>
            <param name="outputMode">Output mode enum</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Format(System.String)">
            <summary>
            Specifies the underlying output data source.
            </summary>
            <param name="source">Name of the data source</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.PartitionBy(System.String[])">
            <summary>
            Partitions the output by the given columns on the file system. If specified,
            the output is laid out on the file system similar to Hive's partitioning scheme.
            </summary>
            <param name="colNames">Column names to partition by</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.String)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.Boolean)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.Int64)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Option(System.String,System.Double)">
            <summary>
            Adds an output option for the underlying data source.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Options(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Adds output options for the underlying data source.
            </summary>
            <param name="options">Key/value options</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Trigger(Microsoft.Spark.Sql.Streaming.Trigger)">
            <summary>
            Sets the trigger for the stream query.
            </summary>
            <param name="trigger">Trigger object</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.QueryName(System.String)">
            <summary>
            Specifies the name of the <see cref="T:Microsoft.Spark.Sql.Streaming.StreamingQuery"/> 
            that can be started with `start()`.
            This name must be unique among all the currently active queries 
            in the associated SQLContext.
            </summary>
            <param name="queryName">Query name</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Start(System.String)">
            <summary>
            Starts the execution of the streaming query.
            </summary>
            <param name="path">Optional output path</param>
            <returns>StreamingQuery object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.Foreach(Microsoft.Spark.Sql.IForeachWriter)">
            <summary>
            Sets the output of the streaming query to be processed using the provided
            writer object. See <see cref="T:Microsoft.Spark.Sql.IForeachWriter"/> for more details on the
            lifecycle and semantics.
            </summary>
            <param name="writer"></param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.ForeachBatch(System.Action{Microsoft.Spark.Sql.DataFrame,System.Int64})">
            <summary>
            Sets the output of the streaming query to be processed using the provided
            function. This is supported only in the micro-batch execution modes (that
            is, when the trigger is not continuous). In every micro-batch, the provided
            function will be called in every micro-batch with (i) the output rows as a
            <see cref="T:Microsoft.Spark.Sql.DataFrame"/> and (ii) the batch identifier. The batchId can be used
            to deduplicate and transactionally write the output (that is, the provided
            Dataset) to external systems. The output <see cref="T:Microsoft.Spark.Sql.DataFrame"/> is guaranteed
            to exactly same for the same batchId (assuming all operations are deterministic
            in the query).
            </summary>
            <param name="func">The function to apply to the DataFrame</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.DataStreamWriter.OptionInternal(System.String,System.Object)">
            <summary>
            Helper function to add given key/value pair as a new option.
            </summary>
            <param name="key">Name of the option</param>
            <param name="value">Value of the option</param>
            <returns>This DataStreamWriter object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.OutputMode">
            <summary>
            Output modes for specifying how data of a streaming DataFrame is written to a
            streaming sink.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.Streaming.OutputMode.Append">
            <summary>
            OutputMode in which only the new rows in the streaming DataFrame/Dataset will be
            written to the sink. This output mode can be only be used in queries that do not
            contain any aggregation.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.Streaming.OutputMode.Complete">
            <summary>
            OutputMode in which all the rows in the streaming DataFrame/Dataset will be written
            to the sink every time these is some updates. This output mode can only be used in
            queries that contain aggregations.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.Streaming.OutputMode.Update">
            <summary>
            OutputMode in which only the rows in the streaming DataFrame/Dataset that were updated
            will be written to the sink every time these is some updates. If the query doesn't
            contain aggregations, it will be equivalent to Append mode.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.StreamingQuery">
            <summary>
            A handle to a query that is executing continuously in the background as new data arrives.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Streaming.StreamingQuery.Name">
            <summary>
            Returns the user-specified name of the query, or null if not specified.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.IsActive">
            <summary>
            Returns true if this query is actively running.
            </summary>
            <returns>True if this query is actively running</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.AwaitTermination">
            <summary>
            Waits for the termination of this query, either by Stop() or by an exception.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.AwaitTermination(System.Int64)">
            <summary>
            Returns true if this query is terminated within the timeout in milliseconds.
            </summary>
            <remarks>
            If the query has terminated, then all subsequent calls to this method will either
            return true immediately (if the query was terminated by Stop()), or throw an
            exception immediately (if the query has terminated with an exception).
            </remarks>
            <param name="timeoutMs">Timeout value in milliseconds</param>
            <returns>true if this query is terminated within timeout</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.ProcessAllAvailable">
            <summary>
            Blocks until all available data in the source has been processed and committed to the
            sink. This method is intended for testing. Note that in the case of continually
            arriving data, this method may block forever. Additionally, this method is only
            guaranteed to block until data that has been synchronously appended data to a
            `org.apache.spark.sql.execution.streaming.Source` prior to invocation.
            (i.e. `getOffset` must immediately reflect the addition).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.Stop">
            <summary>
            Stops the execution of this query if it is running. This method blocks until the
            threads performing execution stop.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.StreamingQuery.Explain(System.Boolean)">
            <summary>
            Prints the physical plan to the console for debugging purposes.
            </summary>
            <param name="extended">Whether to do extended explain or not</param>
        </member>
        <member name="T:Microsoft.Spark.Sql.Streaming.Trigger">
            <summary>
            Policy used to indicate how often results should be produced by a 
            <see cref="T:Microsoft.Spark.Sql.Streaming.StreamingQuery"/>
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.Trigger.ProcessingTime(System.Int64)">
            <summary>
            A trigger policy that runs a query periodically based on an interval 
            in processing time.
            If `interval` is 0, the query will run as fast as possible.
            </summary>
            <param name="intervalMs">Milliseconds</param>
            <returns>Trigger Object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.Trigger.ProcessingTime(System.String)">
            <summary>
            A trigger policy that runs a query periodically based on an interval 
            in processing time.
            If `interval` is effectively 0, the query will run as fast as possible.
            </summary>
            <param name="interval">string representation for interval. eg. "10 seconds"</param>
            <returns>Trigger Object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.Trigger.Once">
            <summary>
            A trigger that process only one batch of data in a streaming query 
            then terminates the query.
            </summary>
            <returns>Trigger Object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.Trigger.Continuous(System.Int64)">
            <summary>
            A trigger that continuously processes streaming data,
            asynchronously checkpointing at the specified interval.
            </summary>
            <param name="intervalMs">Milliseconds</param>
            <returns>Trigger Object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Streaming.Trigger.Continuous(System.String)">
            <summary>
            A trigger that continuously processes streaming data,
            asynchronously checkpointing at the specified interval.
            </summary>
            <param name="interval">string representation for interval. eg. "10 seconds"</param>
            <returns>Trigger Object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.ArrayType">
            <summary>
            An array type containing multiple values of a type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.ArrayType.#ctor(Microsoft.Spark.Sql.Types.DataType,System.Boolean)">
            <summary>
            Constructor for ArrayType class.
            </summary>
            <param name="elementType">The data type of elements in this array</param>
            <param name="containsNull">Indicates if elements can be null</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.ArrayType.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for ArrayType class.
            </summary>
            <param name="json">JSON object to create the array type from</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.ElementType">
            <summary>
            Returns the data type of the elements in an array.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.ContainsNull">
            <summary>
            Checks if the array can contain null values.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.SimpleString">
            <summary>
            Readable string representation for this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.ArrayType.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.ArrayType.FromJson(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructs a ArrayType object from a JSON object.
            </summary>
            <param name="json">JSON object used to construct a ArrayType object</param>
            <returns>ArrayType object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.MapType">
            <summary>
            The data type for a map. 
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.MapType.#ctor(Microsoft.Spark.Sql.Types.DataType,Microsoft.Spark.Sql.Types.DataType,System.Boolean)">
            <summary>
            Constructor for MapType class.
            </summary>
            <param name="keyType">The data type of keys in this map</param>
            <param name="valueType">The data type of values in this map</param>
            <param name="valueContainsNull">Indicates if values can be null</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.MapType.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for MapType class.
            </summary>
            <param name="json">JSON object to create the map type from</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.MapType.KeyType">
            <summary>
            Returns the data type of the keys in the map.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.MapType.ValueType">
            <summary>
            Returns the data type of the values in the map.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.MapType.ValueContainsNull">
            <summary>
            Checks if the value can contain null values.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.MapType.SimpleString">
            <summary>
            Readable string representation for this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.MapType.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.MapType.FromJson(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructs a MapType object from a JSON object.
            </summary>
            <param name="json">JSON object used to construct a MapType object</param>
            <returns>MapType object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.StructField">
            <summary>
            A type that represents a field inside StructType.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructField.#ctor(System.String,Microsoft.Spark.Sql.Types.DataType,System.Boolean,Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for StructFieldType class.
            </summary>
            <param name="name">The name of this field</param>
            <param name="dataType">The data type of this field</param>
            <param name="isNullable">Indicates if values of this field can be null</param>
            <param name="metadata">The metadata of this field</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructField.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for StructFieldType class.
            </summary>
            <param name="json">JSON object to construct a StructFieldType object</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.Name">
            <summary>
            The name of this field.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.DataType">
            <summary>
            The data type of this field.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.IsNullable">
            <summary>
            Checks if values of this field can be null.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.Metadata">
            <summary>
            The metadata of this field.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructField.ToString">
            <summary>
            Returns a readable string that represents this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructField.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.StructType">
            <summary>
            Struct type represents a struct with multiple fields.
            This type is also used to represent a Row object in Spark.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.#ctor(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructor for StructType class.
            </summary>
            <param name="json">JSON object to construct a StructType object</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.#ctor(Microsoft.Spark.Interop.Ipc.JvmObjectReference)">
            <summary>
            Constructor for StructType class.
            </summary>
            <param name="jvmObject">StructType object on JVM</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructType.Fields">
            <summary>
            Returns a list of StructFieldType objects.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.#ctor(System.Collections.Generic.IEnumerable{Microsoft.Spark.Sql.Types.StructField})">
            <summary>
            Constructor for StructType class.
            </summary>
            <param name="fields">A collection of StructFieldType objects</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructType.SimpleString">
            <summary>
            Returns a readable string that represents this type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.StructType.JsonValue">
            <summary>
            Returns JSON object describing this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.StructType.FromJson(Newtonsoft.Json.Linq.JObject)">
            <summary>
            Constructs a StructType object from a JSON object
            </summary>
            <param name="json">JSON object used to construct a StructType object</param>
            <returns>A StuructType object</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DataType">
            <summary>
            The base type of all Spark SQL data types.
            Note that the implementation mirrors PySpark: spark/python/pyspark/sql/types.py
            The Scala version is spark/sql/catalyst/src/main/scala/org/apache/spark/sql/types/*.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.TypeName">
            <summary>
            Normalized type name.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.SimpleString">
            <summary>
            Simple string version of the current data type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.Json">
            <summary>
            The compact JSON representation of this data type.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DataType.JsonValue">
            <summary>
            JSON value of this data type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.FromJson(Microsoft.Spark.Interop.Ipc.IJvmBridge,System.String)">
            <summary>
            Parses a JSON string to create a <see cref="T:Microsoft.Spark.Interop.Ipc.JvmObjectReference"/>.
            It references a <see cref="T:Microsoft.Spark.Sql.Types.StructType"/> on the JVM side.
            </summary>
            <param name="jvm">JVM bridge to use</param>
            <param name="json">JSON string to parse</param>
            <returns>The new JvmObjectReference created from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.ParseDataType(System.String)">
            <summary>
            Parses a JSON string to construct a DataType.
            </summary>
            <param name="json">JSON string to parse</param>
            <returns>The new DataType instance from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object by
            checking the string version of this type.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal.</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.ParseDataType(Newtonsoft.Json.Linq.JToken)">
            <summary>
            Parses a JToken object to construct a DataType.
            </summary>
            <param name="json">JToken object to parse</param>
            <returns>The new DataType instance from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.NeedConversion">
            <summary>
            Does this type need to conversion between C# object and internal SQL object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.FromInternal(System.Object)">
            <summary>
            Converts an internal SQL object into a native C# object.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.ParseSimpleType(Newtonsoft.Json.Linq.JToken)">
            <summary>
            Parses a JToken object that represents a simple type.
            </summary>
            <param name="json">JToken object to parse</param>
            <returns>The new DataType instance from the JSON string</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DataType.NormalizeTypeName(System.String)">
            <summary>
            Remove "Type" from the end of type name and lower cases to align with Scala type name.
            </summary>
            <param name="typeName">Type name to normalize</param>
            <returns>Normalized type name</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.Date">
            <summary>
            Represents Date containing year, month, and day.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.#ctor(System.DateTime)">
            <summary>
            Constructor for Date class.
            </summary>
            <param name="dateTime">DateTime object</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.#ctor(System.Int32,System.Int32,System.Int32)">
            <summary>
            Constructor for Date class.
            </summary>
            <param name="year">The year (1 through 9999)</param>
            <param name="month">The month (1 through 12)</param>
            <param name="day">The day (1 through the number of days in month)</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Date.Year">
            <summary>
            Returns the year component of the date.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Date.Month">
            <summary>
            Returns the month component of the date.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Date.Day">
            <summary>
            Returns the day component of the date.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.ToString">
            <summary>
            Readable string representation for this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.ToDateTime">
            <summary>
            Returns DateTime object describing this type.
            </summary>
            <returns>DateTime object of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Date.GetInterval">
            <summary>
            Returns an integer object that represents a count of days from 1970-01-01.
            </summary>
            <returns>Integer object that represents a count of days from 1970-01-01</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.AtomicType">
            <summary>
            An internal type used to represent everything that is not null, arrays, structs, and maps.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.NumericType">
            <summary>
            Represents a numeric type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.IntegralType">
            <summary>
            Represents an integral type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.FractionalType">
            <summary>
            Represents a fractional type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.NullType">
            <summary>
            Represents a null type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.StringType">
            <summary>
            Represents a string type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.BinaryType">
            <summary>
            Represents a binary (byte array) type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.BooleanType">
            <summary>
            Represents a boolean type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DateType">
            <summary>
            Represents a date type. It represents a valid date in the proleptic Gregorian
            calendar. Valid range is [0001-01-01, 9999-12-31].
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DateType.FromInternal(System.Object)">
            <summary>
            Internally, a date is stored as a simple incrementing count of days as int
            where day 0 is 1970-01-01. This will convert internal SQL DateType objects
            from count of days into native C# Date objects.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.TimestampType">
            <summary>
            Represents a timestamp type. It represents a time instant in microsecond precision.
            Valid range is [0001-01-01T00:00:00.000000Z, 9999-12-31T23:59:59.999999Z] where
            the left/right-bound is a date and time of the proleptic Gregorian calendar in UTC+00:00.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.TimestampType.FromInternal(System.Object)">
            <summary>
            Internally, a timestamp is stored as the number of microseconds as long from the epoch
            of 1970-01-01T00:00:00.000000Z(UTC+00:00). This will convert internal SQL TimestampType
            objects from the number of microseconds into native C# Timestamp objects.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DoubleType">
            <summary>
            Represents a double type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.FloatType">
            <summary>
            Represents a float type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.ByteType">
            <summary>
            Represents a byte type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.IntegerType">
            <summary>
            Represents an int type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.LongType">
            <summary>
            Represents a long type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.ShortType">
            <summary>
            Represents a short type.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.DecimalType">
            <summary>
            Represents a decimal type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.DecimalType.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes the <see cref="T:Microsoft.Spark.Sql.Types.DecimalType"/> instance.
            </summary>
            <remarks>
            Default values of precision and scale are from Scala:
            sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala.
            </remarks>
            <param name="precision">Number of digits in a number</param>
            <param name="scale">
            Number of digits to the right of the decimal point in a number
            </param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.DecimalType.SimpleString">
            <summary>
            Returns simple string version of DecimalType.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.Types.Timestamp">
            <summary>
            Represents Timestamp containing year, month, day, hour, minute, second, microsecond in
            Coordinated Universal Time (UTC).
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.#ctor(System.DateTime)">
            <summary>
            Constructor for Timestamp class.
            </summary>
            <param name="dateTime">DateTime object</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.#ctor(System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Constructor for Timestamp class, the defaut timezone is
            Coordinated Universal Time (UTC).
            </summary>
            <param name="year">The year (1 through 9999)</param>
            <param name="month">The month (1 through 12)</param>
            <param name="day">The day (1 through the number of days in month)</param>
            <param name="hour">The hour (0 through 23)</param>
            <param name="minute">The minute (0 through 59)</param>
            <param name="second">The second (0 through 59)</param>
            <param name="microsecond">The microsecond (0 through 999999)</param>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Year">
            <summary>
            Returns the year component of the timestamp.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Month">
            <summary>
            Returns the month component of the timestamp.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Day">
            <summary>
            Returns the day component of the timestamp.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Hour">
            <summary>
            Returns the hour component of the timestamp.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Minute">
            <summary>
            Returns the minute component of the timestamp.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Second">
            <summary>
            Returns the second component of the timestamp.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Sql.Types.Timestamp.Microsecond">
            <summary>
            Returns the microsecond component of the timestamp.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.ToString">
            <summary>
            Readable string representation for this type.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.Equals(System.Object)">
            <summary>
            Checks if the given object is same as the current object.
            </summary>
            <param name="obj">Other object to compare against</param>
            <returns>True if the other object is equal</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.GetHashCode">
            <summary>
            Returns the hash code of the current object.
            </summary>
            <returns>The hash code of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.ToDateTime">
            <summary>
            Returns DateTime object describing this type.
            </summary>
            <returns>DateTime object of the current object</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.GetIntervalInSeconds">
            <summary>
            Returns a double object that represents the number of microseconds from the epoch
            of 1970-01-01T00:00:00.000000Z(UTC+00:00) in the second unit to serialize and
            deserialize between CLR and JVM.
            </summary>
            <returns>Double object that represents the number of seconds from the epoch of
            1970-01-01T00:00:00.000000Z(UTC+00:00)</returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.Types.Timestamp.GetIntervalInMicroseconds">
            <summary>
            Returns a long object that represents the number of microseconds from the epoch of
            1970-01-01T00:00:00.000000Z(UTC+00:00).
            </summary>
            <returns>Long object that represents the number of microseconds from the epoch of
            1970-01-01T00:00:00.000000Z(UTC+00:00)</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.UdfRegistration">
            <summary>
            Functions for registering user-defined functions.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``1(System.String,System.Func{``0})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``2(System.String,System.Func{``0,``1})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``3(System.String,System.Func{``0,``1,``2})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``4(System.String,System.Func{``0,``1,``2,``3})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``5(System.String,System.Func{``0,``1,``2,``3,``4})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``6(System.String,System.Func{``0,``1,``2,``3,``4,``5})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``7(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``8(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``9(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``10(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``11(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,``10})">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <typeparam name="TResult">Specifies the return type of the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register(System.String,System.Func{Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``1(System.String,System.Func{``0,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T">Specifies the type of the first argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``2(System.String,System.Func{``0,``1,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``3(System.String,System.Func{``0,``1,``2,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``4(System.String,System.Func{``0,``1,``2,``3,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``5(System.String,System.Func{``0,``1,``2,``3,``4,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``6(System.String,System.Func{``0,``1,``2,``3,``4,``5,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``7(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``8(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``9(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``10(System.String,System.Func{``0,``1,``2,``3,``4,``5,``6,``7,``8,``9,Microsoft.Spark.Sql.Row},Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Registers the given delegate as a user-defined function with the specified name.
            </summary>
            <typeparam name="T1">Specifies the type of the first argument to the UDF.</typeparam>
            <typeparam name="T2">Specifies the type of the second argument to the UDF.</typeparam>
            <typeparam name="T3">Specifies the type of the third argument to the UDF.</typeparam>
            <typeparam name="T4">Specifies the type of the fourth argument to the UDF.</typeparam>
            <typeparam name="T5">Specifies the type of the fifth argument to the UDF.</typeparam>
            <typeparam name="T6">Specifies the type of the sixth argument to the UDF.</typeparam>
            <typeparam name="T7">Specifies the type of the seventh argument to the UDF.</typeparam>
            <typeparam name="T8">Specifies the type of the eighth argument to the UDF.</typeparam>
            <typeparam name="T9">Specifies the type of the ninth argument to the UDF.</typeparam>
            <typeparam name="T10">Specifies the type of the tenth argument to the UDF.</typeparam>
            <param name="name">The UDF name.</param>
            <param name="f">The UDF function implementation.</param>
            <param name="returnType">Schema associated with this row</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.RegisterJava``1(System.String,System.String)">
            <summary>
            Register a Java UDF class using reflection.
            </summary>
            <typeparam name="TResult">Return type</typeparam>
            <param name="name">Name of the UDF</param>
            <param name="className">Class name that defines UDF</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.RegisterJavaUDAF(System.String,System.String)">
            <summary>
            Register a Java UDAF class using reflection.
            </summary>
            <param name="name">Name of the UDAF</param>
            <param name="className">Class name that defines UDAF</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``1(System.String,System.Delegate)">
            <summary>
            Helper function to register wrapped udf.
            </summary>
            <typeparam name="TResult">Return type of the udf</typeparam>
            <param name="name">Name of the udf</param>
            <param name="func">Wrapped UDF function</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register(System.String,System.Delegate,Microsoft.Spark.Sql.Types.StructType)">
            <summary>
            Helper function to register wrapped udf.
            </summary>
            <param name="name">Name of the udf</param>
            <param name="func">Wrapped UDF function</param>
            <param name="returnType">Schema associated with the udf</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register``1(System.String,System.Delegate,Microsoft.Spark.Utils.UdfUtils.PythonEvalType)">
            <summary>
            Helper function to register wrapped udf.
            </summary>
            <typeparam name="TResult">Return type of the udf</typeparam>
            <param name="name">Name of the udf</param>
            <param name="func">Wrapped UDF function</param>
            <param name="evalType">The EvalType of the function</param>
        </member>
        <member name="M:Microsoft.Spark.Sql.UdfRegistration.Register(System.String,System.Delegate,Microsoft.Spark.Utils.UdfUtils.PythonEvalType,System.String)">
            <summary>
            Helper function to register wrapped udf.
            </summary>
            <param name="name">Name of the udf</param>
            <param name="func">Wrapped UDF function</param>
            <param name="evalType">The EvalType of the function</param>
            <param name="returnType">The return type of the function in JSON format</param>
        </member>
        <member name="T:Microsoft.Spark.Sql.WorkerFunction">
            <summary>
            Function that will be executed in the worker.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowWorkerFunction">
            <summary>
            Function that will be executed in the worker using the Apache Arrow format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowWorkerFunction.ExecuteDelegate">
            <summary>
            Type of the UDF to run. Refer to <see cref="T:Microsoft.Spark.Sql.ArrowUdfWrapper`2"/>.Execute.
            </summary>
            <param name="input">unpickled data, representing a row</param>
            <param name="argOffsets">offsets to access input</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.ArrowWorkerFunction.Chain(Microsoft.Spark.Sql.ArrowWorkerFunction,Microsoft.Spark.Sql.ArrowWorkerFunction)">
            <summary>
            Used to chain functions.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.ArrowWorkerFunction.WorkerFuncChainHelper.s_outerFuncArgOffsets">
            <summary>
            The outer function will always take 0 as an offset since there is only one
            return value from an inner function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowGroupedMapWorkerFunction">
            <summary>
            Function for Grouped Map Vector UDFs using the Apache Arrow format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.ArrowGroupedMapWorkerFunction.ExecuteDelegate">
            <summary>
            A delegate to invoke a Grouped Map Vector UDF.
            </summary>
            <param name="input">The input data frame.</param>
            <returns>The resultant data frame.</returns>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingWorkerFunction">
            <summary>
            Function that will be executed in the worker using the Python pickling format.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Sql.PicklingWorkerFunction.ExecuteDelegate">
            <summary>
            Type of the UDF to run. Refer to <see cref="T:Microsoft.Spark.Sql.PicklingUdfWrapper`1"/>.Execute.
            </summary>
            <param name="splitId">split id for the current task</param>
            <param name="input">unpickled data, representing a row</param>
            <param name="argOffsets">offsets to access input</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.Spark.Sql.PicklingWorkerFunction.Chain(Microsoft.Spark.Sql.PicklingWorkerFunction,Microsoft.Spark.Sql.PicklingWorkerFunction)">
            <summary>
            Used to chain functions.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Sql.PicklingWorkerFunction.WorkerFuncChainHelper.s_outerFuncArgOffsets">
            <summary>
            The outer function will always take 0 as an offset since there is only one
            return value from an inner function.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.TaskContext">
            <summary>
            TaskContext stores information related to a task.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.AssemblySearchPathResolver.GetAssemblySearchPaths">
            <summary>
            Returns the paths to search when loading assemblies in the following order of
            precedence:
            1) Comma-separated paths specified in DOTNET_ASSEMBLY_SEARCH_PATHS environment
            variable. Note that if a path starts with ".", the working directory will be prepended.
            2) The path of the files added through
            <see cref="M:Microsoft.Spark.SparkContext.AddFile(System.String,System.Boolean)"/>.
            3) The working directory.
            4) The directory of the application.
            </summary>
            <remarks>
            The reason that the working directory has higher precedence than the directory
            of the application is for cases when spark is launched on YARN. The executors are run
            inside 'containers' and files that are passed via 'spark-submit --files' will be pushed
            to these 'containers'. This path is the working directory and the 1st probing path that
            will be checked.
            </remarks>
            <returns>Assembly search paths</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.AssemblyLoader.LoadAssembly(System.String,System.String)">
            <summary>
            Return the cached assembly, otherwise attempt to load and cache the assembly
            by searching for the assembly filename in the search paths.
            </summary>
            <param name="assemblyName">The full name of the assembly</param>
            <param name="assemblyFileName">Name of the file that contains the assembly</param>
            <returns>Cached or Loaded Assembly or null if not found</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.AssemblyLoader.ResolveAssembly(System.String)">
            <summary>
            Return the cached assembly, otherwise look in the probing paths returned
            by AssemblySearchPathResolver, searching for the simple assembly name and
            s_extension combination.
            </summary>
            <param name="assemblyName">The fullname of the assembly to load</param>
            <returns>The loaded assembly or null if not found</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.AssemblyLoader.TryLoadAssembly(System.String,System.Reflection.Assembly@)">
            <summary>
            Returns the loaded assembly by probing paths returned by AssemblySearchPathResolver.
            </summary>
            <param name="assemblyFileName">Name of the file that contains the assembly</param>
            <param name="assembly">The loaded assembly.</param>
            <returns>True if assembly is loaded, false otherwise.</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.AssemblyLoader.NormalizeAssemblyName(System.String)">
            <summary>
            Normalizes the assemblyName by removing characters known to cause
            issues. This is useful in situations where the assemblyName is
            automatically generated, ie the Roslyn compiler used in the REPL
            generates an assembly name that contains * and #.
            </summary>
            <param name="assemblyName">Assembly name</param>
            <returns>Normalized assembly name</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.Authenticator">
            <summary>
            Authenticator provides functionalities to authenticate between
            Spark and .NET worker.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.Authenticator.AuthenticateAsClient(System.IO.Stream,System.String)">
            <summary>
            Authenticates by writing secret to stream and validate the response.
            </summary>
            <param name="stream">Valid stream.</param>
            <param name="secret">Secret string to authenticate against.</param>
            <returns>True if authentication succeeds.</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.Authenticator.AuthenticateAsServer(Microsoft.Spark.Network.ISocketWrapper,System.String)">
            <summary>
            Authenticates by reading secret from stream and writes the response code
            back to the stream.
            </summary>
            <param name="socket">Valid socket.</param>
            <param name="secret">Secret string to authenticate against.</param>
            <returns>True if authentication succeeds.</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.CommandSerDe">
            <summary>
            CommandSerDe provides functionality to serialize/deserialize WorkerFunction
            along with other information.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperMethodName">
            <summary>
            The function name of any UDF wrappers that wrap the UDF.
            ex) <see cref="M:Microsoft.Spark.RDD`1.MapUdfWrapper`2.Execute(System.Int32,System.Collections.Generic.IEnumerable{System.Object})"/>
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode">
            <summary>
            Captures the information about the UDF wrapper.
            Example classes for wrapping UDF are:
             - SQL: * <see cref="T:Microsoft.Spark.Sql.ArrowUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.Sql.PicklingUdfWrapper`1"/>
             - RDD: * <see cref="T:Microsoft.Spark.RDD`1.MapUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.RDD`1.FlatMapUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.RDD`1.MapPartitionsUdfWrapper`2"/>
                    * <see cref="T:Microsoft.Spark.RDD.WorkerFunction.WorkerFuncChainHelper"/>
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode.TypeName">
            <summary>
            Type name of the UDF wrapper.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode.NumChildren">
            <summary>
            Number of children (UDF wrapper or UDF) this node is associated with.
            Note that there can be up to two children and if the child is an UDF,
            this will be set to one.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperNode.HasUdf">
            <summary>
            True if the child is an UDF.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperData">
            <summary>
            UdfWrapperData represents the flattened tree structure.
            For example:
                                   WorkerChainHelper#1
                                    /                \
                      WorkerChainHelper#2        MapUdfWrapper#3
                          /          \                  \
             MapUdfWrapper#1   MapUdfWrapper#2         UDF#3
                    |                 |
                  UDF#1             UDF#2
            
            will be translated into:
            UdfWrapperNodes: (WorkerChainHelper(WCH), MapUdfWrapper(MUW))
               [ WCH#1(2, false), WCH#2(2, false), MUW#1(1, true), MUW#2(1, true), MUW#3(1, true) ]
               where WCH#1(2, false) means the node has two children and HasUdf is false.
            Udfs:
               [ UDF#1, UDF#2, UDF#3 ]
            
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperData.UdfWrapperNodes">
            <summary>
            Flattened UDF wrapper nodes.
            </summary>
        </member>
        <member name="P:Microsoft.Spark.Utils.CommandSerDe.UdfWrapperData.Udfs">
            <summary>
            Serialized UDF data.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.DataFrameUdfUtils">
            <summary>
            DataFrameUdfUtils provides utility functions to wrap UDFs that use Microsoft.Data.Analysis
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.EnvironmentUtils">
            <summary>
            Various environment utility methods.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.PythonSerDe">
            <summary>
            Used for SerDe of Python objects.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.PythonSerDe.GetUnpickledObjects(System.IO.Stream,System.Int32)">
            <summary>
            Unpickles objects from Stream.
            </summary>
            <param name="stream">Pickled byte stream</param>
            <param name="messageLength">Size (in bytes) of the pickled input</param>
            <returns>Unpicked objects</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfSerDe">
            <summary>
            UdfSerDe is responsible for serializing/deserializing an UDF.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfTypeUtils">
            <summary>
            UdfTypeUtils provides functions related to UDF types.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfTypeUtils.CanBeNull(System.Type)">
            <summary>
            Returns "true" if the given type is nullable.
            </summary>
            <param name="type">Type to check if it is nullable</param>
            <returns>"true" if the given type is nullable. Otherwise, returns "false"</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfTypeUtils.ImplementsGenericTypeOf(System.Type,System.Type)">
            <summary>
            Returns the generic type definition of a given type if the given type is equal
            to or implements the `compare` type. Returns null if there is no match.
            </summary>
            <param name="type">This type object</param>
            <param name="compare">Generic type definition to compare to</param>
            <returns>Matching generic type object</returns>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfUtils">
            <summary>
            UdfUtils provides UDF-related functions and enum.
            </summary>
        </member>
        <member name="T:Microsoft.Spark.Utils.UdfUtils.PythonEvalType">
            <summary>
            Enum for Python evaluation type. This determines how the data will be serialized
            from Spark executor to its worker.
            Since UDF is based on PySpark implementation, PythonEvalType is used. Once
            generic interop layer is introduced, this will be revisited.
            This mirrors values defined in python/pyspark/rdd.py.
            </summary>
        </member>
        <member name="F:Microsoft.Spark.Utils.UdfUtils.s_returnTypes">
            <summary>
            Mapping of supported types from .NET to org.apache.spark.sql.types.DataType in Scala.
            Refer to spark/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala
            for more information.
            </summary>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfUtils.GetReturnType(System.Type)">
            <summary>
            Returns the return type of an UDF in JSON format. This value is used to
            create a org.apache.spark.sql.types.DataType object from JSON string.
            </summary>
            <param name="type">Return type of an UDF</param>
            <returns>JSON format of the return type</returns>
        </member>
        <member name="M:Microsoft.Spark.Utils.UdfUtils.CreatePythonFunction(Microsoft.Spark.Interop.Ipc.IJvmBridge,System.Byte[])">
            <summary>
            Creates the PythonFunction object on the JVM side wrapping the given command bytes.
            </summary>
            <param name="jvm">JVM bridge to use</param>
            <param name="command">Serialized command bytes</param>
            <returns>JvmObjectReference object to the PythonFunction object</returns>
        </member>
    </members>
</doc>
